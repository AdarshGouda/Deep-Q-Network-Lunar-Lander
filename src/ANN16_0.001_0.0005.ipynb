{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"ANN16.csv\"\n",
    "        self.model_filename = \"ANN16.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(16, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(16, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "                self.ep *= self.ep_decay\n",
    "            else:\n",
    "                self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "             \n",
    "        self.df_ddqn.to_csv(self.csv_filename)    \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return R, R_moving\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -139.27284009862137 , mean_reward:  -139.27284009862137 , time_score:  83 , memory:  83\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -391.43504196251547 , mean_reward:  -226.25916434659192 , time_score:  86 , memory:  533\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -76.68257583707945 , mean_reward:  -235.7711132648212 , time_score:  73 , memory:  1031\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -110.80771005507088 , mean_reward:  -230.46410421170134 , time_score:  102 , memory:  1489\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -106.93539218789076 , mean_reward:  -200.17246639214807 , time_score:  76 , memory:  1981\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -229.78016141034888 , mean_reward:  -186.48196029679198 , time_score:  85 , memory:  2472\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -106.61431569034681 , mean_reward:  -174.36938871902456 , time_score:  83 , memory:  2937\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -114.17389969092942 , mean_reward:  -165.65988006028223 , time_score:  113 , memory:  3437\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -94.34251553432959 , mean_reward:  -167.11849477263678 , time_score:  85 , memory:  3948\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -112.81064360062156 , mean_reward:  -161.0752101658143 , time_score:  116 , memory:  4464\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward -75.44570943136029 , mean_reward:  -167.84258911954313 , time_score:  131 , memory:  5061\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -170.26968237106706 , mean_reward:  -172.38268523324632 , time_score:  82 , memory:  5580\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -233.45290012939373 , mean_reward:  -178.01635808671747 , time_score:  96 , memory:  6115\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -75.59219022006297 , mean_reward:  -174.83662988070063 , time_score:  128 , memory:  6783\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -73.43064191400441 , mean_reward:  -168.5188641221448 , time_score:  126 , memory:  7305\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -8.122520005755291 , mean_reward:  -164.12021741842912 , time_score:  116 , memory:  7990\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward -273.504315747168 , mean_reward:  -167.97791441546823 , time_score:  122 , memory:  8671\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -231.39519394065601 , mean_reward:  -169.04410625717293 , time_score:  295 , memory:  9573\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -274.49882825282606 , mean_reward:  -173.4453435826977 , time_score:  123 , memory:  10193\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -71.47203283504982 , mean_reward:  -173.4516673719622 , time_score:  203 , memory:  10799\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -195.66265710087146 , mean_reward:  -172.2040144591707 , time_score:  203 , memory:  11683\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -182.3830711097707 , mean_reward:  -169.79631119779947 , time_score:  130 , memory:  12549\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -420.2333351379442 , mean_reward:  -169.43360814433476 , time_score:  308 , memory:  13379\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -103.16676754753789 , mean_reward:  -167.23662436896518 , time_score:  102 , memory:  14157\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -245.27908582856986 , mean_reward:  -170.6742684351408 , time_score:  154 , memory:  14885\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -148.53080484536792 , mean_reward:  -173.42911052120394 , time_score:  114 , memory:  15727\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -199.2469658492102 , mean_reward:  -177.6601910403688 , time_score:  102 , memory:  16771\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -185.62096030472154 , mean_reward:  -181.9842616633701 , time_score:  160 , memory:  17655\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward -207.2451572277643 , mean_reward:  -185.12704451897508 , time_score:  239 , memory:  18678\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward -213.17693839497048 , mean_reward:  -186.82074678154274 , time_score:  243 , memory:  19753\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -151.26713050276283 , mean_reward:  -183.49882947723242 , time_score:  137 , memory:  20558\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -254.83750194396026 , mean_reward:  -180.32514867715258 , time_score:  351 , memory:  22082\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -200.21127828492394 , mean_reward:  -177.73001001485355 , time_score:  204 , memory:  23609\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward -98.38025350430142 , mean_reward:  -178.43914306407143 , time_score:  144 , memory:  24508\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -325.3866024001875 , mean_reward:  -185.47606537427797 , time_score:  277 , memory:  25700\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward -60.1753590874722 , mean_reward:  -186.36434888378852 , time_score:  500 , memory:  27533\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward -55.50565448960937 , mean_reward:  -179.45843959158321 , time_score:  158 , memory:  29366\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -165.43971309164556 , mean_reward:  -175.59534099123502 , time_score:  266 , memory:  30944\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward 20.113059238092557 , mean_reward:  -164.95725024332313 , time_score:  500 , memory:  33444\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -59.89882803471744 , mean_reward:  -160.26869995664995 , time_score:  500 , memory:  35380\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -58.21812203081984 , mean_reward:  -154.8500259547079 , time_score:  291 , memory:  37270\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward -110.6571564638166 , mean_reward:  -147.4517162788573 , time_score:  493 , memory:  39653\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -32.44315701752834 , mean_reward:  -138.9885644550957 , time_score:  500 , memory:  41379\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward -297.4936623303888 , mean_reward:  -135.74246147094033 , time_score:  246 , memory:  42904\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward -26.40313869394258 , mean_reward:  -128.75227432678167 , time_score:  351 , memory:  45046\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward -5.040122163392495 , mean_reward:  -122.95503484736757 , time_score:  500 , memory:  46673\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward -141.2363132428177 , mean_reward:  -116.72463432337702 , time_score:  155 , memory:  48321\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward -24.212721033830405 , mean_reward:  -109.96900682499948 , time_score:  332 , memory:  50248\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward 26.19680607103905 , mean_reward:  -98.68594298035205 , time_score:  500 , memory:  52588\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward 49.4955868620154 , mean_reward:  -94.02380317434354 , time_score:  500 , memory:  54856\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward -7.450528097517944 , mean_reward:  -88.05257810666001 , time_score:  500 , memory:  57196\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward -33.0213858422325 , mean_reward:  -83.48587369412839 , time_score:  500 , memory:  59328\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward 67.6919334841202 , mean_reward:  -75.14906669080479 , time_score:  500 , memory:  61471\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward 122.53311728342946 , mean_reward:  -66.19437581158473 , time_score:  500 , memory:  63971\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward 26.72961697912331 , mean_reward:  -55.08482837291398 , time_score:  500 , memory:  66228\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward -51.30012289675661 , mean_reward:  -49.16292833834652 , time_score:  255 , memory:  68483\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward 32.19255227255247 , mean_reward:  -42.91928571141584 , time_score:  500 , memory:  70983\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward -20.223450209182364 , mean_reward:  -39.92208743663944 , time_score:  500 , memory:  73243\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward -3.6100772334320617 , mean_reward:  -36.74720270520109 , time_score:  500 , memory:  75743\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward 89.70907791119248 , mean_reward:  -32.74936357171429 , time_score:  500 , memory:  77859\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward -68.26287195344973 , mean_reward:  -32.76069075310734 , time_score:  306 , memory:  80104\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward 9.804644208488359 , mean_reward:  -29.445730270504765 , time_score:  500 , memory:  82604\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward 9.376050338860086 , mean_reward:  -24.692053423168794 , time_score:  500 , memory:  85104\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward 23.588808285040994 , mean_reward:  -17.80895244518009 , time_score:  500 , memory:  87391\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward 6.311040388106054 , mean_reward:  -15.717626534436924 , time_score:  500 , memory:  89891\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward 44.50394985248506 , mean_reward:  -11.36884641749545 , time_score:  500 , memory:  92391\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward 43.79389708480153 , mean_reward:  -6.371074518607109 , time_score:  500 , memory:  94891\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward -3.4740712448338 , mean_reward:  -3.360078130697551 , time_score:  500 , memory:  97265\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward 49.51614367621882 , mean_reward:  -1.2865146582663614 , time_score:  500 , memory:  99765\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward 6.800536896136 , mean_reward:  2.536227008737797 , time_score:  500 , memory:  102265\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward 32.861576179473275 , mean_reward:  6.000184251679352 , time_score:  500 , memory:  104589\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward 5.021817953733651 , mean_reward:  10.698223623599487 , time_score:  500 , memory:  107089\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward 29.382465703092215 , mean_reward:  12.929333533453084 , time_score:  500 , memory:  109589\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward -8.436141622481557 , mean_reward:  13.466054764542072 , time_score:  500 , memory:  112089\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward 20.32466137164852 , mean_reward:  14.72920065184268 , time_score:  500 , memory:  114589\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward 25.28176821737352 , mean_reward:  15.368138368823475 , time_score:  500 , memory:  117089\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward 42.57393862029061 , mean_reward:  14.145950825314944 , time_score:  500 , memory:  119589\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward -3.4472972656413305 , mean_reward:  16.840022266075035 , time_score:  500 , memory:  122089\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward 11.229397204842924 , mean_reward:  16.407972402875835 , time_score:  500 , memory:  124589\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward -14.743239213018962 , mean_reward:  16.61114989466777 , time_score:  500 , memory:  127089\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward -16.346348734961282 , mean_reward:  17.824144494420715 , time_score:  500 , memory:  129589\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward 34.43274185919367 , mean_reward:  16.595781684238876 , time_score:  500 , memory:  132089\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward 19.43044807223507 , mean_reward:  14.709677945575645 , time_score:  500 , memory:  134589\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward 28.224198926867818 , mean_reward:  13.296837649440317 , time_score:  500 , memory:  137089\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward 1.159893737905308 , mean_reward:  13.058149405387544 , time_score:  500 , memory:  139589\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward 10.186902761551112 , mean_reward:  12.720703323207408 , time_score:  500 , memory:  142089\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward 18.723180181299135 , mean_reward:  10.991718859892718 , time_score:  500 , memory:  144589\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward 11.604836593545492 , mean_reward:  10.961893487077258 , time_score:  500 , memory:  147089\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward 24.55267720401565 , mean_reward:  10.973572661108719 , time_score:  500 , memory:  149589\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward -5.806712938787129 , mean_reward:  10.214130640431687 , time_score:  500 , memory:  152089\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward -6.359965695754088 , mean_reward:  9.788063512702196 , time_score:  500 , memory:  154589\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward 0.8594507590352074 , mean_reward:  8.830041478967841 , time_score:  500 , memory:  157089\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward 2.7808937183975764 , mean_reward:  8.604061149096179 , time_score:  500 , memory:  159589\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward -8.652729279446842 , mean_reward:  7.412761699433003 , time_score:  500 , memory:  162089\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward 23.324232009877946 , mean_reward:  6.960947678592192 , time_score:  500 , memory:  164589\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward 61.46649620046048 , mean_reward:  8.595782314502541 , time_score:  500 , memory:  167089\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward -23.14988563412031 , mean_reward:  8.935874616187407 , time_score:  500 , memory:  169589\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward 27.998586890584566 , mean_reward:  9.250086608264974 , time_score:  500 , memory:  172010\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward 10.848089554940273 , mean_reward:  9.029953311814882 , time_score:  500 , memory:  174510\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward 25.385694429035194 , mean_reward:  9.899133287464974 , time_score:  500 , memory:  177010\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward -8.675059930066089 , mean_reward:  10.933332581957725 , time_score:  500 , memory:  179510\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward 63.7626548300553 , mean_reward:  12.52878371169426 , time_score:  500 , memory:  182010\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward -5.069056083773205 , mean_reward:  13.983881478685339 , time_score:  500 , memory:  184510\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward 28.525341664855347 , mean_reward:  14.546978545150127 , time_score:  500 , memory:  187010\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward 34.316611817533975 , mean_reward:  16.132117062512663 , time_score:  500 , memory:  189510\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward 88.40485109613414 , mean_reward:  17.78217817930251 , time_score:  500 , memory:  192010\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward 73.16098318647893 , mean_reward:  19.737440797420426 , time_score:  500 , memory:  194510\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward 40.583429009206995 , mean_reward:  21.44885553303594 , time_score:  500 , memory:  196823\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward -6.3862791123139315 , mean_reward:  20.670372579911074 , time_score:  500 , memory:  199323\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward 84.78324452127724 , mean_reward:  21.682748461565176 , time_score:  500 , memory:  201823\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward 41.65173012964043 , mean_reward:  21.970743941505262 , time_score:  500 , memory:  204323\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward 47.508417311455304 , mean_reward:  23.403318549303876 , time_score:  500 , memory:  206823\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward 58.56233733607655 , mean_reward:  24.48529000002406 , time_score:  500 , memory:  209323\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward 23.924169386499415 , mean_reward:  25.554876193123864 , time_score:  500 , memory:  211823\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward 70.14416414720803 , mean_reward:  24.62245138432536 , time_score:  500 , memory:  214101\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward 36.23043424769199 , mean_reward:  22.71282872170086 , time_score:  500 , memory:  216601\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward 29.76286876521642 , mean_reward:  23.818355036535827 , time_score:  500 , memory:  219101\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward 46.62386123758575 , mean_reward:  23.234917945668784 , time_score:  500 , memory:  221601\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward 11.487862732299348 , mean_reward:  23.246487435771666 , time_score:  500 , memory:  224101\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward 24.312328644802673 , mean_reward:  22.818484896335214 , time_score:  500 , memory:  226601\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward 86.50931753985152 , mean_reward:  23.639542438782215 , time_score:  500 , memory:  229101\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward 47.91915820959586 , mean_reward:  25.325346345321044 , time_score:  500 , memory:  231601\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward 14.399512939214366 , mean_reward:  26.112808420320476 , time_score:  500 , memory:  234101\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward 24.415001088510703 , mean_reward:  27.276306829077406 , time_score:  500 , memory:  236601\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward 7.3982497160090395 , mean_reward:  26.760311204758413 , time_score:  500 , memory:  239101\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward 7.174108812834202 , mean_reward:  24.5721760744065 , time_score:  500 , memory:  241601\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward 1.0384284293714592 , mean_reward:  24.299154683164037 , time_score:  500 , memory:  244101\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward 42.0931754326958 , mean_reward:  25.390904040317775 , time_score:  500 , memory:  246601\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward -19.020442344575 , mean_reward:  25.502418826977074 , time_score:  500 , memory:  249101\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward 12.995437151275556 , mean_reward:  24.289446919777063 , time_score:  500 , memory:  251601\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward -16.319351641228913 , mean_reward:  23.277949273600658 , time_score:  500 , memory:  254101\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward 42.39810387694209 , mean_reward:  22.744531598264793 , time_score:  500 , memory:  256601\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 46.52617746376936 , mean_reward:  21.027502244928986 , time_score:  500 , memory:  259101\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward -9.90763112023014 , mean_reward:  18.623421818190074 , time_score:  500 , memory:  261601\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward -14.734494979566074 , mean_reward:  19.9301821912748 , time_score:  500 , memory:  264101\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward 22.291700890661932 , mean_reward:  20.758269489805087 , time_score:  500 , memory:  266601\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward 74.42039924293059 , mean_reward:  20.43840255329809 , time_score:  500 , memory:  269101\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward 61.588428504830844 , mean_reward:  20.324411310774174 , time_score:  500 , memory:  271601\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward 49.77727304691277 , mean_reward:  21.630120662483364 , time_score:  500 , memory:  274101\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward -4.3986748269188665 , mean_reward:  23.25011284822901 , time_score:  500 , memory:  276601\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward 3.8365554462792915 , mean_reward:  23.70599417865866 , time_score:  500 , memory:  279101\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward 24.018862901587585 , mean_reward:  21.159553336280283 , time_score:  500 , memory:  281601\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward 6.463217057309639 , mean_reward:  19.87408890558683 , time_score:  500 , memory:  284101\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 11.634578299160307 , mean_reward:  19.501727332983247 , time_score:  500 , memory:  286601\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward 70.21547603401682 , mean_reward:  19.6137073202874 , time_score:  500 , memory:  289101\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 15.1301786712052 , mean_reward:  20.69486863985862 , time_score:  500 , memory:  291601\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward 63.28974846010835 , mean_reward:  21.415803887439598 , time_score:  500 , memory:  294101\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward 5.334418010384595 , mean_reward:  19.920940884422585 , time_score:  500 , memory:  296601\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 1.2879304396938416 , mean_reward:  20.928026929154495 , time_score:  500 , memory:  299101\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward 3.356692825127036 , mean_reward:  22.408875896502487 , time_score:  500 , memory:  301601\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward -41.17404015078959 , mean_reward:  22.917691682298873 , time_score:  500 , memory:  304101\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward 55.3617036230431 , mean_reward:  22.70177382764336 , time_score:  500 , memory:  306601\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward 56.26525449635105 , mean_reward:  23.509454402241172 , time_score:  500 , memory:  309101\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward 0.834819188396062 , mean_reward:  24.79865275418675 , time_score:  500 , memory:  311601\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward -8.043005139770576 , mean_reward:  23.757031485685797 , time_score:  500 , memory:  314101\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward 10.14208757609769 , mean_reward:  23.617323047642405 , time_score:  500 , memory:  316601\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward -12.368985083066502 , mean_reward:  22.908559506438266 , time_score:  500 , memory:  319101\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward -11.087661887506933 , mean_reward:  24.639725815283278 , time_score:  500 , memory:  321601\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward 44.364526028213376 , mean_reward:  23.13997308631649 , time_score:  500 , memory:  324101\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 32.86601209877285 , mean_reward:  22.986450150113775 , time_score:  500 , memory:  326601\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward 19.620869859264758 , mean_reward:  21.704437126033 , time_score:  500 , memory:  329101\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward 3.1353454716795635 , mean_reward:  21.798543156899537 , time_score:  500 , memory:  331601\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward 2.101724167641141 , mean_reward:  21.53296583201318 , time_score:  500 , memory:  334101\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward 17.980226437871796 , mean_reward:  21.56315126110967 , time_score:  500 , memory:  336601\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward -18.742192069601764 , mean_reward:  21.275511692847232 , time_score:  500 , memory:  339101\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward -11.376236693285424 , mean_reward:  21.869635703418886 , time_score:  500 , memory:  341601\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward -4.958431342921636 , mean_reward:  21.17873198701833 , time_score:  500 , memory:  344101\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward -26.952589316038317 , mean_reward:  19.26080540584421 , time_score:  500 , memory:  346601\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward 1.3040634215874378 , mean_reward:  18.7121872543803 , time_score:  500 , memory:  349101\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward -26.500659993504723 , mean_reward:  16.714418215179105 , time_score:  500 , memory:  351601\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward -1.29016464492252 , mean_reward:  14.932515750824631 , time_score:  500 , memory:  354101\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward 26.0110760205422 , mean_reward:  14.757932676923371 , time_score:  500 , memory:  356601\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward 19.72631416464316 , mean_reward:  14.591498802676826 , time_score:  500 , memory:  359101\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward -11.788779960142291 , mean_reward:  14.915687054953738 , time_score:  500 , memory:  361326\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward 6.3889440219958695 , mean_reward:  15.6674060585903 , time_score:  500 , memory:  363826\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward -28.44465135081412 , mean_reward:  15.223600362385875 , time_score:  500 , memory:  366326\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward 63.602183261753744 , mean_reward:  16.155247696305253 , time_score:  500 , memory:  368826\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward 40.657488413089396 , mean_reward:  14.610935944418415 , time_score:  500 , memory:  371326\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward 35.046762531817734 , mean_reward:  14.420038820479215 , time_score:  500 , memory:  373826\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward 11.34797083892223 , mean_reward:  12.68204822876562 , time_score:  500 , memory:  376326\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward 7.581755869343846 , mean_reward:  12.594363599261193 , time_score:  500 , memory:  378826\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward 4.184371109565427 , mean_reward:  12.866246269293883 , time_score:  500 , memory:  381326\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward 40.08848407169212 , mean_reward:  12.699987787475779 , time_score:  500 , memory:  383826\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward 77.01217408864258 , mean_reward:  12.155775072526374 , time_score:  500 , memory:  386326\n",
      "Episode:  920  , Epsilon:  0.01 , Reward 61.811968225525916 , mean_reward:  12.784196630753957 , time_score:  500 , memory:  388826\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 7.001768712408908 , mean_reward:  11.333659727593577 , time_score:  500 , memory:  391326\n",
      "Episode:  930  , Epsilon:  0.01 , Reward 10.480542812378019 , mean_reward:  10.123328907422108 , time_score:  500 , memory:  393826\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 2.9036785672917524 , mean_reward:  11.582043940779258 , time_score:  500 , memory:  396326\n",
      "Episode:  940  , Epsilon:  0.01 , Reward -5.966973606793304 , mean_reward:  9.648345836579951 , time_score:  500 , memory:  398826\n",
      "Episode:  945  , Epsilon:  0.01 , Reward 44.903343313343555 , mean_reward:  12.444874639609598 , time_score:  500 , memory:  401326\n",
      "Episode:  950  , Epsilon:  0.01 , Reward 37.26428315941297 , mean_reward:  15.169084248092076 , time_score:  500 , memory:  403826\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 43.03858157638834 , mean_reward:  14.177970122303616 , time_score:  500 , memory:  406326\n",
      "Episode:  960  , Epsilon:  0.01 , Reward 18.69604945210138 , mean_reward:  13.881785234999223 , time_score:  500 , memory:  408826\n",
      "Episode:  965  , Epsilon:  0.01 , Reward 55.4714147599772 , mean_reward:  13.877343902970338 , time_score:  500 , memory:  411326\n",
      "Episode:  970  , Epsilon:  0.01 , Reward -12.042689137227322 , mean_reward:  13.754984630973599 , time_score:  500 , memory:  413826\n",
      "Episode:  975  , Epsilon:  0.01 , Reward -43.303124564229144 , mean_reward:  14.2359833180964 , time_score:  500 , memory:  416326\n",
      "Episode:  980  , Epsilon:  0.01 , Reward -39.69258509009908 , mean_reward:  12.748909194777909 , time_score:  500 , memory:  418826\n",
      "Episode:  985  , Epsilon:  0.01 , Reward -8.091677854180789 , mean_reward:  13.081139472156417 , time_score:  500 , memory:  421326\n",
      "Episode:  990  , Epsilon:  0.01 , Reward 6.8862676228348105 , mean_reward:  12.842590304616065 , time_score:  500 , memory:  423826\n",
      "Episode:  995  , Epsilon:  0.01 , Reward 32.04837308023822 , mean_reward:  14.389932689142153 , time_score:  500 , memory:  426326\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward -21.79511668812882 , mean_reward:  13.712101107700668 , time_score:  500 , memory:  428826\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward -24.522774003975375 , mean_reward:  13.891295322800726 , time_score:  500 , memory:  431326\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward 8.211337270656898 , mean_reward:  15.151773472489445 , time_score:  500 , memory:  433826\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward 28.4849634869073 , mean_reward:  14.26029551160233 , time_score:  500 , memory:  436326\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward 26.649986137244007 , mean_reward:  13.437444981031906 , time_score:  500 , memory:  438826\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward 13.543896416349114 , mean_reward:  13.994187949310431 , time_score:  500 , memory:  441326\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward 29.379929348957106 , mean_reward:  14.875796671410733 , time_score:  500 , memory:  443826\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward 31.83568981688753 , mean_reward:  16.75453412083302 , time_score:  500 , memory:  446326\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward 100.91217611890768 , mean_reward:  19.959724929762388 , time_score:  500 , memory:  448826\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward 7.712160698610414 , mean_reward:  17.744664000848623 , time_score:  500 , memory:  451326\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward -26.71417373564448 , mean_reward:  17.75273144243083 , time_score:  500 , memory:  453826\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward 57.41029672064141 , mean_reward:  18.28409567830271 , time_score:  500 , memory:  456326\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward -26.947068595011082 , mean_reward:  18.949230368626605 , time_score:  500 , memory:  458826\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward 39.52593614539847 , mean_reward:  18.900930791032387 , time_score:  500 , memory:  461326\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward 0.5304165574102715 , mean_reward:  17.565405412658478 , time_score:  500 , memory:  463826\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward 69.78074066609368 , mean_reward:  18.555825760456194 , time_score:  500 , memory:  466326\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward 72.81497901400459 , mean_reward:  19.257190976601052 , time_score:  500 , memory:  468826\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward -38.07302044807736 , mean_reward:  19.706518018944074 , time_score:  500 , memory:  471326\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward -56.36636597724903 , mean_reward:  18.591308833121776 , time_score:  500 , memory:  473826\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward -0.2174297813828816 , mean_reward:  17.148237026856076 , time_score:  500 , memory:  476326\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward 61.70103036283152 , mean_reward:  19.090225912580493 , time_score:  500 , memory:  478826\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward 33.06775091971289 , mean_reward:  19.40068380544926 , time_score:  500 , memory:  481326\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward 22.5138415413949 , mean_reward:  19.986508922400226 , time_score:  500 , memory:  483826\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 35.5175968633205 , mean_reward:  22.140770382208647 , time_score:  500 , memory:  486326\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward 22.651180875714424 , mean_reward:  23.081764945563695 , time_score:  500 , memory:  488826\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward 80.10786764856931 , mean_reward:  23.60919202851095 , time_score:  500 , memory:  491326\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward -9.264108880018886 , mean_reward:  24.662575218923457 , time_score:  500 , memory:  493826\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward 13.209699673802804 , mean_reward:  22.913972471812272 , time_score:  500 , memory:  496326\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward 1.5278524413507633 , mean_reward:  21.586074022530205 , time_score:  500 , memory:  498826\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward 18.143633041448464 , mean_reward:  22.84332135814179 , time_score:  500 , memory:  501326\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward -44.51047081718771 , mean_reward:  21.376159986850443 , time_score:  500 , memory:  503826\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward 28.61522837964874 , mean_reward:  22.99685025867068 , time_score:  500 , memory:  506326\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward 80.74146578569072 , mean_reward:  24.464847265551455 , time_score:  500 , memory:  508826\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 94.63883765426301 , mean_reward:  24.666130627167377 , time_score:  500 , memory:  511326\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward -12.300801919707004 , mean_reward:  26.79501337687771 , time_score:  500 , memory:  513826\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward -42.6427723785537 , mean_reward:  25.187661290913454 , time_score:  500 , memory:  516326\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward 49.2674167495719 , mean_reward:  26.978290473301808 , time_score:  500 , memory:  518826\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward 13.117922179205912 , mean_reward:  29.21173288865878 , time_score:  500 , memory:  521326\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward -6.086714545400364 , mean_reward:  31.30538474598076 , time_score:  500 , memory:  523826\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward 14.109686077401904 , mean_reward:  32.206286733089826 , time_score:  500 , memory:  526326\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward 14.795630377026104 , mean_reward:  31.594339227475697 , time_score:  500 , memory:  528826\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward -16.257669565232597 , mean_reward:  31.4065809509922 , time_score:  500 , memory:  531326\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 11.176571194443257 , mean_reward:  31.256500959407223 , time_score:  500 , memory:  533826\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward 37.92932540916949 , mean_reward:  31.237973030914528 , time_score:  500 , memory:  536326\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward 14.122030918704837 , mean_reward:  28.95745996154751 , time_score:  500 , memory:  538826\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward 79.35444199206015 , mean_reward:  29.641499021031695 , time_score:  500 , memory:  541326\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward 21.868651420039697 , mean_reward:  29.05092323009663 , time_score:  500 , memory:  543826\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward -3.8957164343847763 , mean_reward:  29.150280715277468 , time_score:  500 , memory:  546326\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 69.00548422272665 , mean_reward:  29.286783154845143 , time_score:  500 , memory:  548826\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 12.514043333562483 , mean_reward:  28.276837792874502 , time_score:  500 , memory:  551326\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 16.33404803258186 , mean_reward:  29.12886450178023 , time_score:  500 , memory:  553826\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward 74.26882310447192 , mean_reward:  29.946517569109645 , time_score:  500 , memory:  556318\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward 114.68016528536646 , mean_reward:  29.974069656060493 , time_score:  500 , memory:  558818\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 14.626100898186138 , mean_reward:  29.585104653849545 , time_score:  500 , memory:  561318\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward 42.05885431247495 , mean_reward:  28.06690432893545 , time_score:  500 , memory:  563818\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward -39.79894694371504 , mean_reward:  27.442657887449435 , time_score:  500 , memory:  566318\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward -5.785826602238546 , mean_reward:  26.192579554489654 , time_score:  500 , memory:  568818\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward 16.534979613812396 , mean_reward:  24.11130702235533 , time_score:  500 , memory:  571318\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward 24.42144915611316 , mean_reward:  22.878598762121364 , time_score:  500 , memory:  573818\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward -27.070441241528847 , mean_reward:  22.03205982670519 , time_score:  500 , memory:  576318\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward 63.65073449963276 , mean_reward:  23.80569241634601 , time_score:  500 , memory:  578818\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward -30.852194167857906 , mean_reward:  23.230389334542977 , time_score:  500 , memory:  581318\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward 45.04250114450568 , mean_reward:  24.03888177619302 , time_score:  500 , memory:  583818\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward 4.730202589739237 , mean_reward:  21.772394800527646 , time_score:  500 , memory:  586318\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 13.880007566432038 , mean_reward:  22.903435363125617 , time_score:  500 , memory:  588818\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward -11.835944829522326 , mean_reward:  20.097653720240952 , time_score:  500 , memory:  591318\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward 38.73408581900129 , mean_reward:  21.43825499740435 , time_score:  500 , memory:  593818\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward -6.558090672471329 , mean_reward:  20.69517335322366 , time_score:  500 , memory:  596318\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward -9.873060711595137 , mean_reward:  19.320839738178236 , time_score:  500 , memory:  598818\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward -9.468604405897057 , mean_reward:  18.790811296834907 , time_score:  500 , memory:  601318\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward 65.53821624964623 , mean_reward:  18.66388689172086 , time_score:  500 , memory:  603818\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward -35.35059821182301 , mean_reward:  16.103188462331474 , time_score:  500 , memory:  606318\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward 13.327383473543208 , mean_reward:  15.32410984330791 , time_score:  500 , memory:  608818\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward -12.844018506943854 , mean_reward:  14.847493936048988 , time_score:  500 , memory:  611318\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward -18.055504091817898 , mean_reward:  15.069881807217085 , time_score:  500 , memory:  613818\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward -20.14106108116071 , mean_reward:  16.569706274071596 , time_score:  500 , memory:  616318\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward 15.963423622394336 , mean_reward:  13.883677213419357 , time_score:  500 , memory:  618818\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward 45.38253316835063 , mean_reward:  13.810388228985541 , time_score:  500 , memory:  621318\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward -28.8333707130903 , mean_reward:  13.077630021234258 , time_score:  500 , memory:  623818\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward -27.825155594525338 , mean_reward:  12.623211696430646 , time_score:  500 , memory:  626318\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward -18.49852671559443 , mean_reward:  9.533061787551805 , time_score:  500 , memory:  628818\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward -20.696434561838494 , mean_reward:  8.653168720768896 , time_score:  500 , memory:  631318\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward 3.222527045739392 , mean_reward:  6.826960647964641 , time_score:  500 , memory:  633818\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward 57.125370215435304 , mean_reward:  6.428890238670869 , time_score:  500 , memory:  636318\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward 22.021804346629434 , mean_reward:  7.431799915609926 , time_score:  500 , memory:  638818\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward -16.11139796638747 , mean_reward:  8.235727309118667 , time_score:  500 , memory:  641318\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward 14.384565618787356 , mean_reward:  6.5494900968292065 , time_score:  500 , memory:  643818\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward -17.112093015886522 , mean_reward:  7.201533054681803 , time_score:  500 , memory:  646318\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward 58.730940238913476 , mean_reward:  8.672768637691222 , time_score:  500 , memory:  648818\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward 1.4938055717653442 , mean_reward:  10.047885085049764 , time_score:  500 , memory:  651318\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward -34.02971630010917 , mean_reward:  9.271175891753538 , time_score:  500 , memory:  653818\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward 44.325600618249325 , mean_reward:  8.821278849938716 , time_score:  500 , memory:  656318\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward 20.210593722680795 , mean_reward:  7.368773319630144 , time_score:  500 , memory:  658663\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward -10.380427739379295 , mean_reward:  7.9752553382867095 , time_score:  500 , memory:  661163\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward 40.68810291591062 , mean_reward:  10.695683765622901 , time_score:  500 , memory:  663663\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward -19.028949811016336 , mean_reward:  9.7643339466422 , time_score:  500 , memory:  666163\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward 9.950277629419228 , mean_reward:  10.35292450546225 , time_score:  500 , memory:  668663\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward -15.771228679132236 , mean_reward:  9.871058420775604 , time_score:  500 , memory:  671163\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward 32.94038561488229 , mean_reward:  11.42205221035906 , time_score:  500 , memory:  673606\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward -18.19804703154426 , mean_reward:  11.809142622324705 , time_score:  500 , memory:  676106\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward 11.215066261793265 , mean_reward:  12.262448329932633 , time_score:  500 , memory:  678606\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward -7.349524545297076 , mean_reward:  13.294277501742663 , time_score:  500 , memory:  681106\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward 6.5188713282901345 , mean_reward:  13.480233423633013 , time_score:  500 , memory:  683606\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward -7.556726686488181 , mean_reward:  13.935691551989345 , time_score:  500 , memory:  686106\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward -29.606382942795804 , mean_reward:  12.844909035826845 , time_score:  500 , memory:  688606\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward 113.19793943576471 , mean_reward:  14.539788267189733 , time_score:  500 , memory:  691106\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward 4.57080118304751 , mean_reward:  14.746752391795077 , time_score:  500 , memory:  693606\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward -11.839832140871131 , mean_reward:  14.39952362660787 , time_score:  500 , memory:  696106\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward 36.20270656424357 , mean_reward:  14.237932286337077 , time_score:  500 , memory:  698606\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward 2.4675163233069934 , mean_reward:  13.646792855845773 , time_score:  500 , memory:  701106\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward -13.80218075166386 , mean_reward:  13.427936147345505 , time_score:  500 , memory:  703606\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward 63.4931349671309 , mean_reward:  14.93694626205713 , time_score:  500 , memory:  706106\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward 18.711568477814385 , mean_reward:  13.741204676705827 , time_score:  500 , memory:  708606\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward 6.450318684819278 , mean_reward:  13.340140585501556 , time_score:  500 , memory:  711106\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward -2.2643807462137673 , mean_reward:  11.056398439380166 , time_score:  500 , memory:  713606\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward 7.197738496236478 , mean_reward:  12.242832588060397 , time_score:  500 , memory:  716106\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward 13.473309806961044 , mean_reward:  13.175323982427164 , time_score:  500 , memory:  718606\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward -14.54368867049455 , mean_reward:  13.292874574674027 , time_score:  500 , memory:  721106\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward -28.69041058387487 , mean_reward:  12.351728823221158 , time_score:  500 , memory:  723606\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward -17.10823782979667 , mean_reward:  12.129274440070798 , time_score:  500 , memory:  726106\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward 120.28283947820022 , mean_reward:  13.113927690402956 , time_score:  500 , memory:  728606\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward 82.65610335560152 , mean_reward:  13.793978145287724 , time_score:  500 , memory:  731106\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward -8.297801925159385 , mean_reward:  14.425233756316002 , time_score:  500 , memory:  733606\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward -20.883546030895584 , mean_reward:  15.308977512715382 , time_score:  500 , memory:  736106\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward 50.64029278387137 , mean_reward:  15.635719913824211 , time_score:  500 , memory:  738606\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward -9.155344708501985 , mean_reward:  13.384537704936307 , time_score:  500 , memory:  741106\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward 46.06638926572415 , mean_reward:  12.256583407264461 , time_score:  500 , memory:  743606\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward 106.82657126105751 , mean_reward:  14.765443514385392 , time_score:  500 , memory:  746106\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward 23.35777363350828 , mean_reward:  14.007709760883253 , time_score:  500 , memory:  748606\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward 13.684269589204346 , mean_reward:  15.408486993926665 , time_score:  500 , memory:  751106\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 83.4338189079421 , mean_reward:  16.410536960435465 , time_score:  500 , memory:  753606\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward 6.52309911895344 , mean_reward:  14.915560712364243 , time_score:  500 , memory:  756106\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward 19.44642556825872 , mean_reward:  17.653362189966842 , time_score:  500 , memory:  758606\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward -41.04280363417332 , mean_reward:  18.35017767608065 , time_score:  500 , memory:  761106\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward 83.48292112266046 , mean_reward:  18.041749901678504 , time_score:  500 , memory:  763606\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward -22.83480149372965 , mean_reward:  15.915917715107016 , time_score:  500 , memory:  766106\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward 46.15615771183947 , mean_reward:  16.098084175082356 , time_score:  500 , memory:  768606\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward -4.166546444789474 , mean_reward:  14.207816197604933 , time_score:  500 , memory:  771106\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward 15.687461575639947 , mean_reward:  14.70852837807298 , time_score:  500 , memory:  773606\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward 38.538387992949666 , mean_reward:  15.493832720226333 , time_score:  500 , memory:  776106\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward 16.25669097453152 , mean_reward:  14.495766114621636 , time_score:  500 , memory:  778606\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward -16.20162540865408 , mean_reward:  14.026405108777077 , time_score:  500 , memory:  781106\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward 17.310449314551406 , mean_reward:  14.271776436484402 , time_score:  500 , memory:  783606\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward 59.279172342784406 , mean_reward:  15.113344355911465 , time_score:  500 , memory:  786106\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward 9.307376733504562 , mean_reward:  14.449425524903946 , time_score:  500 , memory:  788606\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward -49.11807022353139 , mean_reward:  13.821453994584344 , time_score:  500 , memory:  791106\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward 26.86168996972283 , mean_reward:  13.781848216383933 , time_score:  500 , memory:  793606\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward 7.070995882881306 , mean_reward:  12.188502386869255 , time_score:  500 , memory:  796106\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward -23.127344754599005 , mean_reward:  12.072558965104268 , time_score:  500 , memory:  798606\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward -55.473493488557835 , mean_reward:  9.28900809740218 , time_score:  500 , memory:  801106\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward 60.59620031730295 , mean_reward:  10.129203469422272 , time_score:  500 , memory:  803606\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward -19.75983476210182 , mean_reward:  9.623831889402865 , time_score:  500 , memory:  806106\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward 0.5289055551280517 , mean_reward:  6.700822926745662 , time_score:  500 , memory:  808606\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward 37.106359992449086 , mean_reward:  7.247194191732754 , time_score:  500 , memory:  811106\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward 45.88741501491533 , mean_reward:  7.2008852894273625 , time_score:  500 , memory:  813606\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward -9.66082782039849 , mean_reward:  7.177265205419247 , time_score:  500 , memory:  816106\n",
      "Episode:  1780  , Epsilon:  0.01 , Reward -40.40961872404863 , mean_reward:  5.489556161541585 , time_score:  500 , memory:  818606\n",
      "Episode:  1785  , Epsilon:  0.01 , Reward 51.304968923320615 , mean_reward:  8.068010851282457 , time_score:  500 , memory:  821106\n",
      "Episode:  1790  , Epsilon:  0.01 , Reward 16.873482140977757 , mean_reward:  7.71185616723648 , time_score:  500 , memory:  823606\n",
      "Episode:  1795  , Epsilon:  0.01 , Reward 105.39293733342852 , mean_reward:  7.774688649819504 , time_score:  500 , memory:  826106\n",
      "Episode:  1800  , Epsilon:  0.01 , Reward -0.4663331450105143 , mean_reward:  7.820792315722501 , time_score:  500 , memory:  828606\n",
      "Episode:  1805  , Epsilon:  0.01 , Reward -19.877093863802667 , mean_reward:  6.634530514534195 , time_score:  500 , memory:  831106\n",
      "Episode:  1810  , Epsilon:  0.01 , Reward 89.44431541515473 , mean_reward:  7.39667273593404 , time_score:  500 , memory:  833606\n",
      "Episode:  1815  , Epsilon:  0.01 , Reward -6.835737477480973 , mean_reward:  5.8026614753661985 , time_score:  500 , memory:  836106\n",
      "Episode:  1820  , Epsilon:  0.01 , Reward -22.027261681039835 , mean_reward:  5.295725094317044 , time_score:  500 , memory:  838606\n",
      "Episode:  1825  , Epsilon:  0.01 , Reward -34.46138286910956 , mean_reward:  5.927396417902819 , time_score:  500 , memory:  841106\n",
      "Episode:  1830  , Epsilon:  0.01 , Reward -34.75944228333544 , mean_reward:  5.582271249275007 , time_score:  500 , memory:  843606\n",
      "Episode:  1835  , Epsilon:  0.01 , Reward -20.046834543728263 , mean_reward:  3.7233597303965946 , time_score:  500 , memory:  846106\n",
      "Episode:  1840  , Epsilon:  0.01 , Reward -17.278355185746626 , mean_reward:  3.7032298463291315 , time_score:  500 , memory:  848606\n",
      "Episode:  1845  , Epsilon:  0.01 , Reward -6.522867342238999 , mean_reward:  4.407212132080976 , time_score:  500 , memory:  851106\n",
      "Episode:  1850  , Epsilon:  0.01 , Reward -1.6180235734446617 , mean_reward:  3.58253484447403 , time_score:  500 , memory:  853606\n",
      "Episode:  1855  , Epsilon:  0.01 , Reward 3.3264020533359533 , mean_reward:  3.762751342587015 , time_score:  500 , memory:  856106\n",
      "Episode:  1860  , Epsilon:  0.01 , Reward -46.477509261189255 , mean_reward:  5.193761912088462 , time_score:  500 , memory:  858606\n",
      "Episode:  1865  , Epsilon:  0.01 , Reward 16.869982595459877 , mean_reward:  5.0070307514402295 , time_score:  500 , memory:  861106\n",
      "Episode:  1870  , Epsilon:  0.01 , Reward -22.56377622462934 , mean_reward:  5.863076063215433 , time_score:  500 , memory:  863606\n",
      "Episode:  1875  , Epsilon:  0.01 , Reward 27.285589083116697 , mean_reward:  7.225299678950714 , time_score:  500 , memory:  866106\n",
      "Episode:  1880  , Epsilon:  0.01 , Reward -24.489244145543978 , mean_reward:  7.6053585530922145 , time_score:  500 , memory:  868606\n",
      "Episode:  1885  , Epsilon:  0.01 , Reward -13.563097592307933 , mean_reward:  5.764063101341403 , time_score:  500 , memory:  871106\n",
      "Episode:  1890  , Epsilon:  0.01 , Reward -4.426908056987682 , mean_reward:  5.863475910975193 , time_score:  500 , memory:  873606\n",
      "Episode:  1895  , Epsilon:  0.01 , Reward 67.56680762278818 , mean_reward:  5.602545110550173 , time_score:  500 , memory:  876106\n",
      "Episode:  1900  , Epsilon:  0.01 , Reward -6.595477475954629 , mean_reward:  5.1094725129391385 , time_score:  500 , memory:  878606\n",
      "Episode:  1905  , Epsilon:  0.01 , Reward 14.191693584022957 , mean_reward:  7.245219489696202 , time_score:  500 , memory:  881106\n",
      "Episode:  1910  , Epsilon:  0.01 , Reward 13.077444799852312 , mean_reward:  6.0991444518321005 , time_score:  500 , memory:  883606\n",
      "Episode:  1915  , Epsilon:  0.01 , Reward -10.587547477165256 , mean_reward:  6.485086088049675 , time_score:  500 , memory:  886106\n",
      "Episode:  1920  , Epsilon:  0.01 , Reward -27.21009965683803 , mean_reward:  7.745557522875734 , time_score:  500 , memory:  888606\n",
      "Episode:  1925  , Epsilon:  0.01 , Reward 47.4708231035102 , mean_reward:  9.579016921605328 , time_score:  500 , memory:  891106\n",
      "Episode:  1930  , Epsilon:  0.01 , Reward 18.24872362867358 , mean_reward:  10.500112923790105 , time_score:  500 , memory:  893606\n",
      "Episode:  1935  , Epsilon:  0.01 , Reward -32.23332591039035 , mean_reward:  10.597134014489562 , time_score:  500 , memory:  896106\n",
      "Episode:  1940  , Epsilon:  0.01 , Reward 70.58269760909181 , mean_reward:  11.813632935777823 , time_score:  500 , memory:  898606\n",
      "Episode:  1945  , Epsilon:  0.01 , Reward -17.268150347030332 , mean_reward:  12.207334629362153 , time_score:  500 , memory:  901106\n",
      "Episode:  1950  , Epsilon:  0.01 , Reward 47.10332102112138 , mean_reward:  12.543913623028015 , time_score:  500 , memory:  903606\n",
      "Episode:  1955  , Epsilon:  0.01 , Reward 10.943358314869444 , mean_reward:  13.045869795945766 , time_score:  500 , memory:  906106\n",
      "Episode:  1960  , Epsilon:  0.01 , Reward 33.698537625115584 , mean_reward:  13.493567852909607 , time_score:  500 , memory:  908606\n",
      "Episode:  1965  , Epsilon:  0.01 , Reward 90.80189580260081 , mean_reward:  12.045734756490464 , time_score:  500 , memory:  911106\n",
      "Episode:  1970  , Epsilon:  0.01 , Reward 39.511200177217724 , mean_reward:  12.4789337233997 , time_score:  500 , memory:  913606\n",
      "Episode:  1975  , Epsilon:  0.01 , Reward 7.771183086592632 , mean_reward:  12.920355372446062 , time_score:  500 , memory:  916106\n",
      "Episode:  1980  , Epsilon:  0.01 , Reward 4.941584891868715 , mean_reward:  13.577819375969598 , time_score:  500 , memory:  918606\n",
      "Episode:  1985  , Epsilon:  0.01 , Reward -9.474001898876883 , mean_reward:  14.44651348951734 , time_score:  500 , memory:  921106\n",
      "Episode:  1990  , Epsilon:  0.01 , Reward 73.22833979793869 , mean_reward:  15.089907032833175 , time_score:  500 , memory:  923606\n",
      "Episode:  1995  , Epsilon:  0.01 , Reward 25.455193181866886 , mean_reward:  15.172929504393094 , time_score:  500 , memory:  926106\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.001, lr=0.0005)\n",
    "R, R_moving = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
