{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"ep_90_0p001_0p0005.csv\"\n",
    "        self.model_filename = \"ep_90_0p001_0p0005.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "                self.ep *= self.ep_decay\n",
    "            else:\n",
    "                self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "                \n",
    "        with open(self.csv_filename, 'a') as f:\n",
    "            self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return self.df_ddqn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -361.52841470422703 , mean_reward:  -361.52841470422703 , time_score:  111 , memory:  111\n",
      "Episode:  5  , Epsilon:  0.7737809374999999 , Reward -611.5362206365564 , mean_reward:  -257.7425217460358 , time_score:  131 , memory:  566\n",
      "Episode:  10  , Epsilon:  0.5987369392383786 , Reward -398.49784411791 , mean_reward:  -268.4818577379064 , time_score:  90 , memory:  1035\n",
      "Episode:  15  , Epsilon:  0.46329123015975293 , Reward -99.47473624383647 , mean_reward:  -232.77704918327913 , time_score:  123 , memory:  1610\n",
      "Episode:  20  , Epsilon:  0.35848592240854177 , Reward -403.928654834483 , mean_reward:  -255.96132451207856 , time_score:  252 , memory:  2530\n",
      "Episode:  25  , Epsilon:  0.27738957312183365 , Reward -146.54942945778333 , mean_reward:  -257.1475269530902 , time_score:  218 , memory:  3400\n",
      "Episode:  30  , Epsilon:  0.2146387639429372 , Reward -375.479485495431 , mean_reward:  -264.45399701912135 , time_score:  184 , memory:  4440\n",
      "Episode:  35  , Epsilon:  0.16608338398760714 , Reward 47.69977525486812 , mean_reward:  -242.6291262354919 , time_score:  500 , memory:  6661\n",
      "Episode:  40  , Epsilon:  0.1285121565651031 , Reward -194.1062839908435 , mean_reward:  -250.27375622316166 , time_score:  364 , memory:  7921\n",
      "Episode:  45  , Epsilon:  0.09944025698709223 , Reward -66.85886172908596 , mean_reward:  -238.8684514024028 , time_score:  313 , memory:  9287\n",
      "Episode:  50  , Epsilon:  0.07694497527671314 , Reward -184.27847947916746 , mean_reward:  -230.87982565890002 , time_score:  280 , memory:  10317\n",
      "Episode:  55  , Epsilon:  0.059538555105529384 , Reward -58.21094884631983 , mean_reward:  -234.7628273132969 , time_score:  127 , memory:  11343\n",
      "Episode:  60  , Epsilon:  0.04606979898695193 , Reward -389.1474315595415 , mean_reward:  -243.0438256871623 , time_score:  267 , memory:  12467\n",
      "Episode:  65  , Epsilon:  0.03564793225056021 , Reward 54.758916945122635 , mean_reward:  -247.33129811693166 , time_score:  500 , memory:  14184\n",
      "Episode:  70  , Epsilon:  0.027583690436774957 , Reward -253.01883732395598 , mean_reward:  -243.45002862055415 , time_score:  255 , memory:  15524\n",
      "Episode:  75  , Epsilon:  0.021343733845877507 , Reward -341.9049682822573 , mean_reward:  -240.7141435277259 , time_score:  401 , memory:  17554\n",
      "Episode:  80  , Epsilon:  0.016515374385013576 , Reward -439.6267528069218 , mean_reward:  -244.98560679096863 , time_score:  280 , memory:  19200\n",
      "Episode:  85  , Epsilon:  0.012779281874799287 , Reward -37.19357859583633 , mean_reward:  -241.3853172132781 , time_score:  265 , memory:  20909\n",
      "Episode:  90  , Epsilon:  0.009888364709658948 , Reward -216.92259741395205 , mean_reward:  -239.37182847960736 , time_score:  315 , memory:  22707\n",
      "Episode:  95  , Epsilon:  0.01 , Reward -446.64166072681485 , mean_reward:  -241.03673741293164 , time_score:  316 , memory:  24471\n",
      "Episode:  100  , Epsilon:  0.01 , Reward -210.67940812356872 , mean_reward:  -236.61136081085724 , time_score:  285 , memory:  25874\n",
      "Episode:  105  , Epsilon:  0.01 , Reward -104.71916266502616 , mean_reward:  -235.19099970945052 , time_score:  390 , memory:  27500\n",
      "Episode:  110  , Epsilon:  0.01 , Reward -165.5385996526252 , mean_reward:  -225.79327324344314 , time_score:  242 , memory:  29742\n",
      "Episode:  115  , Epsilon:  0.01 , Reward -244.16850719842046 , mean_reward:  -222.77125501231637 , time_score:  469 , memory:  32127\n",
      "Episode:  120  , Epsilon:  0.01 , Reward -83.55357657613945 , mean_reward:  -218.1311507414705 , time_score:  500 , memory:  33791\n",
      "Episode:  125  , Epsilon:  0.01 , Reward -244.4800437169396 , mean_reward:  -211.97905721186197 , time_score:  425 , memory:  35708\n",
      "Episode:  130  , Epsilon:  0.01 , Reward -68.81333320803013 , mean_reward:  -203.57933757073585 , time_score:  500 , memory:  38040\n",
      "Episode:  135  , Epsilon:  0.01 , Reward -149.22839811819665 , mean_reward:  -200.7251363134305 , time_score:  240 , memory:  40280\n",
      "Episode:  140  , Epsilon:  0.01 , Reward -77.0643543504197 , mean_reward:  -186.48897413667638 , time_score:  500 , memory:  42780\n",
      "Episode:  145  , Epsilon:  0.01 , Reward 8.154735131942674 , mean_reward:  -179.18948557844593 , time_score:  500 , memory:  45280\n",
      "Episode:  150  , Epsilon:  0.01 , Reward 12.978155901550384 , mean_reward:  -172.69197906769048 , time_score:  500 , memory:  47780\n",
      "Episode:  155  , Epsilon:  0.01 , Reward 83.96482320847562 , mean_reward:  -160.09680630423836 , time_score:  500 , memory:  50280\n",
      "Episode:  160  , Epsilon:  0.01 , Reward -28.95361389771135 , mean_reward:  -144.22897214817328 , time_score:  500 , memory:  52780\n",
      "Episode:  165  , Epsilon:  0.01 , Reward -9.852573836024835 , mean_reward:  -130.45823874969176 , time_score:  500 , memory:  55280\n",
      "Episode:  170  , Epsilon:  0.01 , Reward -5.945800439434874 , mean_reward:  -121.54576170428885 , time_score:  500 , memory:  57780\n",
      "Episode:  175  , Epsilon:  0.01 , Reward -39.62695721935684 , mean_reward:  -112.89699034153223 , time_score:  500 , memory:  60280\n",
      "Episode:  180  , Epsilon:  0.01 , Reward -32.709359584941204 , mean_reward:  -98.63727954359298 , time_score:  500 , memory:  62520\n",
      "Episode:  185  , Epsilon:  0.01 , Reward 21.65100666524062 , mean_reward:  -89.7699760173795 , time_score:  500 , memory:  65020\n",
      "Episode:  190  , Epsilon:  0.01 , Reward -3.2924706034947744 , mean_reward:  -80.5884743699311 , time_score:  500 , memory:  67520\n",
      "Episode:  195  , Epsilon:  0.01 , Reward 14.4199553686334 , mean_reward:  -67.57388553120018 , time_score:  500 , memory:  70020\n",
      "Episode:  200  , Epsilon:  0.01 , Reward -0.0608228111572382 , mean_reward:  -59.22371083323034 , time_score:  500 , memory:  72520\n",
      "Episode:  205  , Epsilon:  0.01 , Reward -43.12538271517904 , mean_reward:  -49.528075310636744 , time_score:  500 , memory:  75020\n",
      "Episode:  210  , Epsilon:  0.01 , Reward 1.9294310197844748 , mean_reward:  -46.02075890610012 , time_score:  500 , memory:  77520\n",
      "Episode:  215  , Epsilon:  0.01 , Reward 20.71486961196081 , mean_reward:  -41.85719533483417 , time_score:  500 , memory:  80020\n",
      "Episode:  220  , Epsilon:  0.01 , Reward 17.201066935449138 , mean_reward:  -30.723001555885457 , time_score:  500 , memory:  82520\n",
      "Episode:  225  , Epsilon:  0.01 , Reward -105.5103301875971 , mean_reward:  -26.388237373561036 , time_score:  64 , memory:  84584\n",
      "Episode:  230  , Epsilon:  0.01 , Reward -22.609172070041666 , mean_reward:  -21.22194305770552 , time_score:  500 , memory:  87084\n",
      "Episode:  235  , Epsilon:  0.01 , Reward 0.08739789433959855 , mean_reward:  -20.225194655563655 , time_score:  500 , memory:  89381\n",
      "Episode:  240  , Epsilon:  0.01 , Reward -30.218695330716677 , mean_reward:  -20.598016789918788 , time_score:  500 , memory:  91881\n",
      "Episode:  245  , Epsilon:  0.01 , Reward -132.11316743763746 , mean_reward:  -24.816124722019552 , time_score:  434 , memory:  94258\n",
      "Episode:  250  , Epsilon:  0.01 , Reward -186.6722590147927 , mean_reward:  -28.0760607721272 , time_score:  310 , memory:  96558\n",
      "Episode:  255  , Epsilon:  0.01 , Reward 10.840841008950559 , mean_reward:  -28.35220123533255 , time_score:  500 , memory:  99058\n",
      "Episode:  260  , Epsilon:  0.01 , Reward -57.82047907538445 , mean_reward:  -30.114140833586443 , time_score:  500 , memory:  101232\n",
      "Episode:  265  , Epsilon:  0.01 , Reward -37.9718727324593 , mean_reward:  -32.65001720440982 , time_score:  500 , memory:  103539\n",
      "Episode:  270  , Epsilon:  0.01 , Reward -4.921999686890445 , mean_reward:  -32.63464856129366 , time_score:  500 , memory:  106039\n",
      "Episode:  275  , Epsilon:  0.01 , Reward -3.6061327820326126 , mean_reward:  -32.52886146780103 , time_score:  500 , memory:  108539\n",
      "Episode:  280  , Epsilon:  0.01 , Reward -6.867172498527009 , mean_reward:  -34.132644838776976 , time_score:  500 , memory:  110833\n",
      "Episode:  285  , Epsilon:  0.01 , Reward -36.07531996773403 , mean_reward:  -34.635421401711994 , time_score:  500 , memory:  113333\n",
      "Episode:  290  , Epsilon:  0.01 , Reward -51.658366208671104 , mean_reward:  -36.95536493959728 , time_score:  500 , memory:  115717\n",
      "Episode:  295  , Epsilon:  0.01 , Reward -58.20702116168607 , mean_reward:  -37.81162290916254 , time_score:  500 , memory:  118217\n",
      "Episode:  300  , Epsilon:  0.01 , Reward -100.84931573778046 , mean_reward:  -39.33970096745141 , time_score:  145 , memory:  120362\n",
      "Episode:  305  , Epsilon:  0.01 , Reward -116.99707182811991 , mean_reward:  -43.20507433266931 , time_score:  388 , memory:  122146\n",
      "Episode:  310  , Epsilon:  0.01 , Reward -20.090820195667924 , mean_reward:  -41.84366294884184 , time_score:  500 , memory:  124646\n",
      "Episode:  315  , Epsilon:  0.01 , Reward -52.22493446726308 , mean_reward:  -46.32407550148599 , time_score:  500 , memory:  126259\n",
      "Episode:  320  , Epsilon:  0.01 , Reward -44.95063832538574 , mean_reward:  -46.993412421731556 , time_score:  500 , memory:  128759\n",
      "Episode:  325  , Epsilon:  0.01 , Reward -0.021272081628298523 , mean_reward:  -45.331114630302324 , time_score:  500 , memory:  131259\n",
      "Episode:  330  , Epsilon:  0.01 , Reward -20.460439361176277 , mean_reward:  -44.765009965696756 , time_score:  500 , memory:  133364\n",
      "Episode:  335  , Epsilon:  0.01 , Reward -85.6111769825544 , mean_reward:  -45.50901614997399 , time_score:  267 , memory:  135631\n",
      "Episode:  340  , Epsilon:  0.01 , Reward 28.109813338013378 , mean_reward:  -46.94941038813802 , time_score:  500 , memory:  137658\n",
      "Episode:  345  , Epsilon:  0.01 , Reward 4.019029713977709 , mean_reward:  -46.208019272045334 , time_score:  500 , memory:  138883\n",
      "Episode:  350  , Epsilon:  0.01 , Reward 53.968668709327034 , mean_reward:  -41.733428709439565 , time_score:  500 , memory:  141383\n",
      "Episode:  355  , Epsilon:  0.01 , Reward -1.3327945467050613 , mean_reward:  -42.19950639669739 , time_score:  500 , memory:  143528\n",
      "Episode:  360  , Epsilon:  0.01 , Reward -81.01440951927216 , mean_reward:  -41.91951417761846 , time_score:  245 , memory:  145475\n",
      "Episode:  365  , Epsilon:  0.01 , Reward -28.555153539003545 , mean_reward:  -37.073096031637796 , time_score:  500 , memory:  147975\n",
      "Episode:  370  , Epsilon:  0.01 , Reward -66.14913617051096 , mean_reward:  -38.63445969452893 , time_score:  197 , memory:  150016\n",
      "Episode:  375  , Epsilon:  0.01 , Reward -174.00680266468748 , mean_reward:  -40.074411149248334 , time_score:  497 , memory:  152155\n",
      "Episode:  380  , Epsilon:  0.01 , Reward 46.4418328320111 , mean_reward:  -40.213493418934014 , time_score:  500 , memory:  153857\n",
      "Episode:  385  , Epsilon:  0.01 , Reward -79.04258398735905 , mean_reward:  -41.15437062580949 , time_score:  132 , memory:  155958\n",
      "Episode:  390  , Epsilon:  0.01 , Reward -72.59302543447464 , mean_reward:  -39.984306728498524 , time_score:  205 , memory:  157838\n",
      "Episode:  395  , Epsilon:  0.01 , Reward -23.023109661641254 , mean_reward:  -40.922658476768234 , time_score:  500 , memory:  159761\n",
      "Episode:  400  , Epsilon:  0.01 , Reward 42.445355801354516 , mean_reward:  -39.4268051324658 , time_score:  500 , memory:  161913\n",
      "Episode:  405  , Epsilon:  0.01 , Reward 8.456748982803601 , mean_reward:  -33.178522398002045 , time_score:  500 , memory:  164413\n",
      "Episode:  410  , Epsilon:  0.01 , Reward -84.55353165707656 , mean_reward:  -34.32505519841385 , time_score:  210 , memory:  166623\n",
      "Episode:  415  , Epsilon:  0.01 , Reward -35.07133975231634 , mean_reward:  -29.599847686811994 , time_score:  500 , memory:  169123\n",
      "Episode:  420  , Epsilon:  0.01 , Reward -92.94006570377091 , mean_reward:  -29.787306849265374 , time_score:  81 , memory:  171204\n",
      "Episode:  425  , Epsilon:  0.01 , Reward -126.69427487472707 , mean_reward:  -31.46230552393122 , time_score:  106 , memory:  172969\n",
      "Episode:  430  , Epsilon:  0.01 , Reward 34.51453534268208 , mean_reward:  -32.68247828906752 , time_score:  500 , memory:  174742\n",
      "Episode:  435  , Epsilon:  0.01 , Reward -109.08947984820699 , mean_reward:  -31.131676828582204 , time_score:  119 , memory:  176861\n",
      "Episode:  440  , Epsilon:  0.01 , Reward -3.1873796599379 , mean_reward:  -28.08509467009527 , time_score:  500 , memory:  179361\n",
      "Episode:  445  , Epsilon:  0.01 , Reward 25.57685069124627 , mean_reward:  -25.489456975971116 , time_score:  500 , memory:  181275\n",
      "Episode:  450  , Epsilon:  0.01 , Reward -129.04379790459905 , mean_reward:  -26.724698179739345 , time_score:  123 , memory:  183012\n",
      "Episode:  455  , Epsilon:  0.01 , Reward 35.54749418728735 , mean_reward:  -24.887441904901067 , time_score:  500 , memory:  185512\n",
      "Episode:  460  , Epsilon:  0.01 , Reward 35.15062976714104 , mean_reward:  -25.03353874797787 , time_score:  500 , memory:  187303\n",
      "Episode:  465  , Epsilon:  0.01 , Reward 35.99705672370678 , mean_reward:  -29.171747794088603 , time_score:  500 , memory:  189424\n",
      "Episode:  470  , Epsilon:  0.01 , Reward -5.137357701206624 , mean_reward:  -27.97273261037788 , time_score:  500 , memory:  191924\n",
      "Episode:  475  , Epsilon:  0.01 , Reward -134.58359877682585 , mean_reward:  -28.496756612108943 , time_score:  114 , memory:  193360\n",
      "Episode:  480  , Epsilon:  0.01 , Reward 53.65687150887788 , mean_reward:  -28.453409475345275 , time_score:  500 , memory:  194990\n",
      "Episode:  485  , Epsilon:  0.01 , Reward 57.018829881837746 , mean_reward:  -25.689946548780863 , time_score:  500 , memory:  197490\n",
      "Episode:  490  , Epsilon:  0.01 , Reward 81.50597710552476 , mean_reward:  -22.661342629019018 , time_score:  500 , memory:  199990\n",
      "Episode:  495  , Epsilon:  0.01 , Reward -22.75761747980494 , mean_reward:  -23.72413340042232 , time_score:  500 , memory:  201832\n",
      "Episode:  500  , Epsilon:  0.01 , Reward -7.382438070054419 , mean_reward:  -23.032708427751217 , time_score:  500 , memory:  204332\n",
      "Episode:  505  , Epsilon:  0.01 , Reward -4.318949193429819 , mean_reward:  -26.064490903984534 , time_score:  500 , memory:  206832\n",
      "Episode:  510  , Epsilon:  0.01 , Reward 1.384121364452204 , mean_reward:  -25.12213829746765 , time_score:  500 , memory:  209332\n",
      "Episode:  515  , Epsilon:  0.01 , Reward -188.60648240008277 , mean_reward:  -26.625398533915543 , time_score:  375 , memory:  211707\n",
      "Episode:  520  , Epsilon:  0.01 , Reward 77.79526038335068 , mean_reward:  -24.700452036853143 , time_score:  500 , memory:  214207\n",
      "Episode:  525  , Epsilon:  0.01 , Reward -104.46634575304769 , mean_reward:  -22.45383117020744 , time_score:  465 , memory:  216672\n",
      "Episode:  530  , Epsilon:  0.01 , Reward -36.658738452935246 , mean_reward:  -20.5008329801953 , time_score:  500 , memory:  219172\n",
      "Episode:  535  , Epsilon:  0.01 , Reward -1.0753050084594304 , mean_reward:  -18.642563487501967 , time_score:  500 , memory:  221672\n",
      "Episode:  540  , Epsilon:  0.01 , Reward -12.213947876798702 , mean_reward:  -19.227210428854317 , time_score:  500 , memory:  223795\n",
      "Episode:  545  , Epsilon:  0.01 , Reward 49.46370739759634 , mean_reward:  -18.774260001491072 , time_score:  500 , memory:  226126\n",
      "Episode:  550  , Epsilon:  0.01 , Reward 42.40840816004566 , mean_reward:  -19.071300491725324 , time_score:  500 , memory:  228432\n",
      "Episode:  555  , Epsilon:  0.01 , Reward 27.243354251382538 , mean_reward:  -18.757725224978937 , time_score:  500 , memory:  230932\n",
      "Episode:  560  , Epsilon:  0.01 , Reward 4.551662376333318 , mean_reward:  -17.247436000419093 , time_score:  500 , memory:  233284\n",
      "Episode:  565  , Epsilon:  0.01 , Reward 2.909665819349774 , mean_reward:  -14.991481446165736 , time_score:  500 , memory:  235733\n",
      "Episode:  570  , Epsilon:  0.01 , Reward 46.561601224186404 , mean_reward:  -12.733784358350556 , time_score:  500 , memory:  238233\n",
      "Episode:  575  , Epsilon:  0.01 , Reward 21.911738665698653 , mean_reward:  -8.1050986282741 , time_score:  500 , memory:  240733\n",
      "Episode:  580  , Epsilon:  0.01 , Reward 26.360877763513447 , mean_reward:  -4.334887737779808 , time_score:  500 , memory:  243233\n",
      "Episode:  585  , Epsilon:  0.01 , Reward 13.350710205004432 , mean_reward:  -4.449801461729015 , time_score:  500 , memory:  245733\n",
      "Episode:  590  , Epsilon:  0.01 , Reward -27.859204175839338 , mean_reward:  -5.03721416602496 , time_score:  500 , memory:  248233\n",
      "Episode:  595  , Epsilon:  0.01 , Reward -22.46223302646142 , mean_reward:  -3.2620931060368137 , time_score:  500 , memory:  250023\n",
      "Episode:  600  , Epsilon:  0.01 , Reward -12.957829735848684 , mean_reward:  -2.7831694414623165 , time_score:  500 , memory:  252523\n",
      "Episode:  605  , Epsilon:  0.01 , Reward 10.967419382878006 , mean_reward:  -0.44698540126908065 , time_score:  500 , memory:  255023\n",
      "Episode:  610  , Epsilon:  0.01 , Reward 67.23630589851962 , mean_reward:  -0.019170264548131966 , time_score:  500 , memory:  257269\n",
      "Episode:  615  , Epsilon:  0.01 , Reward 6.502212854670301 , mean_reward:  2.0219233778280863 , time_score:  500 , memory:  259769\n",
      "Episode:  620  , Epsilon:  0.01 , Reward 26.54784597702173 , mean_reward:  3.1956260859546415 , time_score:  500 , memory:  262269\n",
      "Episode:  625  , Epsilon:  0.01 , Reward 26.669066204221696 , mean_reward:  4.35821667929284 , time_score:  500 , memory:  264769\n",
      "Episode:  630  , Epsilon:  0.01 , Reward -5.131052980011978 , mean_reward:  5.273412416152162 , time_score:  500 , memory:  267269\n",
      "Episode:  635  , Epsilon:  0.01 , Reward 15.350643640625746 , mean_reward:  4.779375739134861 , time_score:  500 , memory:  269769\n",
      "Episode:  640  , Epsilon:  0.01 , Reward 33.71001583871189 , mean_reward:  6.3503870645154725 , time_score:  500 , memory:  272269\n",
      "Episode:  645  , Epsilon:  0.01 , Reward 5.135018573381734 , mean_reward:  5.119097957128996 , time_score:  500 , memory:  274728\n",
      "Episode:  650  , Epsilon:  0.01 , Reward 13.254517003105962 , mean_reward:  6.938700819614905 , time_score:  500 , memory:  277228\n",
      "Episode:  655  , Epsilon:  0.01 , Reward 23.43275703972296 , mean_reward:  5.995829961260207 , time_score:  500 , memory:  279348\n",
      "Episode:  660  , Epsilon:  0.01 , Reward 15.368817534133894 , mean_reward:  7.740089677327021 , time_score:  500 , memory:  281848\n",
      "Episode:  665  , Epsilon:  0.01 , Reward -97.42675493599167 , mean_reward:  8.023791126631671 , time_score:  343 , memory:  284191\n",
      "Episode:  670  , Epsilon:  0.01 , Reward 15.89174263010763 , mean_reward:  7.478453824050936 , time_score:  500 , memory:  286691\n",
      "Episode:  675  , Epsilon:  0.01 , Reward 21.433181797419138 , mean_reward:  7.239463305903238 , time_score:  500 , memory:  289191\n",
      "Episode:  680  , Epsilon:  0.01 , Reward 35.57309499791066 , mean_reward:  6.829005827891638 , time_score:  500 , memory:  291691\n",
      "Episode:  685  , Epsilon:  0.01 , Reward -7.7161096537607285 , mean_reward:  4.78802302690783 , time_score:  500 , memory:  293847\n",
      "Episode:  690  , Epsilon:  0.01 , Reward -21.343546653860937 , mean_reward:  3.5986339784682166 , time_score:  500 , memory:  296347\n",
      "Episode:  695  , Epsilon:  0.01 , Reward 12.097711540716755 , mean_reward:  5.777699163493939 , time_score:  500 , memory:  298847\n",
      "Episode:  700  , Epsilon:  0.01 , Reward -6.088282282618826 , mean_reward:  4.942876636485437 , time_score:  500 , memory:  301347\n",
      "Episode:  705  , Epsilon:  0.01 , Reward 0.8734963412040082 , mean_reward:  5.042441813885485 , time_score:  500 , memory:  303847\n",
      "Episode:  710  , Epsilon:  0.01 , Reward 29.922109692063703 , mean_reward:  5.684271746782758 , time_score:  500 , memory:  306347\n",
      "Episode:  715  , Epsilon:  0.01 , Reward -12.562226836961047 , mean_reward:  4.0928317335021696 , time_score:  500 , memory:  308703\n",
      "Episode:  720  , Epsilon:  0.01 , Reward -3.358690261443783 , mean_reward:  2.0744540522761845 , time_score:  500 , memory:  311203\n",
      "Episode:  725  , Epsilon:  0.01 , Reward -15.872784502968385 , mean_reward:  1.8655495380185767 , time_score:  500 , memory:  313703\n",
      "Episode:  730  , Epsilon:  0.01 , Reward -37.08531835109894 , mean_reward:  0.9427637903952004 , time_score:  500 , memory:  316203\n",
      "Episode:  735  , Epsilon:  0.01 , Reward 73.09558342912793 , mean_reward:  1.5613370715434052 , time_score:  500 , memory:  318345\n",
      "Episode:  740  , Epsilon:  0.01 , Reward 25.173658492478772 , mean_reward:  0.8001697449945013 , time_score:  500 , memory:  320845\n",
      "Episode:  745  , Epsilon:  0.01 , Reward -41.093683068273684 , mean_reward:  2.2257696281888686 , time_score:  500 , memory:  323345\n",
      "Episode:  750  , Epsilon:  0.01 , Reward -9.504970403143947 , mean_reward:  2.096336538627131 , time_score:  500 , memory:  325845\n",
      "Episode:  755  , Epsilon:  0.01 , Reward 29.814169294253656 , mean_reward:  2.701706371558612 , time_score:  500 , memory:  328345\n",
      "Episode:  760  , Epsilon:  0.01 , Reward 43.45371028006039 , mean_reward:  3.208990589007211 , time_score:  500 , memory:  330845\n",
      "Episode:  765  , Epsilon:  0.01 , Reward 54.78397523970215 , mean_reward:  4.995816176816755 , time_score:  500 , memory:  333345\n",
      "Episode:  770  , Epsilon:  0.01 , Reward 1.278871224331884 , mean_reward:  4.154111889799102 , time_score:  500 , memory:  335845\n",
      "Episode:  775  , Epsilon:  0.01 , Reward -2.6099996627532533 , mean_reward:  3.475692956125166 , time_score:  500 , memory:  338345\n",
      "Episode:  780  , Epsilon:  0.01 , Reward 78.02519629850673 , mean_reward:  3.755773629179702 , time_score:  500 , memory:  340845\n",
      "Episode:  785  , Epsilon:  0.01 , Reward 12.377640310298093 , mean_reward:  5.17881033146171 , time_score:  500 , memory:  343345\n",
      "Episode:  790  , Epsilon:  0.01 , Reward 51.46226042397121 , mean_reward:  6.975099703949992 , time_score:  500 , memory:  345845\n",
      "Episode:  795  , Epsilon:  0.01 , Reward 37.50437053989262 , mean_reward:  6.822607952967379 , time_score:  500 , memory:  348345\n",
      "Episode:  800  , Epsilon:  0.01 , Reward 0.6760514607689088 , mean_reward:  8.809071856432276 , time_score:  500 , memory:  350845\n",
      "Episode:  805  , Epsilon:  0.01 , Reward 22.633304042351554 , mean_reward:  8.291529499176177 , time_score:  500 , memory:  353345\n",
      "Episode:  810  , Epsilon:  0.01 , Reward 26.133323548301036 , mean_reward:  7.48828611262259 , time_score:  500 , memory:  355640\n",
      "Episode:  815  , Epsilon:  0.01 , Reward 22.898469603969673 , mean_reward:  10.444167959510759 , time_score:  500 , memory:  358140\n",
      "Episode:  820  , Epsilon:  0.01 , Reward 51.863091796464396 , mean_reward:  10.405689185940878 , time_score:  500 , memory:  360640\n",
      "Episode:  825  , Epsilon:  0.01 , Reward 0.5271237890413425 , mean_reward:  9.93498087685892 , time_score:  500 , memory:  363140\n",
      "Episode:  830  , Epsilon:  0.01 , Reward -14.893663351612073 , mean_reward:  11.72659183077352 , time_score:  500 , memory:  365640\n",
      "Episode:  835  , Epsilon:  0.01 , Reward 15.211546459591464 , mean_reward:  11.078624824328529 , time_score:  500 , memory:  368140\n",
      "Episode:  840  , Epsilon:  0.01 , Reward 1.271323929667996 , mean_reward:  10.465165185352499 , time_score:  500 , memory:  370385\n",
      "Episode:  845  , Epsilon:  0.01 , Reward 90.4945022370498 , mean_reward:  12.41436087864314 , time_score:  500 , memory:  372885\n",
      "Episode:  850  , Epsilon:  0.01 , Reward -9.497535753129217 , mean_reward:  10.238063532483746 , time_score:  500 , memory:  375146\n",
      "Episode:  855  , Epsilon:  0.01 , Reward 22.878584659219417 , mean_reward:  11.00069048385536 , time_score:  500 , memory:  377646\n",
      "Episode:  860  , Epsilon:  0.01 , Reward -10.309041006199854 , mean_reward:  9.537036882112158 , time_score:  500 , memory:  380146\n",
      "Episode:  865  , Epsilon:  0.01 , Reward -16.05734403359368 , mean_reward:  9.486911159636819 , time_score:  500 , memory:  382646\n",
      "Episode:  870  , Epsilon:  0.01 , Reward 29.619139245505263 , mean_reward:  9.725094049843165 , time_score:  500 , memory:  385146\n",
      "Episode:  875  , Epsilon:  0.01 , Reward -20.328534123915325 , mean_reward:  10.296603100907895 , time_score:  500 , memory:  387646\n",
      "Episode:  880  , Epsilon:  0.01 , Reward -42.03187141900933 , mean_reward:  9.74494714313689 , time_score:  500 , memory:  390146\n",
      "Episode:  885  , Epsilon:  0.01 , Reward -5.846948196814333 , mean_reward:  10.290713147446823 , time_score:  500 , memory:  392646\n",
      "Episode:  890  , Epsilon:  0.01 , Reward 38.668806528031595 , mean_reward:  9.765376997439928 , time_score:  500 , memory:  395146\n",
      "Episode:  895  , Epsilon:  0.01 , Reward -52.82997343056324 , mean_reward:  9.55828780307807 , time_score:  500 , memory:  397349\n",
      "Episode:  900  , Epsilon:  0.01 , Reward 19.92221638624221 , mean_reward:  8.304161335034124 , time_score:  500 , memory:  399849\n",
      "Episode:  905  , Epsilon:  0.01 , Reward 37.39759890364799 , mean_reward:  9.431300606098276 , time_score:  500 , memory:  402349\n",
      "Episode:  910  , Epsilon:  0.01 , Reward 4.994734925366958 , mean_reward:  9.929160260009048 , time_score:  500 , memory:  404849\n",
      "Episode:  915  , Epsilon:  0.01 , Reward 40.13690167009872 , mean_reward:  9.031058323299295 , time_score:  500 , memory:  407349\n",
      "Episode:  920  , Epsilon:  0.01 , Reward 4.64024689193819 , mean_reward:  9.994002905630873 , time_score:  500 , memory:  409849\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 7.2125917428142605 , mean_reward:  10.435322957994199 , time_score:  500 , memory:  412349\n",
      "Episode:  930  , Epsilon:  0.01 , Reward -96.01580255056066 , mean_reward:  9.322217657212683 , time_score:  210 , memory:  414559\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 0.5935871661001211 , mean_reward:  8.761714369192372 , time_score:  500 , memory:  416627\n",
      "Episode:  940  , Epsilon:  0.01 , Reward -0.6239253921441252 , mean_reward:  9.42439708001257 , time_score:  500 , memory:  419127\n",
      "Episode:  945  , Epsilon:  0.01 , Reward 32.561793363470656 , mean_reward:  7.604478649604099 , time_score:  500 , memory:  421187\n",
      "Episode:  950  , Epsilon:  0.01 , Reward -100.77603755937815 , mean_reward:  8.331066412736583 , time_score:  54 , memory:  422803\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 7.418627395909875 , mean_reward:  9.059071875116926 , time_score:  500 , memory:  425303\n",
      "Episode:  960  , Epsilon:  0.01 , Reward 36.63559401895567 , mean_reward:  11.00758876667389 , time_score:  500 , memory:  427803\n",
      "Episode:  965  , Epsilon:  0.01 , Reward 34.44085892019435 , mean_reward:  10.740584551770413 , time_score:  500 , memory:  430303\n",
      "Episode:  970  , Epsilon:  0.01 , Reward 25.72483739072937 , mean_reward:  11.865145239154716 , time_score:  500 , memory:  432803\n",
      "Episode:  975  , Epsilon:  0.01 , Reward 30.517912208844184 , mean_reward:  10.888129908210306 , time_score:  500 , memory:  434992\n",
      "Episode:  980  , Epsilon:  0.01 , Reward 30.910583960406438 , mean_reward:  12.300127456487829 , time_score:  500 , memory:  437492\n",
      "Episode:  985  , Epsilon:  0.01 , Reward 56.839531339108234 , mean_reward:  13.21919976454668 , time_score:  500 , memory:  439992\n",
      "Episode:  990  , Epsilon:  0.01 , Reward 60.06823094088463 , mean_reward:  14.076397336054061 , time_score:  500 , memory:  442492\n",
      "Episode:  995  , Epsilon:  0.01 , Reward 11.610546858197912 , mean_reward:  14.257826968333012 , time_score:  500 , memory:  444992\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward 44.99489493645919 , mean_reward:  15.22906178379604 , time_score:  500 , memory:  447492\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward -0.10383813732835723 , mean_reward:  14.112153790885237 , time_score:  500 , memory:  449992\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward 12.916232146516904 , mean_reward:  15.227490911136066 , time_score:  500 , memory:  452492\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward -111.14853401464744 , mean_reward:  13.296846593928455 , time_score:  153 , memory:  454340\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward 35.157858833096334 , mean_reward:  15.086583009555325 , time_score:  500 , memory:  456840\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward -26.720795582004108 , mean_reward:  15.088513519047266 , time_score:  500 , memory:  459340\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward 39.89958264996052 , mean_reward:  16.19484409307934 , time_score:  500 , memory:  461840\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward 23.67747050086627 , mean_reward:  18.064545774157796 , time_score:  500 , memory:  464340\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward 30.6212810723088 , mean_reward:  19.277211989502057 , time_score:  500 , memory:  466840\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward 26.796330664406018 , mean_reward:  20.726647796362023 , time_score:  500 , memory:  469340\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward -3.777498774688474 , mean_reward:  23.082980688808274 , time_score:  500 , memory:  471840\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward 45.61211347531227 , mean_reward:  23.38267151304542 , time_score:  500 , memory:  474340\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward -163.01309113125586 , mean_reward:  22.442809747041277 , time_score:  147 , memory:  476487\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward -126.25428395635997 , mean_reward:  21.004722622544882 , time_score:  138 , memory:  478625\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward 21.5091615406158 , mean_reward:  21.66321637654338 , time_score:  500 , memory:  481125\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward 74.76105909742027 , mean_reward:  24.45962743636645 , time_score:  500 , memory:  483625\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward 12.43327366278556 , mean_reward:  24.63422252667402 , time_score:  500 , memory:  486125\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward 54.523510629191904 , mean_reward:  24.162152693728252 , time_score:  500 , memory:  488625\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward 26.48853562853634 , mean_reward:  25.005409042420037 , time_score:  500 , memory:  491125\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward 36.47871298219039 , mean_reward:  25.868726610744403 , time_score:  500 , memory:  493625\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward 34.24812122039474 , mean_reward:  25.778039665215758 , time_score:  500 , memory:  496125\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward 39.59958143209811 , mean_reward:  26.42767142052026 , time_score:  500 , memory:  498625\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward 12.024801692336775 , mean_reward:  26.33169890434856 , time_score:  500 , memory:  501125\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 33.66003413321556 , mean_reward:  30.003589561358773 , time_score:  500 , memory:  503625\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward 36.163197140938145 , mean_reward:  29.055701689480927 , time_score:  500 , memory:  506125\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward 45.269422069536745 , mean_reward:  30.13020672802702 , time_score:  500 , memory:  508625\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward 10.155006380742536 , mean_reward:  30.427835837628994 , time_score:  500 , memory:  511125\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward -20.961694092459812 , mean_reward:  27.267110476015237 , time_score:  500 , memory:  513243\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward 27.876024984846545 , mean_reward:  27.677606922913768 , time_score:  500 , memory:  515743\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward 15.376559388191644 , mean_reward:  28.095566107894474 , time_score:  500 , memory:  518243\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward 83.32278372756147 , mean_reward:  29.2057047509764 , time_score:  500 , memory:  520743\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward 29.666921363772907 , mean_reward:  28.749677232144492 , time_score:  500 , memory:  523243\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward 35.8866936857774 , mean_reward:  30.41947764438352 , time_score:  500 , memory:  525743\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 90.94361016167574 , mean_reward:  33.311178073983854 , time_score:  500 , memory:  528243\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward -5.7852430603558025 , mean_reward:  33.217501899238066 , time_score:  500 , memory:  530743\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward 49.191202578171826 , mean_reward:  32.15763994717421 , time_score:  500 , memory:  533243\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward 23.639427357912805 , mean_reward:  31.523907839026773 , time_score:  500 , memory:  535743\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward 38.41054206540967 , mean_reward:  32.868072069804796 , time_score:  500 , memory:  538243\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward 40.110998999418754 , mean_reward:  32.964669669468584 , time_score:  500 , memory:  540743\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward 51.27742816352063 , mean_reward:  34.43976885136217 , time_score:  500 , memory:  543243\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward 63.5686575783171 , mean_reward:  34.600241265417274 , time_score:  500 , memory:  545743\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward -0.06106972443687697 , mean_reward:  35.10673737460926 , time_score:  500 , memory:  548243\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 13.32023086715088 , mean_reward:  35.07804395144809 , time_score:  500 , memory:  550743\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward 87.3931722675783 , mean_reward:  35.16891048739817 , time_score:  500 , memory:  553243\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward 7.083309191149544 , mean_reward:  35.569328688300295 , time_score:  500 , memory:  555743\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward 116.66101881264518 , mean_reward:  36.42105751147939 , time_score:  500 , memory:  558243\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward 52.42897839998581 , mean_reward:  36.59993528953582 , time_score:  500 , memory:  560743\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward 36.66828432462439 , mean_reward:  39.17146085180901 , time_score:  500 , memory:  563243\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 12.754641255066732 , mean_reward:  38.24397699750236 , time_score:  500 , memory:  565743\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 40.93863352734325 , mean_reward:  37.541966982751035 , time_score:  500 , memory:  568243\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 63.15501777647236 , mean_reward:  36.528832403557395 , time_score:  500 , memory:  570743\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward 62.47846057341586 , mean_reward:  36.208585457842474 , time_score:  500 , memory:  573243\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward 37.24996285859093 , mean_reward:  35.38373935552486 , time_score:  500 , memory:  575743\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 35.468132236714695 , mean_reward:  35.628096899884866 , time_score:  500 , memory:  578243\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward 64.31510483955925 , mean_reward:  35.02384156751458 , time_score:  500 , memory:  580743\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward -8.487861179098566 , mean_reward:  33.15128130222808 , time_score:  500 , memory:  582882\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward 16.17790949426978 , mean_reward:  33.588592625589655 , time_score:  500 , memory:  585382\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward 66.08840803462512 , mean_reward:  33.363027614778375 , time_score:  500 , memory:  587882\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward -20.145773686058625 , mean_reward:  32.336476049391706 , time_score:  500 , memory:  590382\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward 48.51549237636567 , mean_reward:  32.63305759496604 , time_score:  500 , memory:  592882\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward 96.95924470022737 , mean_reward:  33.62668986663759 , time_score:  500 , memory:  595382\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward 137.48925297992466 , mean_reward:  33.72048019611365 , time_score:  500 , memory:  597882\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward 59.71026043794689 , mean_reward:  33.742092827293355 , time_score:  500 , memory:  600382\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward 9.438273169412069 , mean_reward:  33.7123761343627 , time_score:  500 , memory:  602882\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 37.34494119055382 , mean_reward:  33.108677390756824 , time_score:  500 , memory:  605382\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward -18.552739758490276 , mean_reward:  30.973408583665012 , time_score:  500 , memory:  607882\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward 41.27451354218894 , mean_reward:  30.99508321442612 , time_score:  500 , memory:  610382\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward 53.852993777032744 , mean_reward:  33.1331247616901 , time_score:  500 , memory:  612882\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward 13.727423336320594 , mean_reward:  34.52331991523188 , time_score:  500 , memory:  615382\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward -21.780642237571207 , mean_reward:  35.44056392379195 , time_score:  500 , memory:  617882\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward 20.164429656368462 , mean_reward:  35.107655976556 , time_score:  500 , memory:  620277\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward 75.71410971993959 , mean_reward:  35.80170003268021 , time_score:  500 , memory:  622777\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward -5.760917945792359 , mean_reward:  35.3894513666749 , time_score:  500 , memory:  625277\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward 90.77355948477756 , mean_reward:  35.32166358009081 , time_score:  500 , memory:  627777\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward 24.66248675266447 , mean_reward:  35.47892772986883 , time_score:  500 , memory:  630277\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward 54.215719735064575 , mean_reward:  37.631763939281 , time_score:  500 , memory:  632777\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward 25.102552905526846 , mean_reward:  38.537563702448026 , time_score:  500 , memory:  635277\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward 58.47233948504147 , mean_reward:  38.68989381915355 , time_score:  500 , memory:  637777\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward 73.64498028233227 , mean_reward:  41.007686646840305 , time_score:  500 , memory:  640277\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward 98.81023671240409 , mean_reward:  40.582874637619796 , time_score:  500 , memory:  642777\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward 14.642257672106318 , mean_reward:  39.59078592621455 , time_score:  500 , memory:  645277\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward 81.30483956901311 , mean_reward:  39.56343122689684 , time_score:  500 , memory:  647777\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward 38.8368170657931 , mean_reward:  39.17520704399954 , time_score:  500 , memory:  650277\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward 35.33878555565243 , mean_reward:  39.908852944105135 , time_score:  500 , memory:  652731\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward -3.949862291977478 , mean_reward:  40.02916355551404 , time_score:  500 , memory:  655231\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward 3.0930597447808124 , mean_reward:  41.05538712535928 , time_score:  500 , memory:  657731\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward 32.815736732896426 , mean_reward:  42.287015905590486 , time_score:  500 , memory:  660231\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward 34.755241322034834 , mean_reward:  39.727878176201735 , time_score:  500 , memory:  662420\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward 47.231482551754326 , mean_reward:  38.82113544210543 , time_score:  500 , memory:  664920\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward 37.539643621086825 , mean_reward:  38.90160128989733 , time_score:  500 , memory:  667420\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward 51.43367548567601 , mean_reward:  40.6246790271589 , time_score:  500 , memory:  669920\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward 23.90609992918692 , mean_reward:  40.84363377997801 , time_score:  500 , memory:  672420\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward 60.640498558064024 , mean_reward:  41.081858097222366 , time_score:  500 , memory:  674920\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward 26.446252913455858 , mean_reward:  39.40544508365978 , time_score:  500 , memory:  677420\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward 98.445584267669 , mean_reward:  41.8407833448702 , time_score:  500 , memory:  679920\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward 68.23597438854154 , mean_reward:  42.889253727288505 , time_score:  500 , memory:  682420\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward 79.84650830183281 , mean_reward:  41.94138253273766 , time_score:  500 , memory:  684920\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward 42.72006815921201 , mean_reward:  41.12990757866035 , time_score:  500 , memory:  687420\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward 44.597165884056814 , mean_reward:  40.32043335506048 , time_score:  500 , memory:  689920\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward 67.96818233638587 , mean_reward:  40.932983975392766 , time_score:  500 , memory:  692420\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward 31.64218368735176 , mean_reward:  41.30861332277885 , time_score:  500 , memory:  694920\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward -39.34158196265043 , mean_reward:  40.329112356589164 , time_score:  500 , memory:  697420\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward 22.782180686953794 , mean_reward:  40.3769168251295 , time_score:  500 , memory:  699920\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward 65.01125982590871 , mean_reward:  39.736663711342686 , time_score:  500 , memory:  702420\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward 76.07870565003131 , mean_reward:  41.28548740997264 , time_score:  500 , memory:  704920\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward 62.60952354052109 , mean_reward:  43.03934254865871 , time_score:  500 , memory:  707420\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward 55.84580345311618 , mean_reward:  42.2218340373506 , time_score:  500 , memory:  709920\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward 88.78570893841109 , mean_reward:  43.49821498363905 , time_score:  500 , memory:  712420\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward 14.861136892398537 , mean_reward:  43.721803190320614 , time_score:  500 , memory:  714920\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward 68.76473021840654 , mean_reward:  45.121385443969785 , time_score:  500 , memory:  717420\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward 51.44997645673105 , mean_reward:  45.46473004559045 , time_score:  500 , memory:  719920\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward 70.60771406468245 , mean_reward:  47.50570891139363 , time_score:  500 , memory:  722420\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward 33.464220937375835 , mean_reward:  48.18382143292981 , time_score:  500 , memory:  724920\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward 52.00455958138718 , mean_reward:  50.195885959555 , time_score:  500 , memory:  727420\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward 4.501974325552989 , mean_reward:  47.23641455135819 , time_score:  500 , memory:  729920\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward 46.90013892108215 , mean_reward:  47.269194839656706 , time_score:  500 , memory:  732420\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward 16.322379315169147 , mean_reward:  47.621797617988925 , time_score:  500 , memory:  734920\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward 30.99401121872843 , mean_reward:  48.43696403904123 , time_score:  500 , memory:  737420\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward 76.68992257184179 , mean_reward:  47.47796578175871 , time_score:  500 , memory:  739920\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward 73.09107804817822 , mean_reward:  46.91199055205636 , time_score:  500 , memory:  742420\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward 106.6585120564305 , mean_reward:  48.36014216294439 , time_score:  500 , memory:  744920\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward 32.65748324074611 , mean_reward:  50.65145444912574 , time_score:  500 , memory:  747420\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward -8.898949719955194 , mean_reward:  50.561342785764225 , time_score:  500 , memory:  749920\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward 31.957915580044943 , mean_reward:  49.309241455203185 , time_score:  500 , memory:  752420\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward 5.940122408247226 , mean_reward:  48.44699117837612 , time_score:  500 , memory:  754920\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward 97.63509122986203 , mean_reward:  48.81001235526258 , time_score:  500 , memory:  757420\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward 14.607711977080774 , mean_reward:  48.01886911369399 , time_score:  500 , memory:  759920\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward 73.4248588848538 , mean_reward:  49.56000546088612 , time_score:  500 , memory:  762420\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward 78.0787278471987 , mean_reward:  49.96058820976502 , time_score:  500 , memory:  764920\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward 53.48125417613889 , mean_reward:  49.71361427995979 , time_score:  500 , memory:  767420\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 99.4795726484068 , mean_reward:  51.146971254423654 , time_score:  500 , memory:  769920\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward 74.04980399164822 , mean_reward:  48.019971766761365 , time_score:  500 , memory:  772420\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward 36.40476157503894 , mean_reward:  48.07400303683429 , time_score:  500 , memory:  774920\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward 40.506628974728784 , mean_reward:  48.05689547150586 , time_score:  500 , memory:  777420\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward -1.8971230715761485 , mean_reward:  49.63018213737833 , time_score:  500 , memory:  779920\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward 95.64042957467285 , mean_reward:  50.414629657252426 , time_score:  500 , memory:  782420\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward 24.92913133517946 , mean_reward:  50.884790096641936 , time_score:  500 , memory:  784920\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward 25.390635002255763 , mean_reward:  51.77361560296006 , time_score:  500 , memory:  787420\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward 44.18526773276203 , mean_reward:  52.700316602416194 , time_score:  500 , memory:  789920\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward 56.66300470062724 , mean_reward:  53.0353225082943 , time_score:  500 , memory:  792420\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward 66.74694867307856 , mean_reward:  51.929600869790136 , time_score:  500 , memory:  794920\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward 47.4355481435628 , mean_reward:  51.850099410952524 , time_score:  500 , memory:  797420\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward 47.22589796491114 , mean_reward:  52.25457397413435 , time_score:  500 , memory:  799920\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward 15.875276359231869 , mean_reward:  54.346256132005784 , time_score:  500 , memory:  802420\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward 112.28349279925288 , mean_reward:  56.26310891570065 , time_score:  500 , memory:  804920\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward 20.819194031524844 , mean_reward:  54.70503533868073 , time_score:  500 , memory:  807420\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward -4.558153946649162 , mean_reward:  54.846928371573064 , time_score:  500 , memory:  809920\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward 60.55835446878797 , mean_reward:  54.3444082098716 , time_score:  500 , memory:  812420\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward 92.3111502885334 , mean_reward:  55.30674426957737 , time_score:  500 , memory:  814920\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward 57.891217804656385 , mean_reward:  54.48803919420629 , time_score:  500 , memory:  817420\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward 5.9288729343327535 , mean_reward:  52.67439812874417 , time_score:  500 , memory:  819920\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward 64.73737965216621 , mean_reward:  54.75496848432876 , time_score:  500 , memory:  822420\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward 15.230045427230971 , mean_reward:  54.717592028226264 , time_score:  500 , memory:  824920\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward 71.60164691595834 , mean_reward:  53.485422958839834 , time_score:  500 , memory:  827420\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward 94.65106984529285 , mean_reward:  54.61261498622162 , time_score:  500 , memory:  829920\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward 88.08098673818976 , mean_reward:  54.197097230542376 , time_score:  500 , memory:  832420\n",
      "Episode:  1780  , Epsilon:  0.01 , Reward 71.73410105784772 , mean_reward:  54.655228287175916 , time_score:  500 , memory:  834920\n",
      "Episode:  1785  , Epsilon:  0.01 , Reward 33.4158617857137 , mean_reward:  52.44173976533067 , time_score:  500 , memory:  837420\n",
      "Episode:  1790  , Epsilon:  0.01 , Reward 104.47982948462368 , mean_reward:  53.37328955262183 , time_score:  500 , memory:  839920\n",
      "Episode:  1795  , Epsilon:  0.01 , Reward 58.73757216256082 , mean_reward:  53.5184465169183 , time_score:  500 , memory:  842420\n",
      "Episode:  1800  , Epsilon:  0.01 , Reward 71.74054402601742 , mean_reward:  54.27362632858657 , time_score:  500 , memory:  844920\n",
      "Episode:  1805  , Epsilon:  0.01 , Reward 74.32315335705931 , mean_reward:  55.011176350493805 , time_score:  500 , memory:  847420\n",
      "Episode:  1810  , Epsilon:  0.01 , Reward 27.82931565616766 , mean_reward:  55.68645933836754 , time_score:  500 , memory:  849920\n",
      "Episode:  1815  , Epsilon:  0.01 , Reward 51.54790450083697 , mean_reward:  54.72078576400805 , time_score:  500 , memory:  852420\n",
      "Episode:  1820  , Epsilon:  0.01 , Reward 87.27076003174369 , mean_reward:  53.55313060184815 , time_score:  500 , memory:  854920\n",
      "Episode:  1825  , Epsilon:  0.01 , Reward 93.05503383368107 , mean_reward:  54.88495402106843 , time_score:  500 , memory:  857420\n",
      "Episode:  1830  , Epsilon:  0.01 , Reward 38.340746275540084 , mean_reward:  55.54885044096356 , time_score:  500 , memory:  859920\n",
      "Episode:  1835  , Epsilon:  0.01 , Reward 42.73490000572498 , mean_reward:  54.93551892107404 , time_score:  500 , memory:  862420\n",
      "Episode:  1840  , Epsilon:  0.01 , Reward 51.71638841203395 , mean_reward:  54.743731476045156 , time_score:  500 , memory:  864920\n",
      "Episode:  1845  , Epsilon:  0.01 , Reward 87.43614159564264 , mean_reward:  55.970363992851034 , time_score:  500 , memory:  867420\n",
      "Episode:  1850  , Epsilon:  0.01 , Reward 29.740031911750997 , mean_reward:  56.40554491775594 , time_score:  500 , memory:  869920\n",
      "Episode:  1855  , Epsilon:  0.01 , Reward 87.52052844763547 , mean_reward:  57.30718605986871 , time_score:  500 , memory:  872420\n",
      "Episode:  1860  , Epsilon:  0.01 , Reward 110.93189048066017 , mean_reward:  58.92700679563803 , time_score:  500 , memory:  874920\n",
      "Episode:  1865  , Epsilon:  0.01 , Reward 58.56966245075942 , mean_reward:  60.15523982146419 , time_score:  500 , memory:  877420\n",
      "Episode:  1870  , Epsilon:  0.01 , Reward 79.41524796716115 , mean_reward:  59.672372733361975 , time_score:  500 , memory:  879920\n",
      "Episode:  1875  , Epsilon:  0.01 , Reward 74.11970003839998 , mean_reward:  59.85944069920109 , time_score:  500 , memory:  882420\n",
      "Episode:  1880  , Epsilon:  0.01 , Reward 91.08395596946987 , mean_reward:  59.97685775068917 , time_score:  500 , memory:  884920\n",
      "Episode:  1885  , Epsilon:  0.01 , Reward 25.582237940600038 , mean_reward:  61.96267911271248 , time_score:  500 , memory:  887420\n",
      "Episode:  1890  , Epsilon:  0.01 , Reward -4.213261417283244 , mean_reward:  60.17294087226649 , time_score:  500 , memory:  889920\n",
      "Episode:  1895  , Epsilon:  0.01 , Reward 45.04035837736753 , mean_reward:  58.72556808641823 , time_score:  500 , memory:  892420\n",
      "Episode:  1900  , Epsilon:  0.01 , Reward 46.8658405504951 , mean_reward:  58.08093254523667 , time_score:  500 , memory:  894920\n",
      "Episode:  1905  , Epsilon:  0.01 , Reward 60.448928729574135 , mean_reward:  56.432715232382385 , time_score:  500 , memory:  897420\n",
      "Episode:  1910  , Epsilon:  0.01 , Reward 48.5429911057474 , mean_reward:  56.49157167172403 , time_score:  500 , memory:  899920\n",
      "Episode:  1915  , Epsilon:  0.01 , Reward -56.507822430549325 , mean_reward:  56.266287696265024 , time_score:  500 , memory:  902420\n",
      "Episode:  1920  , Epsilon:  0.01 , Reward 109.96907492639572 , mean_reward:  56.281914557783814 , time_score:  500 , memory:  904920\n",
      "Episode:  1925  , Epsilon:  0.01 , Reward 83.76013155822929 , mean_reward:  56.211025783401865 , time_score:  500 , memory:  907420\n",
      "Episode:  1930  , Epsilon:  0.01 , Reward 48.32786194374014 , mean_reward:  56.58176293219374 , time_score:  500 , memory:  909920\n",
      "Episode:  1935  , Epsilon:  0.01 , Reward 33.5839817212245 , mean_reward:  56.9766879266241 , time_score:  500 , memory:  912420\n",
      "Episode:  1940  , Epsilon:  0.01 , Reward 45.39140572449514 , mean_reward:  56.705509262879744 , time_score:  500 , memory:  914920\n",
      "Episode:  1945  , Epsilon:  0.01 , Reward 73.01108528717758 , mean_reward:  56.143735597141756 , time_score:  500 , memory:  917420\n",
      "Episode:  1950  , Epsilon:  0.01 , Reward 71.49320173606324 , mean_reward:  56.9934222342076 , time_score:  500 , memory:  919920\n",
      "Episode:  1955  , Epsilon:  0.01 , Reward 67.37641585728792 , mean_reward:  56.4042028831549 , time_score:  500 , memory:  922420\n",
      "Episode:  1960  , Epsilon:  0.01 , Reward 50.48760786895392 , mean_reward:  55.25385216155214 , time_score:  500 , memory:  924920\n",
      "Episode:  1965  , Epsilon:  0.01 , Reward 53.8414544809209 , mean_reward:  55.271021780578806 , time_score:  500 , memory:  927420\n",
      "Episode:  1970  , Epsilon:  0.01 , Reward 18.108289776752507 , mean_reward:  55.44706597468055 , time_score:  500 , memory:  929920\n",
      "Episode:  1975  , Epsilon:  0.01 , Reward 52.668885705083156 , mean_reward:  54.50840207903513 , time_score:  500 , memory:  932420\n",
      "Episode:  1980  , Epsilon:  0.01 , Reward 43.86692338976894 , mean_reward:  53.134440069071246 , time_score:  500 , memory:  934920\n",
      "Episode:  1985  , Epsilon:  0.01 , Reward 77.64167605416844 , mean_reward:  53.453393603816515 , time_score:  500 , memory:  937420\n",
      "Episode:  1990  , Epsilon:  0.01 , Reward 55.28268451985764 , mean_reward:  55.0732917063335 , time_score:  500 , memory:  939920\n",
      "Episode:  1995  , Epsilon:  0.01 , Reward 85.37674030802123 , mean_reward:  58.83229343532584 , time_score:  500 , memory:  942336\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.95, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.001, lr=0.0005)\n",
    "df = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zEK6_8NkZvY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzXeEPyZkZx5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
