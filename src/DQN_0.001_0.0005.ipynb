{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"tryout.csv\"\n",
    "        self.model_filename = \"tryout.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "              with open(self.csv_filename, 'a') as f:\n",
    "                self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "              \n",
    "              self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "\n",
    "              self.Q_model.save(self.model_filename)\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "                self.ep *= self.ep_decay\n",
    "            else:\n",
    "                self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return R, R_moving\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -132.294436358253 , mean_reward:  -132.294436358253 , time_score:  83 , memory:  83\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -128.23340580341736 , mean_reward:  -142.9885104447782 , time_score:  71 , memory:  678\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -105.32527147624414 , mean_reward:  -171.39259888079488 , time_score:  103 , memory:  1179\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -85.78123929043677 , mean_reward:  -165.67304737833507 , time_score:  74 , memory:  1676\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -258.05595572459384 , mean_reward:  -168.71441736155762 , time_score:  150 , memory:  2224\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -86.7609566819509 , mean_reward:  -148.51049121344755 , time_score:  79 , memory:  2794\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -118.13562445777946 , mean_reward:  -148.06787354929583 , time_score:  95 , memory:  3361\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -229.89666885000432 , mean_reward:  -156.23533834304916 , time_score:  145 , memory:  3904\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -273.0854082875244 , mean_reward:  -157.278639074006 , time_score:  89 , memory:  4381\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -93.59551962190802 , mean_reward:  -145.5371899134698 , time_score:  72 , memory:  4802\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward -144.48284466072747 , mean_reward:  -149.0045274456523 , time_score:  97 , memory:  5319\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -115.34717979428775 , mean_reward:  -145.74553688392484 , time_score:  107 , memory:  5901\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -146.0656748722697 , mean_reward:  -154.38996782648587 , time_score:  124 , memory:  6524\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -135.89928232415207 , mean_reward:  -150.9922273319602 , time_score:  88 , memory:  7167\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -193.677411576044 , mean_reward:  -149.77917100336043 , time_score:  139 , memory:  7828\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -85.95973299415591 , mean_reward:  -148.42852267813842 , time_score:  90 , memory:  8497\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward 8.796088792621276 , mean_reward:  -146.81574297693564 , time_score:  128 , memory:  9179\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -72.95870562159057 , mean_reward:  -145.97222546092155 , time_score:  89 , memory:  9782\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -170.96500988809657 , mean_reward:  -141.6698220992919 , time_score:  118 , memory:  10860\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -76.28493234346777 , mean_reward:  -141.10950680271864 , time_score:  142 , memory:  11676\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -159.067456431929 , mean_reward:  -140.4184810407993 , time_score:  122 , memory:  12473\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -185.30600397537464 , mean_reward:  -140.24054118531924 , time_score:  220 , memory:  13345\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -147.49312961264286 , mean_reward:  -140.58633105424005 , time_score:  187 , memory:  13999\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -134.47364212691627 , mean_reward:  -139.65467966873194 , time_score:  103 , memory:  14824\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -108.85607589322119 , mean_reward:  -136.22329564785275 , time_score:  101 , memory:  15562\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -124.92758650809935 , mean_reward:  -136.751004323297 , time_score:  165 , memory:  16505\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -127.08590198838986 , mean_reward:  -137.24624555006244 , time_score:  121 , memory:  17358\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -156.9240920507303 , mean_reward:  -131.67903662434196 , time_score:  198 , memory:  18103\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward -82.4789389695121 , mean_reward:  -128.10940995830194 , time_score:  153 , memory:  18880\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward -11.113366052320828 , mean_reward:  -129.87448734208024 , time_score:  153 , memory:  19806\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -127.90008571430074 , mean_reward:  -124.41621664890565 , time_score:  180 , memory:  20624\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -170.71105029499466 , mean_reward:  -126.98351160030337 , time_score:  329 , memory:  21793\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -17.357607204788366 , mean_reward:  -118.02844282320069 , time_score:  146 , memory:  23172\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward -71.59515565258748 , mean_reward:  -117.73100642439744 , time_score:  238 , memory:  24546\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -258.16576143291695 , mean_reward:  -115.8395682986296 , time_score:  183 , memory:  25576\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward -24.557149414159156 , mean_reward:  -110.71239002426053 , time_score:  225 , memory:  26478\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward 84.39622094928143 , mean_reward:  -105.58229107218177 , time_score:  500 , memory:  28071\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -16.438579604364207 , mean_reward:  -97.69934510407295 , time_score:  171 , memory:  29777\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward -5.257768040897011 , mean_reward:  -93.90824874250126 , time_score:  500 , memory:  30922\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -9.091601963323669 , mean_reward:  -92.75316607045747 , time_score:  180 , memory:  32147\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -83.01401140225391 , mean_reward:  -88.9890170619227 , time_score:  233 , memory:  33553\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward 40.56374110777899 , mean_reward:  -82.93391502412526 , time_score:  500 , memory:  35761\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -15.546254870962215 , mean_reward:  -75.56903461067115 , time_score:  244 , memory:  37484\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward 107.043799192321 , mean_reward:  -68.61357882308778 , time_score:  500 , memory:  39479\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward -25.37711526365652 , mean_reward:  -67.18249784206779 , time_score:  386 , memory:  41370\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward 20.456002654260914 , mean_reward:  -68.19675700735894 , time_score:  199 , memory:  42965\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward 76.06465707817931 , mean_reward:  -57.52020762059524 , time_score:  500 , memory:  45171\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward -26.186942484898267 , mean_reward:  -57.31043531513003 , time_score:  500 , memory:  47197\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward -252.64913048235624 , mean_reward:  -57.481218623586955 , time_score:  356 , memory:  49272\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward -40.68424970074011 , mean_reward:  -57.0341867769423 , time_score:  500 , memory:  50635\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward -33.12636584785031 , mean_reward:  -56.07865592246679 , time_score:  237 , memory:  52000\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward -253.67438901200433 , mean_reward:  -52.28649696136524 , time_score:  356 , memory:  54218\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward 128.08990367632288 , mean_reward:  -46.62197833597599 , time_score:  500 , memory:  56574\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward 11.775679792123363 , mean_reward:  -41.408320384483034 , time_score:  500 , memory:  59074\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward 43.89241127636432 , mean_reward:  -41.47946120630864 , time_score:  500 , memory:  60870\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward 174.42368499453423 , mean_reward:  -37.872906296231335 , time_score:  500 , memory:  63339\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward -45.15671179200308 , mean_reward:  -38.620706900010894 , time_score:  333 , memory:  65410\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward -22.579619181323064 , mean_reward:  -40.83446730645874 , time_score:  500 , memory:  67638\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward -16.26107945179897 , mean_reward:  -42.2884650970324 , time_score:  500 , memory:  69698\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward -31.486795974220296 , mean_reward:  -35.47512941747617 , time_score:  500 , memory:  72198\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward 51.03607535120764 , mean_reward:  -32.894431804103164 , time_score:  500 , memory:  74465\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward -195.19100916032926 , mean_reward:  -35.45300585227578 , time_score:  306 , memory:  76119\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward 60.63365360474956 , mean_reward:  -31.247033919866002 , time_score:  500 , memory:  78619\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward 51.742886167486986 , mean_reward:  -31.25864681820375 , time_score:  500 , memory:  81119\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward 19.69287469859595 , mean_reward:  -25.43408098666474 , time_score:  500 , memory:  83619\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward -34.03366850878826 , mean_reward:  -19.336487631859764 , time_score:  500 , memory:  86119\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward 40.2548974059892 , mean_reward:  -20.373104945694067 , time_score:  500 , memory:  88619\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward -24.350884635340428 , mean_reward:  -16.300762360518544 , time_score:  500 , memory:  90825\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward 18.67850641765985 , mean_reward:  -11.680034229908133 , time_score:  500 , memory:  93325\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward -27.427896425482174 , mean_reward:  -7.099165544598732 , time_score:  500 , memory:  95825\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward -31.56877157686089 , mean_reward:  -5.219592920666235 , time_score:  500 , memory:  98325\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward -25.93613236803266 , mean_reward:  -0.05017936887749602 , time_score:  500 , memory:  100825\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward -35.30747281956923 , mean_reward:  -1.4004599425450959 , time_score:  500 , memory:  103325\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward -20.232740213571077 , mean_reward:  -0.09774214605040214 , time_score:  500 , memory:  105825\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward -104.37540162429661 , mean_reward:  5.423425243887403 , time_score:  293 , memory:  108118\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward 53.787503024151405 , mean_reward:  4.945683640913092 , time_score:  500 , memory:  110618\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward -6.030038023156002 , mean_reward:  8.377295411171447 , time_score:  500 , memory:  113118\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward -2.389736341453571 , mean_reward:  10.836175912986189 , time_score:  500 , memory:  115618\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward -31.32287175286568 , mean_reward:  12.888954553893019 , time_score:  500 , memory:  118118\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward 94.2192457701312 , mean_reward:  14.199979104283385 , time_score:  500 , memory:  120618\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward 75.06438332943713 , mean_reward:  16.456195840727286 , time_score:  500 , memory:  123118\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward 69.43632384652776 , mean_reward:  22.59750523710995 , time_score:  500 , memory:  125618\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward 22.119427701228275 , mean_reward:  23.217098459079267 , time_score:  500 , memory:  128118\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward 92.61769863560716 , mean_reward:  24.747135090070884 , time_score:  500 , memory:  130618\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward 70.970067299721 , mean_reward:  24.571641029619535 , time_score:  500 , memory:  133118\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward 39.13677998334829 , mean_reward:  25.3258002892514 , time_score:  500 , memory:  135618\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward 42.94801876206077 , mean_reward:  25.433909588225095 , time_score:  500 , memory:  138118\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward 23.292132103102553 , mean_reward:  29.03597464328695 , time_score:  500 , memory:  140618\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward 16.2709501812154 , mean_reward:  30.52808289830981 , time_score:  500 , memory:  143118\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward 115.26983377415091 , mean_reward:  31.175550551784646 , time_score:  500 , memory:  145278\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward 51.021980128064406 , mean_reward:  33.97596090441609 , time_score:  500 , memory:  147778\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward 90.26222089977618 , mean_reward:  34.97008763639071 , time_score:  500 , memory:  150278\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward 51.391841694956284 , mean_reward:  36.835682078564695 , time_score:  500 , memory:  152778\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward 57.54080262371076 , mean_reward:  38.037589010694255 , time_score:  500 , memory:  155278\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward 77.1150631701552 , mean_reward:  39.27925627615757 , time_score:  500 , memory:  157778\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward 36.46052037678507 , mean_reward:  39.02436445397217 , time_score:  500 , memory:  160278\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward 16.222635830659105 , mean_reward:  39.25103442024333 , time_score:  500 , memory:  162778\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward -74.47138555232783 , mean_reward:  38.65886327851746 , time_score:  176 , memory:  164954\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward 70.74276413539413 , mean_reward:  39.70282914481781 , time_score:  500 , memory:  167449\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward 82.00746847029049 , mean_reward:  38.62977925944238 , time_score:  500 , memory:  169663\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward 15.304626968056615 , mean_reward:  38.57032327180859 , time_score:  500 , memory:  172163\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward 66.18759407927877 , mean_reward:  38.15388224155917 , time_score:  500 , memory:  174663\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward 104.42065097718395 , mean_reward:  39.50885065459396 , time_score:  500 , memory:  177163\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward -4.863914246267395 , mean_reward:  39.00690556016223 , time_score:  500 , memory:  179663\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward 8.022376510139539 , mean_reward:  38.212394591504236 , time_score:  500 , memory:  181855\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward 15.450020693947916 , mean_reward:  38.04815814782348 , time_score:  500 , memory:  184355\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward 27.84956842799454 , mean_reward:  37.86412088567831 , time_score:  500 , memory:  186855\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward 64.92824944037469 , mean_reward:  36.90008616332298 , time_score:  500 , memory:  189355\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward 51.33901222231722 , mean_reward:  37.43356336265046 , time_score:  500 , memory:  191855\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward 45.77418649364419 , mean_reward:  37.021329754486956 , time_score:  500 , memory:  194355\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward 99.08034660264528 , mean_reward:  37.586385280915486 , time_score:  500 , memory:  196855\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward 70.28089568167687 , mean_reward:  38.07826670978062 , time_score:  500 , memory:  199355\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward 84.75853520852016 , mean_reward:  37.304886071474016 , time_score:  500 , memory:  201855\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward 17.829920167011156 , mean_reward:  37.62803698977528 , time_score:  500 , memory:  204355\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward 87.39053086428325 , mean_reward:  38.4399570362348 , time_score:  500 , memory:  206855\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward 45.94300673396125 , mean_reward:  39.367459726028 , time_score:  500 , memory:  209355\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward 4.1377154895563155 , mean_reward:  37.85599852837306 , time_score:  500 , memory:  211476\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward -10.646275307186766 , mean_reward:  38.51375158550535 , time_score:  500 , memory:  213976\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward 46.78217650051805 , mean_reward:  39.385335013211524 , time_score:  500 , memory:  216476\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward 27.896808831650468 , mean_reward:  39.48877053185368 , time_score:  500 , memory:  218976\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward 46.28841165970606 , mean_reward:  40.29607776313767 , time_score:  500 , memory:  221476\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward 88.80975517348689 , mean_reward:  40.9847905199273 , time_score:  500 , memory:  223976\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward 8.839344647865712 , mean_reward:  39.75488263550319 , time_score:  500 , memory:  226476\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward 56.55486930962768 , mean_reward:  41.443263212033614 , time_score:  500 , memory:  228976\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward 48.77568086926012 , mean_reward:  42.029302718453884 , time_score:  500 , memory:  231476\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward 65.1498409047873 , mean_reward:  42.8815309630495 , time_score:  500 , memory:  233976\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward 46.977959157929085 , mean_reward:  44.26843639344642 , time_score:  500 , memory:  236476\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward -21.113907731363014 , mean_reward:  42.83229420229736 , time_score:  500 , memory:  238976\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward 62.71779205113553 , mean_reward:  42.92584927631895 , time_score:  500 , memory:  241476\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward 34.66443054335918 , mean_reward:  44.34936607936513 , time_score:  500 , memory:  243976\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward 54.158448280723164 , mean_reward:  44.06011504125342 , time_score:  500 , memory:  246476\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward 95.63114457774105 , mean_reward:  45.59225123869218 , time_score:  500 , memory:  248976\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 18.4036092206024 , mean_reward:  48.02794757272701 , time_score:  500 , memory:  251432\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward 58.6481797778099 , mean_reward:  47.39605364241154 , time_score:  500 , memory:  253932\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward 37.49098487531062 , mean_reward:  46.66344217907324 , time_score:  500 , memory:  256432\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward 31.47136065522464 , mean_reward:  45.99129672447689 , time_score:  500 , memory:  258932\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward 63.86005639998528 , mean_reward:  48.1294076254102 , time_score:  500 , memory:  261432\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward 65.45013666932402 , mean_reward:  48.92273322218521 , time_score:  500 , memory:  263932\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward 113.28664183870193 , mean_reward:  48.35695764810748 , time_score:  500 , memory:  266432\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward 60.476614784266324 , mean_reward:  48.95618924557451 , time_score:  500 , memory:  268932\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward 62.26800414083121 , mean_reward:  49.35503596956662 , time_score:  500 , memory:  271432\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward 25.488656859425767 , mean_reward:  48.18426925230965 , time_score:  500 , memory:  273932\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward 21.86813060594168 , mean_reward:  49.50562619000438 , time_score:  500 , memory:  276432\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 51.06589787842134 , mean_reward:  51.4155304584165 , time_score:  500 , memory:  278915\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward 52.34716600085447 , mean_reward:  54.20203537301368 , time_score:  500 , memory:  281415\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 92.30603564451141 , mean_reward:  54.06823720101672 , time_score:  500 , memory:  283915\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward 68.27587584764561 , mean_reward:  56.20397101034998 , time_score:  500 , memory:  286358\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward 53.16462254496235 , mean_reward:  57.86139698838511 , time_score:  500 , memory:  288483\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 8.005355350984983 , mean_reward:  58.7054270320852 , time_score:  500 , memory:  290983\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward 57.56698326911686 , mean_reward:  60.70496562529539 , time_score:  500 , memory:  293483\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward 228.41760748112884 , mean_reward:  63.53179326758174 , time_score:  424 , memory:  295907\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward 65.67242939473314 , mean_reward:  61.913794238805075 , time_score:  500 , memory:  298407\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward 43.25769618375272 , mean_reward:  61.133007430168476 , time_score:  500 , memory:  300907\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward 47.05109672139217 , mean_reward:  62.16001810910122 , time_score:  500 , memory:  303407\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward 43.488863926085685 , mean_reward:  62.56291236004274 , time_score:  500 , memory:  305907\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward 14.125975420975545 , mean_reward:  63.93697035947948 , time_score:  500 , memory:  308407\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward 90.91634088109383 , mean_reward:  64.31436070176669 , time_score:  500 , memory:  310907\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward 85.8116600430543 , mean_reward:  65.17351422160188 , time_score:  500 , memory:  313407\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward 68.8395958127667 , mean_reward:  65.5407978324793 , time_score:  500 , memory:  315907\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 71.86057198112289 , mean_reward:  65.51822106889749 , time_score:  500 , memory:  318407\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward 67.17574073657883 , mean_reward:  65.59461830175827 , time_score:  500 , memory:  320907\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward 46.34732131175474 , mean_reward:  66.77345467250558 , time_score:  500 , memory:  323407\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward 78.21028125285754 , mean_reward:  66.33530694621268 , time_score:  500 , memory:  325907\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward 56.155914099877876 , mean_reward:  64.31327215547103 , time_score:  500 , memory:  328407\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward 62.44565042437658 , mean_reward:  62.895612279132536 , time_score:  500 , memory:  330907\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward 75.34536553245586 , mean_reward:  63.42924266583295 , time_score:  500 , memory:  333407\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward 43.8050838644737 , mean_reward:  61.38770722112206 , time_score:  500 , memory:  335907\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward 46.649345750423784 , mean_reward:  61.510064929060135 , time_score:  500 , memory:  338407\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward 52.83214489330568 , mean_reward:  62.16911387892434 , time_score:  500 , memory:  340907\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward 28.829329134669575 , mean_reward:  61.03694427671253 , time_score:  500 , memory:  343407\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward 70.47631617234619 , mean_reward:  59.88153358611176 , time_score:  500 , memory:  345907\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward 43.57908029387557 , mean_reward:  61.61798766111371 , time_score:  500 , memory:  348407\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward 68.02989208900928 , mean_reward:  61.43254885402749 , time_score:  500 , memory:  350907\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward 107.2605651793372 , mean_reward:  61.11982886891295 , time_score:  500 , memory:  353407\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward 41.97760609581503 , mean_reward:  63.8727932347071 , time_score:  500 , memory:  355790\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward 73.51261927031703 , mean_reward:  64.11195207043694 , time_score:  500 , memory:  358290\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward 77.76269068873015 , mean_reward:  64.33981751549139 , time_score:  500 , memory:  360790\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward 76.7930433481469 , mean_reward:  64.51527536610466 , time_score:  500 , memory:  363290\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward 56.894187375300355 , mean_reward:  65.65924559960708 , time_score:  500 , memory:  365790\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward 86.21171798345546 , mean_reward:  68.99664933891803 , time_score:  500 , memory:  368239\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward 56.653455699708694 , mean_reward:  68.34795180271406 , time_score:  500 , memory:  370739\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward 69.17470378346303 , mean_reward:  69.25286836362756 , time_score:  500 , memory:  373239\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward 64.05865757777255 , mean_reward:  69.37035499027142 , time_score:  500 , memory:  375739\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward 77.15959261358272 , mean_reward:  72.36014575023087 , time_score:  500 , memory:  378174\n",
      "Episode:  920  , Epsilon:  0.01 , Reward 57.96361134251799 , mean_reward:  74.31460600589524 , time_score:  500 , memory:  380596\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 35.13999197819993 , mean_reward:  73.79320151226192 , time_score:  500 , memory:  383096\n",
      "Episode:  930  , Epsilon:  0.01 , Reward 74.60424970580632 , mean_reward:  73.21253723033907 , time_score:  500 , memory:  385596\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 93.86153597555244 , mean_reward:  74.38503689811313 , time_score:  500 , memory:  388096\n",
      "Episode:  940  , Epsilon:  0.01 , Reward 32.24335783282012 , mean_reward:  73.46955347222237 , time_score:  500 , memory:  390596\n",
      "Episode:  945  , Epsilon:  0.01 , Reward 33.11924283174183 , mean_reward:  72.98408031028126 , time_score:  500 , memory:  393096\n",
      "Episode:  950  , Epsilon:  0.01 , Reward 6.2610881448600555 , mean_reward:  71.56875266329887 , time_score:  500 , memory:  395596\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 19.502659039947908 , mean_reward:  70.00717895381264 , time_score:  500 , memory:  398096\n",
      "Episode:  960  , Epsilon:  0.01 , Reward 73.16284220288675 , mean_reward:  69.60460141244407 , time_score:  500 , memory:  400596\n",
      "Episode:  965  , Epsilon:  0.01 , Reward 55.312947992388274 , mean_reward:  69.87866448352655 , time_score:  500 , memory:  403096\n",
      "Episode:  970  , Epsilon:  0.01 , Reward 106.48464340480713 , mean_reward:  68.41927381362265 , time_score:  500 , memory:  405596\n",
      "Episode:  975  , Epsilon:  0.01 , Reward 48.74454451546745 , mean_reward:  67.76994789594158 , time_score:  500 , memory:  408096\n",
      "Episode:  980  , Epsilon:  0.01 , Reward 83.2506912600125 , mean_reward:  69.76761684072693 , time_score:  500 , memory:  410550\n",
      "Episode:  985  , Epsilon:  0.01 , Reward 54.89846739952435 , mean_reward:  69.37310891063484 , time_score:  500 , memory:  413050\n",
      "Episode:  990  , Epsilon:  0.01 , Reward 67.67475161402932 , mean_reward:  69.664124145031 , time_score:  500 , memory:  415550\n",
      "Episode:  995  , Epsilon:  0.01 , Reward 64.71363500679301 , mean_reward:  66.7126819090184 , time_score:  500 , memory:  418050\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward 76.99818081328637 , mean_reward:  66.6964603674491 , time_score:  500 , memory:  420550\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward 64.0672886836284 , mean_reward:  65.68821662353695 , time_score:  500 , memory:  423050\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward 63.31491996149061 , mean_reward:  66.85620156177158 , time_score:  500 , memory:  425550\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward 106.18239685529521 , mean_reward:  65.27359090640599 , time_score:  500 , memory:  428050\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward 61.6135318983682 , mean_reward:  64.00065730499732 , time_score:  500 , memory:  430550\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward 195.58723996705152 , mean_reward:  66.03077700524602 , time_score:  377 , memory:  432927\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward 58.19167746834482 , mean_reward:  66.8737006667567 , time_score:  500 , memory:  435427\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward 82.73293197280134 , mean_reward:  65.93043822120669 , time_score:  500 , memory:  437927\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward -3.2585106383849762 , mean_reward:  66.13703919868787 , time_score:  210 , memory:  440137\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward 86.09100046948654 , mean_reward:  67.07520168294486 , time_score:  500 , memory:  442637\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward 157.31898183178976 , mean_reward:  68.00332450887895 , time_score:  500 , memory:  445137\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward 64.17100766052668 , mean_reward:  70.06985363926518 , time_score:  500 , memory:  447587\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward 54.9129632442618 , mean_reward:  70.56178903259611 , time_score:  500 , memory:  450087\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward 229.0144451376198 , mean_reward:  72.76597610842458 , time_score:  476 , memory:  452563\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward 100.23735040636019 , mean_reward:  75.81575343107632 , time_score:  500 , memory:  454980\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward 37.92990857586083 , mean_reward:  76.04133241384189 , time_score:  500 , memory:  457480\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward 281.42981771995596 , mean_reward:  77.2252412147568 , time_score:  351 , memory:  459831\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward 91.61049053968193 , mean_reward:  79.7773468356733 , time_score:  500 , memory:  462181\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward 78.25845834514068 , mean_reward:  78.80583320372516 , time_score:  500 , memory:  464681\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward 28.07289380302236 , mean_reward:  81.4189769853682 , time_score:  500 , memory:  467015\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward 71.94636143007668 , mean_reward:  82.78640612360961 , time_score:  500 , memory:  469515\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward 63.05626930196706 , mean_reward:  83.21492409706566 , time_score:  500 , memory:  472015\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward 41.40357348981745 , mean_reward:  82.95076592624811 , time_score:  500 , memory:  474515\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 79.30598174879059 , mean_reward:  81.68617155485191 , time_score:  500 , memory:  477015\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward 62.21693494504516 , mean_reward:  81.28186901702523 , time_score:  500 , memory:  479515\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward 45.267059147628984 , mean_reward:  79.70065941518217 , time_score:  500 , memory:  482015\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward 49.94935214716475 , mean_reward:  80.37987950991543 , time_score:  500 , memory:  484515\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward 77.4581537718988 , mean_reward:  83.23107229549751 , time_score:  500 , memory:  486952\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward 94.04321747694426 , mean_reward:  83.45305048471488 , time_score:  500 , memory:  489452\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward 100.34377421863809 , mean_reward:  84.016115736309 , time_score:  500 , memory:  491952\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward 69.26543935990344 , mean_reward:  83.43660864430466 , time_score:  500 , memory:  494452\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward 54.046736450048876 , mean_reward:  82.52441314882357 , time_score:  500 , memory:  496952\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward 82.95530467733403 , mean_reward:  82.71921474073807 , time_score:  500 , memory:  499452\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 67.55914883760443 , mean_reward:  81.16423311846073 , time_score:  500 , memory:  501952\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward 71.3496843306026 , mean_reward:  77.37686909412442 , time_score:  500 , memory:  504452\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward 80.78368335763774 , mean_reward:  79.61921060688732 , time_score:  500 , memory:  506754\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward 30.57225738172823 , mean_reward:  76.51731556721616 , time_score:  500 , memory:  509254\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward 261.1485378526878 , mean_reward:  76.6924546004331 , time_score:  398 , memory:  511652\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward 79.15459920394784 , mean_reward:  77.44812872788486 , time_score:  500 , memory:  514152\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward 50.788637264961466 , mean_reward:  76.74729686550835 , time_score:  500 , memory:  516449\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward 44.79690860931719 , mean_reward:  77.43543632369902 , time_score:  500 , memory:  518786\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward 67.86436974278648 , mean_reward:  78.56939678277291 , time_score:  500 , memory:  521286\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 80.805564408252 , mean_reward:  78.33209727513741 , time_score:  500 , memory:  523786\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward 76.07590455286613 , mean_reward:  78.61171145199603 , time_score:  500 , memory:  526286\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward 115.91518682655517 , mean_reward:  80.16569376028612 , time_score:  500 , memory:  528786\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward 237.34641151101928 , mean_reward:  81.97597397516364 , time_score:  342 , memory:  531128\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward 70.41224150973076 , mean_reward:  81.5801312476023 , time_score:  500 , memory:  533628\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward 104.95561772668292 , mean_reward:  82.10363377670143 , time_score:  500 , memory:  536059\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 90.93749709894415 , mean_reward:  83.85277150600591 , time_score:  500 , memory:  538559\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 82.05657762531193 , mean_reward:  84.81817139205867 , time_score:  500 , memory:  541059\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 124.63417462393457 , mean_reward:  86.61497894709102 , time_score:  500 , memory:  543559\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward 58.05126904349387 , mean_reward:  86.26633636778558 , time_score:  500 , memory:  546059\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward 52.73175994316545 , mean_reward:  88.31958606398074 , time_score:  500 , memory:  548383\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 55.98687474455255 , mean_reward:  88.40222696432129 , time_score:  500 , memory:  550883\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward 91.21967496793732 , mean_reward:  90.15773122500703 , time_score:  500 , memory:  553383\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward 92.31164892251975 , mean_reward:  90.04556442863043 , time_score:  500 , memory:  555880\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward 61.88457587097982 , mean_reward:  90.98162633919884 , time_score:  500 , memory:  558380\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward 80.10309380828215 , mean_reward:  91.05153611720067 , time_score:  500 , memory:  560832\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward 65.06633503815385 , mean_reward:  90.32281100844986 , time_score:  500 , memory:  563332\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward 233.66432903464786 , mean_reward:  97.94527482299235 , time_score:  308 , memory:  565039\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward 60.749592125919385 , mean_reward:  98.91717922569116 , time_score:  500 , memory:  567333\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward 22.270576262271355 , mean_reward:  100.5771094930007 , time_score:  500 , memory:  569595\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward 91.81837870703718 , mean_reward:  104.51903754716044 , time_score:  500 , memory:  571641\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward 89.22713516120425 , mean_reward:  108.08518360661013 , time_score:  500 , memory:  573877\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 226.16113600145053 , mean_reward:  110.8298049767168 , time_score:  381 , memory:  576246\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward 87.30706472006416 , mean_reward:  112.94100684446083 , time_score:  500 , memory:  578470\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward 57.20944778385708 , mean_reward:  114.12716737969724 , time_score:  500 , memory:  580842\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward 137.60855770235887 , mean_reward:  114.86589890617677 , time_score:  500 , memory:  583170\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward 35.146202911121044 , mean_reward:  116.42051037477489 , time_score:  500 , memory:  585455\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward 258.22896781128077 , mean_reward:  118.08347324501375 , time_score:  347 , memory:  587585\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward 239.21281854941608 , mean_reward:  121.24913669318407 , time_score:  299 , memory:  589586\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward 203.0270249506933 , mean_reward:  121.52000435987675 , time_score:  367 , memory:  591953\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward 266.4140489392635 , mean_reward:  120.90194318762956 , time_score:  290 , memory:  594243\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward 44.80908763028542 , mean_reward:  121.78248132292208 , time_score:  500 , memory:  596743\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward 50.98625079658765 , mean_reward:  124.23395169918471 , time_score:  500 , memory:  598640\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward 33.602677288188765 , mean_reward:  125.72115444537289 , time_score:  500 , memory:  600860\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward 22.502911119149516 , mean_reward:  127.89744961719984 , time_score:  500 , memory:  603034\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward 245.0832664203493 , mean_reward:  134.4138931447618 , time_score:  396 , memory:  604827\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward 268.3809090575566 , mean_reward:  139.34229879532856 , time_score:  311 , memory:  606764\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward 181.12113696799412 , mean_reward:  138.99225922824536 , time_score:  465 , memory:  608491\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward 81.47580735864706 , mean_reward:  142.10211066129543 , time_score:  500 , memory:  610516\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward 243.9620039150985 , mean_reward:  144.61817777308394 , time_score:  306 , memory:  612348\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward 41.51556802172062 , mean_reward:  142.55568322293345 , time_score:  500 , memory:  614551\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward 216.2975220713215 , mean_reward:  142.75886184657128 , time_score:  386 , memory:  616761\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward 27.067042764303558 , mean_reward:  140.47999122130634 , time_score:  500 , memory:  619015\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward 269.28455799752135 , mean_reward:  142.5798624253897 , time_score:  308 , memory:  620988\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward 205.1222293281304 , mean_reward:  144.47547351431064 , time_score:  358 , memory:  623191\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward 272.53701695950394 , mean_reward:  143.58090353327793 , time_score:  327 , memory:  625518\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward 252.33455407306525 , mean_reward:  144.56840670762355 , time_score:  354 , memory:  627673\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward 268.88086519486365 , mean_reward:  147.68015199602203 , time_score:  305 , memory:  629495\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward 42.03128272096498 , mean_reward:  145.1250089664482 , time_score:  500 , memory:  631696\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward 28.981912732901744 , mean_reward:  146.52085869182756 , time_score:  500 , memory:  634124\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward 229.27341617353076 , mean_reward:  150.3820482503853 , time_score:  332 , memory:  636283\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward 37.779163444827525 , mean_reward:  153.8311173503616 , time_score:  500 , memory:  638454\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward 22.37306529498684 , mean_reward:  154.5798529355145 , time_score:  500 , memory:  640586\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward 39.317623485532586 , mean_reward:  151.24341374521694 , time_score:  500 , memory:  643086\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward 41.13024454201298 , mean_reward:  152.40902528437306 , time_score:  500 , memory:  645214\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward 24.262402696287733 , mean_reward:  146.34834191043154 , time_score:  500 , memory:  647313\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward 80.45737356785676 , mean_reward:  143.78345531174142 , time_score:  500 , memory:  649752\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward 265.7727041866405 , mean_reward:  143.01204267580317 , time_score:  301 , memory:  651334\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward 86.45347447551984 , mean_reward:  140.0589644833553 , time_score:  500 , memory:  653577\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward 273.81509875931334 , mean_reward:  145.3911252015348 , time_score:  316 , memory:  655248\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward 229.9943864822881 , mean_reward:  148.81992359497815 , time_score:  272 , memory:  657125\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward 75.15628977895219 , mean_reward:  150.20311076425747 , time_score:  500 , memory:  659029\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward 19.821013138159227 , mean_reward:  152.7165026054997 , time_score:  500 , memory:  661244\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward 213.85640742291693 , mean_reward:  154.6788530053157 , time_score:  394 , memory:  663266\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward 233.70303585973716 , mean_reward:  154.58028181159148 , time_score:  293 , memory:  665213\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward 114.82405900950482 , mean_reward:  152.28917978951208 , time_score:  500 , memory:  667713\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward 217.7463955833641 , mean_reward:  151.79780472205263 , time_score:  385 , memory:  670075\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward 258.60226576342666 , mean_reward:  150.79438123036013 , time_score:  444 , memory:  672286\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward 209.75269573274852 , mean_reward:  156.14277839350143 , time_score:  370 , memory:  674302\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward 207.1594966045143 , mean_reward:  152.10407453467246 , time_score:  467 , memory:  676348\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward 192.33540056432292 , mean_reward:  151.22288220933441 , time_score:  396 , memory:  678606\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward 160.45818263225328 , mean_reward:  146.20602365837985 , time_score:  280 , memory:  680711\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward 242.6398757607287 , mean_reward:  143.67947534036009 , time_score:  309 , memory:  682864\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward 267.79640874481356 , mean_reward:  153.13448791286163 , time_score:  450 , memory:  684572\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward 257.58553832419284 , mean_reward:  154.08817574557753 , time_score:  444 , memory:  686334\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward 240.25517074425042 , mean_reward:  154.6860948497655 , time_score:  332 , memory:  688411\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward 197.08224792857055 , mean_reward:  155.2554493110736 , time_score:  350 , memory:  690533\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward 70.47391624780244 , mean_reward:  148.22120454324875 , time_score:  500 , memory:  692939\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward 78.30473888296406 , mean_reward:  147.91593083201525 , time_score:  500 , memory:  695246\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward 239.8279174654835 , mean_reward:  139.292239198619 , time_score:  306 , memory:  697552\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward 216.38712234956714 , mean_reward:  136.3334260445331 , time_score:  382 , memory:  699902\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward 168.7865002613857 , mean_reward:  136.00337553583836 , time_score:  399 , memory:  702098\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward 43.88742643030791 , mean_reward:  131.52976594572922 , time_score:  500 , memory:  704598\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward 236.36894297306742 , mean_reward:  130.01903022780417 , time_score:  440 , memory:  706431\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward 215.24244824899296 , mean_reward:  132.1752266043361 , time_score:  302 , memory:  708581\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward 203.6732609855871 , mean_reward:  136.26610846406996 , time_score:  471 , memory:  710757\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward 252.5162350598734 , mean_reward:  140.2794331852853 , time_score:  397 , memory:  712576\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward 124.2665222390715 , mean_reward:  139.71613247081387 , time_score:  500 , memory:  714743\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 61.96227962604942 , mean_reward:  133.55795473587975 , time_score:  500 , memory:  717243\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward 91.55189192976722 , mean_reward:  138.58095918852197 , time_score:  500 , memory:  719610\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward 83.91460131556639 , mean_reward:  136.92061565860502 , time_score:  500 , memory:  722027\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward -19.567882634343068 , mean_reward:  140.68527996362954 , time_score:  500 , memory:  724180\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward 271.96461381666853 , mean_reward:  146.23745817410196 , time_score:  361 , memory:  726075\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward 58.839633495594605 , mean_reward:  139.2930117091527 , time_score:  500 , memory:  728404\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward 175.97377152915908 , mean_reward:  137.9475713290286 , time_score:  401 , memory:  730710\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward 227.1310536724635 , mean_reward:  137.01610042418096 , time_score:  449 , memory:  733159\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward 214.42668651606863 , mean_reward:  137.8668501532644 , time_score:  443 , memory:  735390\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward 52.251445503774264 , mean_reward:  140.865151047622 , time_score:  500 , memory:  737574\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward 248.31008864127944 , mean_reward:  146.03774337323057 , time_score:  333 , memory:  739415\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward 55.4159434262292 , mean_reward:  150.79208843761683 , time_score:  500 , memory:  741279\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward 257.8842819056982 , mean_reward:  156.3688758101426 , time_score:  340 , memory:  743288\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward 115.43064836386436 , mean_reward:  158.27046074625093 , time_score:  500 , memory:  745341\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward 39.131481767830586 , mean_reward:  163.13729017048544 , time_score:  500 , memory:  747618\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward 246.20344539470818 , mean_reward:  163.21358551637502 , time_score:  370 , memory:  749693\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward 101.68156458877625 , mean_reward:  163.9241483566659 , time_score:  500 , memory:  751924\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward 257.92357523996816 , mean_reward:  167.14697949450624 , time_score:  379 , memory:  753936\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward 265.0348410822611 , mean_reward:  167.48551100463766 , time_score:  355 , memory:  755979\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward 249.69381574846443 , mean_reward:  171.674176577181 , time_score:  343 , memory:  757753\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward 260.9410141273088 , mean_reward:  176.73822114991512 , time_score:  311 , memory:  759709\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward 257.70808287210906 , mean_reward:  180.06841823513435 , time_score:  364 , memory:  761922\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward 281.3605461491637 , mean_reward:  184.3523506995524 , time_score:  341 , memory:  763839\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward 208.33810493130275 , mean_reward:  189.69365019565285 , time_score:  435 , memory:  765721\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward 295.33258711966204 , mean_reward:  191.4534334372953 , time_score:  321 , memory:  767349\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward 206.94848921763497 , mean_reward:  198.24387847388306 , time_score:  410 , memory:  769142\n",
      "BRAVO, GOAL ACHIEVED!!!\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.001, lr=0.0005)\n",
    "R, R_moving = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QzXeEPyZkZx5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-42d00c677984>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tab:green'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episodes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:green'\n",
    "ax1.set_xlabel('Episodes')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.plot(df.iloc[:,0], df.iloc[:,2:3], color=color)\n",
    "ax1.plot(df.iloc[:,0], df.iloc[:,3:4], color='red')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Epsilon')  # we already handled the x-label with ax1\n",
    "ax2.plot(df.iloc[:,0], df.iloc[:,1:2], color=color)\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
