{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"g95_ep0p995_0p001_0p0001.csv\"\n",
    "        self.model_filename = \"g95_ep0p995_0p001_0p0001.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "            if e % 100 == 0:\n",
    "\n",
    "              self.Q_model.save(self.model_filename)\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "              self.ep *= self.ep_decay\n",
    "            else:\n",
    "              self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "\n",
    "        with open(self.csv_filename, 'a') as f:\n",
    "          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return self.df_ddqn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -362.7470932740918 , mean_reward:  -362.7470932740918 , time_score:  80 , memory:  80\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -334.648061059102 , mean_reward:  -203.37628143912698 , time_score:  97 , memory:  518\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -6.772729875144719 , mean_reward:  -172.65324956017733 , time_score:  90 , memory:  966\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -25.82823239114859 , mean_reward:  -171.71418446007556 , time_score:  121 , memory:  1496\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -240.42250897433803 , mean_reward:  -163.45116609578446 , time_score:  64 , memory:  1961\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -142.42670769018622 , mean_reward:  -181.5727829770601 , time_score:  89 , memory:  2477\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -80.08731584663374 , mean_reward:  -175.80813099604572 , time_score:  77 , memory:  2965\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -237.9952984620398 , mean_reward:  -194.3787352655206 , time_score:  98 , memory:  3480\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -409.0364216444567 , mean_reward:  -197.66572396047644 , time_score:  106 , memory:  3974\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -306.77856326620605 , mean_reward:  -207.26917676363578 , time_score:  123 , memory:  4463\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward -253.57267535143447 , mean_reward:  -207.55379109709284 , time_score:  98 , memory:  4957\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -130.32581015151968 , mean_reward:  -198.2675442804049 , time_score:  103 , memory:  5465\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -46.77891516686525 , mean_reward:  -195.77800907991542 , time_score:  75 , memory:  6153\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -61.23456251049897 , mean_reward:  -195.04355590104194 , time_score:  74 , memory:  6685\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -166.85683504567655 , mean_reward:  -197.77712147114016 , time_score:  70 , memory:  7356\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -421.2330059651214 , mean_reward:  -200.75712675979418 , time_score:  132 , memory:  8002\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward -135.35890105656262 , mean_reward:  -199.79133410815209 , time_score:  92 , memory:  8711\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -218.40912140447696 , mean_reward:  -198.85959178125805 , time_score:  161 , memory:  9498\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -304.2887862541654 , mean_reward:  -195.95428185528482 , time_score:  184 , memory:  10172\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -173.80473601050284 , mean_reward:  -193.93053426941023 , time_score:  147 , memory:  11067\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -245.79001381622612 , mean_reward:  -190.72264854288215 , time_score:  155 , memory:  11895\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -273.5966570578583 , mean_reward:  -191.39859934410927 , time_score:  108 , memory:  12539\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -100.94760410598639 , mean_reward:  -194.47623039746344 , time_score:  102 , memory:  13177\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -104.69987998328892 , mean_reward:  -190.93020256379157 , time_score:  235 , memory:  14202\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -268.1309804225867 , mean_reward:  -193.03261391324278 , time_score:  122 , memory:  14997\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -154.80458289848485 , mean_reward:  -189.80795716921205 , time_score:  142 , memory:  15730\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -256.16824807153773 , mean_reward:  -191.11836204255556 , time_score:  167 , memory:  16484\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -126.3891688561513 , mean_reward:  -183.4190878673906 , time_score:  149 , memory:  17328\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward -224.59410438310059 , mean_reward:  -179.22452014498558 , time_score:  164 , memory:  18067\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward -224.101493765867 , mean_reward:  -172.68746460425484 , time_score:  467 , memory:  19221\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -103.44386488228494 , mean_reward:  -169.86695525217354 , time_score:  228 , memory:  20307\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -193.41041520890656 , mean_reward:  -174.62633001700848 , time_score:  215 , memory:  21331\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -196.33581085660919 , mean_reward:  -173.91892444079065 , time_score:  122 , memory:  22190\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward -245.53986521626052 , mean_reward:  -172.26441110142162 , time_score:  117 , memory:  23167\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -227.63909531783548 , mean_reward:  -166.4195530702162 , time_score:  185 , memory:  24054\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward -153.57085474665604 , mean_reward:  -161.01731667980604 , time_score:  281 , memory:  25085\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward -116.46784450065171 , mean_reward:  -159.67849790893132 , time_score:  224 , memory:  26349\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -291.76258787561176 , mean_reward:  -160.89060274820295 , time_score:  268 , memory:  27648\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward -183.43055360201572 , mean_reward:  -160.36184699435012 , time_score:  389 , memory:  29084\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -18.81958291760877 , mean_reward:  -158.49974355186245 , time_score:  236 , memory:  30631\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -56.285889347582454 , mean_reward:  -155.5635388572643 , time_score:  359 , memory:  31949\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward -213.84796041451125 , mean_reward:  -155.4924508305879 , time_score:  334 , memory:  32955\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -116.43458945556354 , mean_reward:  -151.03615720407907 , time_score:  150 , memory:  34290\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward -266.8338798811645 , mean_reward:  -152.11656577112686 , time_score:  332 , memory:  35724\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward 8.064351036047029 , mean_reward:  -146.48937061205874 , time_score:  279 , memory:  37072\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward -193.24827598536342 , mean_reward:  -141.07452887042768 , time_score:  231 , memory:  38408\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward -53.41529302650389 , mean_reward:  -137.54648967932474 , time_score:  500 , memory:  40208\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward -129.3430713070626 , mean_reward:  -139.1016226622518 , time_score:  242 , memory:  41791\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward -190.74889076263406 , mean_reward:  -140.34779602654274 , time_score:  204 , memory:  43362\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward -247.3754208264087 , mean_reward:  -137.23916425340866 , time_score:  497 , memory:  45283\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward -63.52463809507273 , mean_reward:  -135.205708508229 , time_score:  500 , memory:  47564\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward -225.87606137767534 , mean_reward:  -132.42217513472488 , time_score:  280 , memory:  49254\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward 1.0525016471975184 , mean_reward:  -131.15383145492973 , time_score:  500 , memory:  51139\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward -59.438224801248765 , mean_reward:  -125.42927593419044 , time_score:  500 , memory:  53348\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward 7.3927758592107295 , mean_reward:  -125.36013207942675 , time_score:  267 , memory:  54904\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward 40.02242534680898 , mean_reward:  -120.96671480054705 , time_score:  500 , memory:  57371\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward -278.0566678902712 , mean_reward:  -119.16138417512312 , time_score:  382 , memory:  59439\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward 18.89526127119732 , mean_reward:  -112.78888873395863 , time_score:  500 , memory:  61295\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward -14.62921790318653 , mean_reward:  -107.05563583871793 , time_score:  500 , memory:  63531\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward -268.369621051996 , mean_reward:  -114.36628248844299 , time_score:  272 , memory:  65141\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward -52.3105614547234 , mean_reward:  -111.78246811892946 , time_score:  213 , memory:  67169\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward -217.22618177732454 , mean_reward:  -104.98260968675433 , time_score:  316 , memory:  68902\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward -14.25701265198754 , mean_reward:  -102.45113870930294 , time_score:  500 , memory:  71309\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward 8.637576857354858 , mean_reward:  -101.26745461956689 , time_score:  500 , memory:  73226\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward -28.38568199228817 , mean_reward:  -98.97120871994034 , time_score:  500 , memory:  75429\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward 1.0209437630618508 , mean_reward:  -96.08153382131395 , time_score:  500 , memory:  77813\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward -55.050067894238644 , mean_reward:  -91.87079481865669 , time_score:  500 , memory:  80313\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward 1.6773863685259223 , mean_reward:  -82.20144974289386 , time_score:  500 , memory:  82453\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward -24.929672294802767 , mean_reward:  -77.26884459792997 , time_score:  500 , memory:  84929\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward -12.621566762826513 , mean_reward:  -73.40885667232072 , time_score:  500 , memory:  87429\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward -13.496712928963506 , mean_reward:  -68.15988920023952 , time_score:  500 , memory:  89929\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward -7.958836444426812 , mean_reward:  -63.20273247772068 , time_score:  500 , memory:  92311\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward 42.74380355023099 , mean_reward:  -56.92873609542094 , time_score:  500 , memory:  94811\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward -34.562147287762315 , mean_reward:  -55.703820942137234 , time_score:  366 , memory:  97177\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward 12.80597127390851 , mean_reward:  -49.2305884690965 , time_score:  500 , memory:  99677\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward -9.909024179918596 , mean_reward:  -46.271403493598164 , time_score:  500 , memory:  102177\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward -3.736174451767809 , mean_reward:  -41.00661422643347 , time_score:  500 , memory:  104677\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward -20.009513785776125 , mean_reward:  -38.516416108172955 , time_score:  500 , memory:  107177\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward 56.06327789554882 , mean_reward:  -37.81017779428077 , time_score:  500 , memory:  109677\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward 11.597642443631555 , mean_reward:  -24.042612458716302 , time_score:  500 , memory:  112177\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward -39.28984922412897 , mean_reward:  -21.537817460682223 , time_score:  500 , memory:  114677\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward 27.808629325442034 , mean_reward:  -20.710763397617473 , time_score:  500 , memory:  116914\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward -0.014472450191043151 , mean_reward:  -17.74051199758656 , time_score:  500 , memory:  119414\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward 26.139388284440557 , mean_reward:  -13.083528442358688 , time_score:  500 , memory:  121914\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward -10.429017907891819 , mean_reward:  -13.372506265339583 , time_score:  500 , memory:  124414\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward 3.6000563803927994 , mean_reward:  -12.873371314325626 , time_score:  500 , memory:  126914\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward 37.75710887807257 , mean_reward:  -10.821234024743248 , time_score:  500 , memory:  129374\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward 27.917099792797636 , mean_reward:  -12.374392526731107 , time_score:  500 , memory:  131812\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward 44.624974232734175 , mean_reward:  -8.029795431835053 , time_score:  500 , memory:  134312\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward -25.435525408540727 , mean_reward:  -10.247069233712676 , time_score:  500 , memory:  136787\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward 27.869172220029036 , mean_reward:  -11.198375540400182 , time_score:  500 , memory:  139287\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward -37.870838318397155 , mean_reward:  -9.219207007137914 , time_score:  500 , memory:  141787\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward 26.266459733248762 , mean_reward:  -8.347236285101552 , time_score:  500 , memory:  144287\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward -38.45881062591633 , mean_reward:  -9.217119513045354 , time_score:  500 , memory:  146787\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward 4.543318253822448 , mean_reward:  -9.778434849084885 , time_score:  500 , memory:  149287\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward -10.978776421719523 , mean_reward:  -11.295076428790857 , time_score:  500 , memory:  151787\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward 43.7345706499446 , mean_reward:  -10.910054996095523 , time_score:  500 , memory:  154268\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward 18.83472787364445 , mean_reward:  -10.506407896920415 , time_score:  500 , memory:  156768\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward -46.629120002837865 , mean_reward:  -11.560002749831988 , time_score:  500 , memory:  159268\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward -40.55283837756687 , mean_reward:  -12.142284709016044 , time_score:  500 , memory:  161768\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward 12.825559780639 , mean_reward:  -11.710632356978433 , time_score:  500 , memory:  164268\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward -27.52147529289816 , mean_reward:  -9.352717910522635 , time_score:  500 , memory:  166768\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward -20.6914207671559 , mean_reward:  -11.08530443941783 , time_score:  500 , memory:  169268\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward 21.31724959781614 , mean_reward:  -10.810680524627287 , time_score:  500 , memory:  171768\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward -68.3542863264252 , mean_reward:  -10.418788252850717 , time_score:  500 , memory:  174268\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward 3.279400246574025 , mean_reward:  -9.809702804305369 , time_score:  500 , memory:  176768\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward 9.097014214851876 , mean_reward:  -11.267942892794784 , time_score:  500 , memory:  179268\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward 18.31236184794978 , mean_reward:  -10.8521574791413 , time_score:  500 , memory:  181768\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward 6.089925169126091 , mean_reward:  -13.491151610439506 , time_score:  500 , memory:  184268\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward 30.808051645465277 , mean_reward:  -10.760315991626562 , time_score:  500 , memory:  186768\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward 11.747785144608114 , mean_reward:  -10.642486112241079 , time_score:  500 , memory:  189268\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward -53.391846822637284 , mean_reward:  -11.590151790265974 , time_score:  500 , memory:  191768\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward -35.931844789169496 , mean_reward:  -13.44089597905352 , time_score:  500 , memory:  194268\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward 58.151461560728364 , mean_reward:  -11.974299796875673 , time_score:  500 , memory:  196768\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward -44.984173314148485 , mean_reward:  -13.22397472506548 , time_score:  500 , memory:  199268\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward -13.808176333552886 , mean_reward:  -13.728066992654563 , time_score:  500 , memory:  201768\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward 10.341529883590649 , mean_reward:  -13.705713926411063 , time_score:  500 , memory:  204268\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward -56.69308743830141 , mean_reward:  -13.963758626706744 , time_score:  500 , memory:  206768\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward 25.011493275680493 , mean_reward:  -11.608886611120692 , time_score:  500 , memory:  209268\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward -36.7195172966828 , mean_reward:  -13.061010523284784 , time_score:  500 , memory:  211768\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward -43.626372782138795 , mean_reward:  -14.033674432316316 , time_score:  500 , memory:  214268\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward -5.922685434031971 , mean_reward:  -15.065416998058652 , time_score:  500 , memory:  216768\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward -36.80388073231259 , mean_reward:  -15.118639057963751 , time_score:  500 , memory:  219268\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward -9.210490870764634 , mean_reward:  -16.84160517219916 , time_score:  500 , memory:  221768\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward -46.70395715436704 , mean_reward:  -17.472584872336675 , time_score:  500 , memory:  224268\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward -76.52508253578385 , mean_reward:  -18.70553123020518 , time_score:  500 , memory:  226768\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward -38.30196753750248 , mean_reward:  -19.85768913144966 , time_score:  500 , memory:  229268\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward 0.2914754826218652 , mean_reward:  -20.647856823069688 , time_score:  500 , memory:  231768\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward 9.545212620874997 , mean_reward:  -20.50719364997396 , time_score:  500 , memory:  234268\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward -0.1491721037739846 , mean_reward:  -21.062753708865472 , time_score:  500 , memory:  236768\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward 12.072002489661818 , mean_reward:  -21.83062665231928 , time_score:  500 , memory:  239268\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward -57.701949665250716 , mean_reward:  -22.43770080169495 , time_score:  500 , memory:  241768\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 41.45324491729046 , mean_reward:  -21.063051394221148 , time_score:  500 , memory:  244268\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward 8.444132314829083 , mean_reward:  -21.516753424809625 , time_score:  500 , memory:  246768\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward -4.5903502606389415 , mean_reward:  -22.385138344113393 , time_score:  500 , memory:  249268\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward 4.240214016525107 , mean_reward:  -21.51863561766843 , time_score:  500 , memory:  251768\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward -48.67039811751633 , mean_reward:  -23.137046965012143 , time_score:  500 , memory:  254268\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward 37.76823376671125 , mean_reward:  -22.5106450082407 , time_score:  500 , memory:  256768\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward -31.110460985780485 , mean_reward:  -25.120962743306073 , time_score:  500 , memory:  259268\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward -54.782438909326295 , mean_reward:  -25.69956163969909 , time_score:  500 , memory:  261768\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward -10.935148302024537 , mean_reward:  -25.80578656686113 , time_score:  500 , memory:  264268\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward -15.783618890915225 , mean_reward:  -25.808458387580366 , time_score:  500 , memory:  266768\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward -69.69589693238319 , mean_reward:  -26.613504307388563 , time_score:  500 , memory:  269268\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 51.66248099013686 , mean_reward:  -26.207360849014673 , time_score:  500 , memory:  271768\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward -48.97292854511673 , mean_reward:  -26.000401826068796 , time_score:  500 , memory:  274268\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 18.230771384028127 , mean_reward:  -25.620199412091196 , time_score:  500 , memory:  276768\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward -12.19710482451671 , mean_reward:  -24.908283720929486 , time_score:  500 , memory:  279268\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward -39.15354335261243 , mean_reward:  -24.69654765011485 , time_score:  500 , memory:  281768\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 3.3524939017580104 , mean_reward:  -24.26702845366885 , time_score:  500 , memory:  284268\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward -54.17523208063981 , mean_reward:  -24.370728619943282 , time_score:  500 , memory:  286768\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward 15.689106094860854 , mean_reward:  -24.23578124902292 , time_score:  500 , memory:  289268\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward -41.29257107737086 , mean_reward:  -23.49747063025664 , time_score:  500 , memory:  291768\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward -27.0026417629123 , mean_reward:  -25.2971301836137 , time_score:  500 , memory:  294268\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward -4.884462305472425 , mean_reward:  -26.03224002202733 , time_score:  500 , memory:  296768\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward -28.981423897890217 , mean_reward:  -26.18721020502013 , time_score:  500 , memory:  299268\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward -40.428579296843466 , mean_reward:  -27.277907611507903 , time_score:  500 , memory:  301768\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward -23.12269429367841 , mean_reward:  -25.570542364602744 , time_score:  500 , memory:  304268\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward -39.115609684842894 , mean_reward:  -26.151554219594917 , time_score:  500 , memory:  306768\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward 30.317512286027984 , mean_reward:  -26.2514044339069 , time_score:  500 , memory:  309268\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 15.047132958814624 , mean_reward:  -25.217662938250967 , time_score:  500 , memory:  311768\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward 7.189986710097763 , mean_reward:  -25.015103711361334 , time_score:  500 , memory:  314268\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward -26.32959046669893 , mean_reward:  -25.92803201968281 , time_score:  500 , memory:  316768\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward -11.647051948765458 , mean_reward:  -24.1968739486715 , time_score:  500 , memory:  319268\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward -43.04535207457097 , mean_reward:  -24.106065679048758 , time_score:  500 , memory:  321768\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward -39.291325903947794 , mean_reward:  -23.367393890883847 , time_score:  500 , memory:  324268\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward -46.60360226318262 , mean_reward:  -24.313430969759008 , time_score:  500 , memory:  326768\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward -32.45112261064603 , mean_reward:  -23.893130872704795 , time_score:  500 , memory:  329268\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward -46.483268288435625 , mean_reward:  -23.502310036193922 , time_score:  500 , memory:  331768\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward -21.69090371407346 , mean_reward:  -23.62836621188426 , time_score:  500 , memory:  334268\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward 42.82774513506028 , mean_reward:  -23.19918811494944 , time_score:  500 , memory:  336768\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward -57.529352494845114 , mean_reward:  -21.790655365471963 , time_score:  500 , memory:  339268\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward -25.137665420114207 , mean_reward:  -21.868802272233985 , time_score:  500 , memory:  341768\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward -23.54004892313782 , mean_reward:  -22.225611540603186 , time_score:  500 , memory:  344268\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward -30.88528380598551 , mean_reward:  -22.22081969167391 , time_score:  500 , memory:  346768\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward 0.31794930347745765 , mean_reward:  -21.40251736820226 , time_score:  500 , memory:  349268\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward -73.68975376474334 , mean_reward:  -21.60480480504651 , time_score:  500 , memory:  351768\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward 13.401825026946486 , mean_reward:  -22.528552561694077 , time_score:  500 , memory:  354268\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward -64.43830560082597 , mean_reward:  -23.14585874358639 , time_score:  500 , memory:  356768\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward -44.73314591936622 , mean_reward:  -22.92614318877765 , time_score:  500 , memory:  359268\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward 6.2398613064872555 , mean_reward:  -22.791799821810628 , time_score:  500 , memory:  361768\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward -45.96717847241144 , mean_reward:  -23.510990304795698 , time_score:  500 , memory:  364268\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward 0.5319619186344147 , mean_reward:  -23.303740314152172 , time_score:  500 , memory:  366768\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward -17.80398915725068 , mean_reward:  -22.718204949087696 , time_score:  500 , memory:  369268\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward -19.736397337592166 , mean_reward:  -21.599290313454443 , time_score:  500 , memory:  371768\n",
      "Episode:  920  , Epsilon:  0.01 , Reward -3.393485046614737 , mean_reward:  -22.443194540588753 , time_score:  500 , memory:  374268\n",
      "Episode:  925  , Epsilon:  0.01 , Reward -17.752837199018046 , mean_reward:  -21.149856471898094 , time_score:  500 , memory:  376768\n",
      "Episode:  930  , Epsilon:  0.01 , Reward -22.41698332434025 , mean_reward:  -21.457708642620968 , time_score:  500 , memory:  379268\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 25.01892190991421 , mean_reward:  -20.16635683938499 , time_score:  500 , memory:  381768\n",
      "Episode:  940  , Epsilon:  0.01 , Reward -38.805430239597314 , mean_reward:  -17.439648236764622 , time_score:  500 , memory:  384268\n",
      "Episode:  945  , Epsilon:  0.01 , Reward -15.862710560441148 , mean_reward:  -17.293235422442415 , time_score:  500 , memory:  386768\n",
      "Episode:  950  , Epsilon:  0.01 , Reward -38.21881110484988 , mean_reward:  -16.98552397352217 , time_score:  500 , memory:  389268\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 26.57391197450444 , mean_reward:  -15.559065270163256 , time_score:  500 , memory:  391768\n",
      "Episode:  960  , Epsilon:  0.01 , Reward -36.87880795044814 , mean_reward:  -14.887695803716587 , time_score:  500 , memory:  394268\n",
      "Episode:  965  , Epsilon:  0.01 , Reward -53.113810111245606 , mean_reward:  -14.069714731719507 , time_score:  500 , memory:  396768\n",
      "Episode:  970  , Epsilon:  0.01 , Reward 30.240666373638135 , mean_reward:  -12.620972674942738 , time_score:  500 , memory:  399268\n",
      "Episode:  975  , Epsilon:  0.01 , Reward -31.695941559869528 , mean_reward:  -10.417827536632585 , time_score:  500 , memory:  401768\n",
      "Episode:  980  , Epsilon:  0.01 , Reward -72.08658198059365 , mean_reward:  -9.82297620137124 , time_score:  500 , memory:  404268\n",
      "Episode:  985  , Epsilon:  0.01 , Reward 25.989236542498237 , mean_reward:  -7.856706070644232 , time_score:  500 , memory:  406768\n",
      "Episode:  990  , Epsilon:  0.01 , Reward -32.064650777992 , mean_reward:  -7.128709248413021 , time_score:  500 , memory:  409268\n",
      "Episode:  995  , Epsilon:  0.01 , Reward -6.091609264122241 , mean_reward:  -7.072016428821064 , time_score:  500 , memory:  411768\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward -0.9646788887997251 , mean_reward:  -6.611433874676359 , time_score:  500 , memory:  414268\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward -2.856469880140891 , mean_reward:  -6.49174997739811 , time_score:  500 , memory:  416768\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward -11.608354388057055 , mean_reward:  -6.74743790877449 , time_score:  500 , memory:  419268\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward 16.234201450365788 , mean_reward:  -7.405822075391609 , time_score:  500 , memory:  421768\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward 20.00948571654795 , mean_reward:  -4.9864074631616155 , time_score:  500 , memory:  424268\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward -80.89670555167424 , mean_reward:  -3.6743804414468997 , time_score:  500 , memory:  426768\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward 74.13537103997066 , mean_reward:  -3.084933835352832 , time_score:  500 , memory:  429268\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward 14.003682917853343 , mean_reward:  -3.9366030372380254 , time_score:  500 , memory:  431768\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward -52.016318338336646 , mean_reward:  -6.502472465344102 , time_score:  500 , memory:  434268\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward 29.659456989992293 , mean_reward:  -4.534927567240044 , time_score:  500 , memory:  436768\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward 11.914126438805672 , mean_reward:  -3.0857641151151953 , time_score:  500 , memory:  439268\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward -44.59040627041564 , mean_reward:  -3.0121208794309773 , time_score:  500 , memory:  441768\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward 28.436538946267394 , mean_reward:  -0.7729823529927163 , time_score:  500 , memory:  444268\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward -30.718389488390052 , mean_reward:  -0.1328686427731308 , time_score:  500 , memory:  446768\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward -9.219087562859986 , mean_reward:  -1.7093652336066327 , time_score:  500 , memory:  449268\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward -30.935791030589396 , mean_reward:  -2.8820716992839097 , time_score:  500 , memory:  451768\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward -31.879086474963795 , mean_reward:  -3.002315923657191 , time_score:  500 , memory:  454268\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward -14.200598690321396 , mean_reward:  -3.114177670010345 , time_score:  500 , memory:  456768\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward -18.768073419981622 , mean_reward:  -3.0174676883149543 , time_score:  500 , memory:  459268\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward 24.832199395431317 , mean_reward:  -2.219885979342168 , time_score:  500 , memory:  461768\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward 27.020347669484114 , mean_reward:  -1.8284540383879988 , time_score:  500 , memory:  464268\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward -18.42443261943812 , mean_reward:  -0.7059938960899518 , time_score:  500 , memory:  466768\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward 35.5290028532048 , mean_reward:  -0.05868898099968412 , time_score:  500 , memory:  469268\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 20.250869017642422 , mean_reward:  1.5359960671942015 , time_score:  500 , memory:  471768\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward 26.134146838426258 , mean_reward:  1.042047603476018 , time_score:  500 , memory:  474268\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward -38.47879714743288 , mean_reward:  -1.5917908227033717 , time_score:  500 , memory:  476768\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward 42.533966118922336 , mean_reward:  -0.854113667252229 , time_score:  500 , memory:  479268\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward -15.066204358382901 , mean_reward:  -0.26699033506835806 , time_score:  500 , memory:  481768\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward 39.5799553262256 , mean_reward:  0.028394905804013604 , time_score:  500 , memory:  484268\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward 0.03909747927538976 , mean_reward:  -2.192995691246786 , time_score:  500 , memory:  486768\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward 11.633537536898935 , mean_reward:  -3.5809957227665667 , time_score:  500 , memory:  489268\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward 20.802485804014403 , mean_reward:  -2.943896845012087 , time_score:  500 , memory:  491768\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward -3.1799433123771275 , mean_reward:  -4.145712797869794 , time_score:  500 , memory:  494268\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 27.59269662166401 , mean_reward:  -3.886082442593775 , time_score:  500 , memory:  496768\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward 9.68053315347965 , mean_reward:  -2.376594667222238 , time_score:  500 , memory:  499268\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward 34.267303795613444 , mean_reward:  -0.6287652409235815 , time_score:  500 , memory:  501768\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward 32.94404863716306 , mean_reward:  -0.0986880774188824 , time_score:  500 , memory:  504268\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward -44.98532148037394 , mean_reward:  -1.0448885579434393 , time_score:  500 , memory:  506768\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward 7.010192598197039 , mean_reward:  -0.19680029643556268 , time_score:  500 , memory:  509268\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward -57.15716669364491 , mean_reward:  -0.6211066800322729 , time_score:  500 , memory:  511768\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward -20.44480258191269 , mean_reward:  0.012328512723471761 , time_score:  500 , memory:  514268\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward 13.19659547034119 , mean_reward:  -0.14143355261689533 , time_score:  500 , memory:  516768\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 31.528378747609676 , mean_reward:  -1.38163696861115 , time_score:  500 , memory:  519268\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward -33.35505723308132 , mean_reward:  -4.5396915513994704 , time_score:  500 , memory:  521768\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward 8.37445701191138 , mean_reward:  -5.495233191228075 , time_score:  500 , memory:  524268\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward 8.247491857308159 , mean_reward:  -3.378397962423962 , time_score:  500 , memory:  526768\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward 0.814948062236662 , mean_reward:  -5.1060429911167216 , time_score:  500 , memory:  529268\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward 4.007075534288725 , mean_reward:  -5.347806242143447 , time_score:  500 , memory:  531768\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 31.587722763590357 , mean_reward:  -4.275995323018759 , time_score:  500 , memory:  534268\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 29.661157405657416 , mean_reward:  -3.1153563243187303 , time_score:  500 , memory:  536768\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward -23.272287944136743 , mean_reward:  -4.961440940877527 , time_score:  500 , memory:  539268\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward -0.24806528262750271 , mean_reward:  -5.531289513354582 , time_score:  500 , memory:  541768\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward 10.592867304268989 , mean_reward:  -6.119070263105484 , time_score:  500 , memory:  544268\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward -8.420825712578122 , mean_reward:  -7.450474895234888 , time_score:  500 , memory:  546768\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward -10.670536981081677 , mean_reward:  -9.164296003847776 , time_score:  500 , memory:  549268\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward -16.86271052895601 , mean_reward:  -11.949959283370381 , time_score:  500 , memory:  551768\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward -12.013930509280442 , mean_reward:  -12.123672415798012 , time_score:  500 , memory:  554268\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward -32.391292725259746 , mean_reward:  -11.999854070024657 , time_score:  500 , memory:  556768\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward 10.113134369306202 , mean_reward:  -11.61792506967492 , time_score:  500 , memory:  559268\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward -48.693396259670344 , mean_reward:  -13.134005433987818 , time_score:  500 , memory:  561768\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward -46.738190843069646 , mean_reward:  -14.454455486960269 , time_score:  500 , memory:  564268\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward 30.517878877345336 , mean_reward:  -14.836424566021362 , time_score:  500 , memory:  566768\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward -2.0624255391528603 , mean_reward:  -15.448622056015934 , time_score:  500 , memory:  569268\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward -38.59582879657229 , mean_reward:  -14.13998908733398 , time_score:  500 , memory:  571768\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 15.079006332070989 , mean_reward:  -15.189580372531106 , time_score:  500 , memory:  574268\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward 11.617912614259565 , mean_reward:  -15.308383671897891 , time_score:  500 , memory:  576768\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward 1.552127960551305 , mean_reward:  -14.63594928117164 , time_score:  500 , memory:  579268\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward 28.65077082752804 , mean_reward:  -15.081097017036832 , time_score:  500 , memory:  581768\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward 14.502504044968722 , mean_reward:  -14.397011189384875 , time_score:  500 , memory:  584268\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward -73.9421039979803 , mean_reward:  -16.49461628670754 , time_score:  500 , memory:  586768\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward -9.133948858250122 , mean_reward:  -17.123266116738275 , time_score:  500 , memory:  589268\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward -9.58354475875026 , mean_reward:  -18.982770357492807 , time_score:  500 , memory:  591768\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward -16.97636549643914 , mean_reward:  -19.40196958375644 , time_score:  500 , memory:  594268\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward -20.27000000948101 , mean_reward:  -20.41494091985443 , time_score:  500 , memory:  596768\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward -39.72234243097298 , mean_reward:  -20.433804570366163 , time_score:  500 , memory:  599268\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward 19.095534145606447 , mean_reward:  -19.97239336500294 , time_score:  500 , memory:  601768\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward -13.012296321819559 , mean_reward:  -21.31835420852208 , time_score:  500 , memory:  604268\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward -12.410889624888004 , mean_reward:  -22.00635676201871 , time_score:  500 , memory:  606768\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward -37.062460562788644 , mean_reward:  -23.352988762520937 , time_score:  500 , memory:  609268\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward -26.79435474427653 , mean_reward:  -21.743239797057683 , time_score:  500 , memory:  611768\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward -3.0305741334374376 , mean_reward:  -22.45719413713219 , time_score:  500 , memory:  614268\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward -33.01683682608544 , mean_reward:  -23.503450296899718 , time_score:  500 , memory:  616768\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward -7.456994454020381 , mean_reward:  -23.29119314094497 , time_score:  500 , memory:  619268\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward -9.785302570262363 , mean_reward:  -22.95465192158786 , time_score:  500 , memory:  621768\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward -3.5524085929928955 , mean_reward:  -21.94024473001386 , time_score:  500 , memory:  624268\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward -15.670003260882714 , mean_reward:  -23.268668267616693 , time_score:  500 , memory:  626768\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward -34.62189514575348 , mean_reward:  -24.251059515134372 , time_score:  500 , memory:  629268\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward -68.7870800478319 , mean_reward:  -26.02967285204777 , time_score:  500 , memory:  631768\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward -60.13638032451273 , mean_reward:  -29.440780273631344 , time_score:  500 , memory:  634268\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward -81.23652380714775 , mean_reward:  -30.075862241455706 , time_score:  500 , memory:  636768\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward -60.56263517893321 , mean_reward:  -29.05794333568843 , time_score:  500 , memory:  639268\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward -4.138865435378037 , mean_reward:  -28.463581245951133 , time_score:  500 , memory:  641768\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward -49.66605265312796 , mean_reward:  -28.215069983851937 , time_score:  500 , memory:  644268\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward -55.44413593586388 , mean_reward:  -28.484808291942503 , time_score:  500 , memory:  646768\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward 13.865188469538037 , mean_reward:  -27.26486814610287 , time_score:  500 , memory:  649268\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward -30.4281034786368 , mean_reward:  -26.93430256292814 , time_score:  500 , memory:  651768\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward -21.27374477455542 , mean_reward:  -26.617571781488795 , time_score:  500 , memory:  654268\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward 11.554840282015771 , mean_reward:  -25.039944651104097 , time_score:  500 , memory:  656768\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward -34.57891459877407 , mean_reward:  -25.90203657588368 , time_score:  500 , memory:  659268\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward 19.020736564219284 , mean_reward:  -26.105538876075197 , time_score:  500 , memory:  661768\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward 2.9741799742226815 , mean_reward:  -25.09138958988555 , time_score:  500 , memory:  664268\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward -64.66216265267258 , mean_reward:  -24.867118427104018 , time_score:  500 , memory:  666768\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward -26.96668381848999 , mean_reward:  -24.746624438403078 , time_score:  500 , memory:  669268\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward -13.716117966196803 , mean_reward:  -26.281522922560463 , time_score:  500 , memory:  671768\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward -75.86513578858396 , mean_reward:  -26.78037217523827 , time_score:  500 , memory:  674268\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward -52.42994857275819 , mean_reward:  -27.008147569469248 , time_score:  500 , memory:  676768\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward -4.793938944089681 , mean_reward:  -27.428981299192795 , time_score:  500 , memory:  679268\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward -25.564385564023116 , mean_reward:  -26.180401153730955 , time_score:  500 , memory:  681768\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward -29.749639430848273 , mean_reward:  -25.502543074133367 , time_score:  500 , memory:  684268\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward -63.95520630203335 , mean_reward:  -24.794500412089167 , time_score:  500 , memory:  686768\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward -57.50209300457054 , mean_reward:  -24.357517705473178 , time_score:  500 , memory:  689268\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward -12.734810611828745 , mean_reward:  -25.05773235488256 , time_score:  500 , memory:  691768\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward -58.80674004054325 , mean_reward:  -25.220154391138117 , time_score:  500 , memory:  694268\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward -55.24334389380395 , mean_reward:  -24.31513285011972 , time_score:  500 , memory:  696768\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward -18.18983292996667 , mean_reward:  -25.012987196684644 , time_score:  500 , memory:  699268\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward -43.87394435145765 , mean_reward:  -24.85679599876529 , time_score:  500 , memory:  701768\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward -27.90026730436645 , mean_reward:  -24.080214970321048 , time_score:  500 , memory:  704268\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward 10.647966831930134 , mean_reward:  -24.66410668355688 , time_score:  500 , memory:  706768\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward -56.50586633950185 , mean_reward:  -24.597456513135185 , time_score:  500 , memory:  709268\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward 13.716781269246273 , mean_reward:  -24.579706157253057 , time_score:  500 , memory:  711768\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward -47.32721697006817 , mean_reward:  -24.93334484818487 , time_score:  500 , memory:  714268\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward 10.113731007973623 , mean_reward:  -24.287101859884228 , time_score:  500 , memory:  716768\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward -6.889575905526467 , mean_reward:  -24.601254262315166 , time_score:  500 , memory:  719268\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward -17.078531393524656 , mean_reward:  -23.901924226394776 , time_score:  500 , memory:  721768\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward -3.3825853785099804 , mean_reward:  -23.506117338562635 , time_score:  500 , memory:  724268\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward -2.5154016662952037 , mean_reward:  -23.559604495283274 , time_score:  500 , memory:  726768\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward -58.88847033330805 , mean_reward:  -24.220678075907244 , time_score:  500 , memory:  729268\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward 18.849896541478508 , mean_reward:  -24.78915204117753 , time_score:  500 , memory:  731768\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward -1.9292192615292731 , mean_reward:  -23.865609921741495 , time_score:  500 , memory:  734268\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward 17.012440911143916 , mean_reward:  -22.844654384035284 , time_score:  500 , memory:  736768\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 12.70012547301657 , mean_reward:  -22.36454009050523 , time_score:  500 , memory:  739268\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward -18.254379302135145 , mean_reward:  -21.931452082945217 , time_score:  500 , memory:  741768\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward -2.941638563367853 , mean_reward:  -21.441306729110114 , time_score:  500 , memory:  744268\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward -4.741448865244592 , mean_reward:  -22.164982614835093 , time_score:  500 , memory:  746768\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward -50.10781131791608 , mean_reward:  -23.061500896394072 , time_score:  500 , memory:  749268\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward -36.6205043764816 , mean_reward:  -23.802627916834258 , time_score:  500 , memory:  751768\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward -34.283598617925804 , mean_reward:  -24.087287105922027 , time_score:  500 , memory:  754268\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward 3.3412504848021047 , mean_reward:  -23.87672911108569 , time_score:  500 , memory:  756768\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward -35.67660318530797 , mean_reward:  -24.130449975902376 , time_score:  500 , memory:  759268\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward -45.603776299202494 , mean_reward:  -24.341135340785026 , time_score:  500 , memory:  761768\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward -25.193330888735424 , mean_reward:  -25.191632105105917 , time_score:  500 , memory:  764268\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward -28.90368259790843 , mean_reward:  -26.420288015500333 , time_score:  500 , memory:  766768\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward -4.57218155428585 , mean_reward:  -25.913530768796885 , time_score:  500 , memory:  769268\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward 1.1210321335978342 , mean_reward:  -24.987216756372863 , time_score:  500 , memory:  771768\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward 0.8198852771203 , mean_reward:  -24.453027387573876 , time_score:  500 , memory:  774268\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward -39.650286468194196 , mean_reward:  -24.45913021843451 , time_score:  500 , memory:  776768\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward -10.218981899498695 , mean_reward:  -23.256197570458163 , time_score:  500 , memory:  779268\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward -29.146530537895444 , mean_reward:  -22.378934752944566 , time_score:  500 , memory:  781768\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward -4.4800601780208344 , mean_reward:  -23.144149284444953 , time_score:  500 , memory:  784268\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward -16.542996446363155 , mean_reward:  -23.74109391140864 , time_score:  500 , memory:  786768\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward -12.941815262851666 , mean_reward:  -23.927187838057833 , time_score:  500 , memory:  789268\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward -52.433470984273114 , mean_reward:  -24.149908999534887 , time_score:  500 , memory:  791768\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward -18.567693491167486 , mean_reward:  -25.020233906289 , time_score:  500 , memory:  794268\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward -14.474545677011376 , mean_reward:  -24.004380767870288 , time_score:  500 , memory:  796768\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward -17.233632885617048 , mean_reward:  -22.237789580217427 , time_score:  500 , memory:  799268\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward 20.422375782744034 , mean_reward:  -20.203309336630273 , time_score:  500 , memory:  801768\n",
      "Episode:  1780  , Epsilon:  0.01 , Reward -8.304714162698058 , mean_reward:  -20.30580565929891 , time_score:  500 , memory:  804268\n",
      "Episode:  1785  , Epsilon:  0.01 , Reward -29.946967257402836 , mean_reward:  -21.188934439059857 , time_score:  500 , memory:  806768\n",
      "Episode:  1790  , Epsilon:  0.01 , Reward -16.614447801859942 , mean_reward:  -19.984422451150728 , time_score:  500 , memory:  809268\n",
      "Episode:  1795  , Epsilon:  0.01 , Reward 8.44223201881936 , mean_reward:  -19.863297115104775 , time_score:  500 , memory:  811768\n",
      "Episode:  1800  , Epsilon:  0.01 , Reward -5.272764022259861 , mean_reward:  -18.903478632956592 , time_score:  500 , memory:  814268\n",
      "Episode:  1805  , Epsilon:  0.01 , Reward -47.77809489331996 , mean_reward:  -18.63573392218093 , time_score:  500 , memory:  816768\n",
      "Episode:  1810  , Epsilon:  0.01 , Reward 7.583621634825166 , mean_reward:  -17.52196517319784 , time_score:  500 , memory:  819268\n",
      "Episode:  1815  , Epsilon:  0.01 , Reward -31.029292954845758 , mean_reward:  -18.004684944788277 , time_score:  500 , memory:  821768\n",
      "Episode:  1820  , Epsilon:  0.01 , Reward -33.81493895595295 , mean_reward:  -18.467527415538402 , time_score:  500 , memory:  824268\n",
      "Episode:  1825  , Epsilon:  0.01 , Reward -11.865990338939639 , mean_reward:  -17.172944097851275 , time_score:  500 , memory:  826768\n",
      "Episode:  1830  , Epsilon:  0.01 , Reward -4.630214759096535 , mean_reward:  -18.065723867464044 , time_score:  500 , memory:  829268\n",
      "Episode:  1835  , Epsilon:  0.01 , Reward -10.518192870426375 , mean_reward:  -17.88664200292314 , time_score:  500 , memory:  831768\n",
      "Episode:  1840  , Epsilon:  0.01 , Reward -74.10856007338619 , mean_reward:  -18.676668725248007 , time_score:  500 , memory:  834268\n",
      "Episode:  1845  , Epsilon:  0.01 , Reward -18.959276834796082 , mean_reward:  -19.155498095683384 , time_score:  500 , memory:  836768\n",
      "Episode:  1850  , Epsilon:  0.01 , Reward 2.357750200654544 , mean_reward:  -19.02076114187571 , time_score:  500 , memory:  839268\n",
      "Episode:  1855  , Epsilon:  0.01 , Reward -71.31055545758451 , mean_reward:  -19.258697191194994 , time_score:  500 , memory:  841768\n",
      "Episode:  1860  , Epsilon:  0.01 , Reward 4.8538505934594705 , mean_reward:  -18.986275480327308 , time_score:  500 , memory:  844268\n",
      "Episode:  1865  , Epsilon:  0.01 , Reward -0.20185866408775888 , mean_reward:  -17.46127447104463 , time_score:  500 , memory:  846768\n",
      "Episode:  1870  , Epsilon:  0.01 , Reward 9.406591671475104 , mean_reward:  -18.595951699371966 , time_score:  500 , memory:  849268\n",
      "Episode:  1875  , Epsilon:  0.01 , Reward -56.42550908290613 , mean_reward:  -20.870758604729645 , time_score:  500 , memory:  851768\n",
      "Episode:  1880  , Epsilon:  0.01 , Reward -42.29876115207975 , mean_reward:  -21.033519811475262 , time_score:  500 , memory:  854268\n",
      "Episode:  1885  , Epsilon:  0.01 , Reward -44.5407147008694 , mean_reward:  -20.814783448936407 , time_score:  500 , memory:  856768\n",
      "Episode:  1890  , Epsilon:  0.01 , Reward 24.60479791550859 , mean_reward:  -19.382521380938375 , time_score:  500 , memory:  859268\n",
      "Episode:  1895  , Epsilon:  0.01 , Reward 35.685993545877125 , mean_reward:  -19.371956568799327 , time_score:  500 , memory:  861768\n",
      "Episode:  1900  , Epsilon:  0.01 , Reward -27.598937319406826 , mean_reward:  -19.9639599682014 , time_score:  500 , memory:  864268\n",
      "Episode:  1905  , Epsilon:  0.01 , Reward -21.243941424412466 , mean_reward:  -20.030959755269247 , time_score:  500 , memory:  866768\n",
      "Episode:  1910  , Epsilon:  0.01 , Reward -37.317559793907606 , mean_reward:  -21.09732934757029 , time_score:  500 , memory:  869268\n",
      "Episode:  1915  , Epsilon:  0.01 , Reward -46.068876968818586 , mean_reward:  -21.592805333876903 , time_score:  500 , memory:  871768\n",
      "Episode:  1920  , Epsilon:  0.01 , Reward -43.14971376141966 , mean_reward:  -22.17873194737552 , time_score:  500 , memory:  874268\n",
      "Episode:  1925  , Epsilon:  0.01 , Reward -19.906753211125288 , mean_reward:  -22.498474954719903 , time_score:  500 , memory:  876768\n",
      "Episode:  1930  , Epsilon:  0.01 , Reward -12.808303847332635 , mean_reward:  -21.042453218854725 , time_score:  500 , memory:  879268\n",
      "Episode:  1935  , Epsilon:  0.01 , Reward -62.41195816524265 , mean_reward:  -21.676096018152197 , time_score:  500 , memory:  881768\n",
      "Episode:  1940  , Epsilon:  0.01 , Reward -30.48026638804359 , mean_reward:  -20.55219415523834 , time_score:  500 , memory:  884268\n",
      "Episode:  1945  , Epsilon:  0.01 , Reward -67.41287311520287 , mean_reward:  -19.619813019055055 , time_score:  500 , memory:  886768\n",
      "Episode:  1950  , Epsilon:  0.01 , Reward -18.49988327084323 , mean_reward:  -19.926619458323657 , time_score:  500 , memory:  889268\n",
      "Episode:  1955  , Epsilon:  0.01 , Reward -27.05985658418506 , mean_reward:  -20.069729389166593 , time_score:  500 , memory:  891768\n",
      "Episode:  1960  , Epsilon:  0.01 , Reward -40.76243242949548 , mean_reward:  -19.908775760087146 , time_score:  500 , memory:  894268\n",
      "Episode:  1965  , Epsilon:  0.01 , Reward -21.990273727034356 , mean_reward:  -21.299962012164052 , time_score:  500 , memory:  896768\n",
      "Episode:  1970  , Epsilon:  0.01 , Reward -29.034036455072588 , mean_reward:  -20.59844755613712 , time_score:  500 , memory:  899268\n",
      "Episode:  1975  , Epsilon:  0.01 , Reward 2.3685693158638195 , mean_reward:  -19.951779277091447 , time_score:  500 , memory:  901768\n",
      "Episode:  1980  , Epsilon:  0.01 , Reward -3.522109618176481 , mean_reward:  -20.225142138738985 , time_score:  500 , memory:  904268\n",
      "Episode:  1985  , Epsilon:  0.01 , Reward -2.335599543147993 , mean_reward:  -20.069649938078296 , time_score:  500 , memory:  906768\n",
      "Episode:  1990  , Epsilon:  0.01 , Reward 1.3966130415894311 , mean_reward:  -20.39996886452507 , time_score:  500 , memory:  909268\n",
      "Episode:  1995  , Epsilon:  0.01 , Reward 7.851934518339617 , mean_reward:  -21.317939712387915 , time_score:  500 , memory:  911768\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.95, episodes=2000, alpha = 0.001, lr=0.0001)\n",
    "df = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zEK6_8NkZvY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzXeEPyZkZx5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
