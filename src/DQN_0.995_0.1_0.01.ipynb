{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_0.995_0.1_0.01.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"mount_file_id":"1Nxgy522FyNcaA-VjQQYTGyum_2jCgho7","authorship_tag":"ABX9TyM+WxsTlFFZicoHQA3LU++e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624385880165,"user_tz":360,"elapsed":34110,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4903c42e-8fec-433d-bb83-13eeba9fcc0a"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624725474203,"user_tz":360,"elapsed":11549,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"806e6300-9844-4711-9f3f-ee1d7683ef40"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting box2d-py\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 2.9MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d6zV5lkHwD8W","executionInfo":{"status":"ok","timestamp":1624725474206,"user_tz":360,"elapsed":5,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8","executionInfo":{"status":"ok","timestamp":1624725530625,"user_tz":360,"elapsed":544,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.1_0.01/0p1_0p01.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.1_0.01/0p1_0p01.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","   "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624743306406,"user_tz":360,"elapsed":17775642,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"064e2eea-13da-486a-f617-c238abd7e4a5"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1, epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.1, lr=0.01)\n","df = dqn.play()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -167.80851898607034 , mean_reward:  -167.80851898607034 , time_score:  78 , memory:  78\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -225.95491637277183 , mean_reward:  -146.6268890235889 , time_score:  101 , memory:  565\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -100.360395748139 , mean_reward:  -183.58643172333666 , time_score:  86 , memory:  1007\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -396.1980300701737 , mean_reward:  -205.97669059578567 , time_score:  97 , memory:  1466\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward 21.821031471666913 , mean_reward:  -194.33078689455948 , time_score:  93 , memory:  1966\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -138.557408702635 , mean_reward:  -187.21623080967464 , time_score:  77 , memory:  2457\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -371.61364795717236 , mean_reward:  -182.4125422972814 , time_score:  80 , memory:  2830\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -246.83865454765322 , mean_reward:  -178.60264628198289 , time_score:  70 , memory:  3222\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -567.7245166467965 , mean_reward:  -189.13456898166154 , time_score:  107 , memory:  3767\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -115.5987190192882 , mean_reward:  -191.52093688875658 , time_score:  93 , memory:  4209\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -218.3941188995625 , mean_reward:  -196.19057446329174 , time_score:  120 , memory:  4617\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -273.11664137081084 , mean_reward:  -200.56995540540038 , time_score:  73 , memory:  5023\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -147.8087514042079 , mean_reward:  -202.84500278790642 , time_score:  67 , memory:  5399\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -155.15745203195706 , mean_reward:  -204.37321252771744 , time_score:  75 , memory:  5818\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -359.9071496078416 , mean_reward:  -211.387876714723 , time_score:  100 , memory:  6293\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward -169.70521655867174 , mean_reward:  -216.41352922814573 , time_score:  93 , memory:  6702\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -76.77437832453633 , mean_reward:  -223.7820069008502 , time_score:  58 , memory:  7115\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -188.7779528944759 , mean_reward:  -223.29583235602158 , time_score:  92 , memory:  7481\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -270.8912990184492 , mean_reward:  -231.4460424831204 , time_score:  90 , memory:  7963\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -383.5948801759481 , mean_reward:  -234.34987577553875 , time_score:  77 , memory:  8364\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward -601.115168364149 , mean_reward:  -248.75020972142102 , time_score:  117 , memory:  8985\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -112.82272397194318 , mean_reward:  -252.66032264060632 , time_score:  146 , memory:  9663\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward -565.1394068307247 , mean_reward:  -257.45066842976183 , time_score:  130 , memory:  10329\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward -196.98621359255344 , mean_reward:  -260.36569094475885 , time_score:  102 , memory:  10996\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -761.3107003683041 , mean_reward:  -267.3190573057286 , time_score:  130 , memory:  11701\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward -110.81500927687532 , mean_reward:  -274.7216853472175 , time_score:  119 , memory:  12390\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -90.10359801239979 , mean_reward:  -277.78544878943126 , time_score:  64 , memory:  12878\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -211.34719607662763 , mean_reward:  -281.3024098427541 , time_score:  111 , memory:  13545\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -552.1354988875358 , mean_reward:  -290.4924867525704 , time_score:  119 , memory:  14229\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -524.4685840247693 , mean_reward:  -302.64063806436434 , time_score:  252 , memory:  15016\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward -450.3660836885496 , mean_reward:  -313.22066184362376 , time_score:  105 , memory:  15617\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward -353.12455689646663 , mean_reward:  -313.58687758326096 , time_score:  162 , memory:  16238\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward -389.5642178280325 , mean_reward:  -315.74926070729674 , time_score:  142 , memory:  16989\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward -323.6720878479719 , mean_reward:  -321.4806174766372 , time_score:  115 , memory:  17690\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward -303.8729172780022 , mean_reward:  -317.0320508245755 , time_score:  188 , memory:  18432\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward -145.22451473182954 , mean_reward:  -314.6029230843954 , time_score:  112 , memory:  19151\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward -308.27473463675915 , mean_reward:  -310.4414758004514 , time_score:  311 , memory:  20128\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward -403.2572008335266 , mean_reward:  -314.66205756389513 , time_score:  283 , memory:  21497\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward -401.49359354802993 , mean_reward:  -314.4835835884682 , time_score:  188 , memory:  23163\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward -391.9190481470099 , mean_reward:  -318.3738826195861 , time_score:  500 , memory:  25031\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward -649.2422498269195 , mean_reward:  -311.589721761583 , time_score:  216 , memory:  26424\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward -311.1844971252078 , mean_reward:  -318.69739376698124 , time_score:  191 , memory:  27241\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward -666.0156720792693 , mean_reward:  -322.2763931876314 , time_score:  229 , memory:  28031\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward -637.4929584983632 , mean_reward:  -327.9004769136149 , time_score:  235 , memory:  29063\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward -257.81116286569943 , mean_reward:  -324.39954896881505 , time_score:  134 , memory:  29929\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward -457.4077561710934 , mean_reward:  -325.08769552804273 , time_score:  177 , memory:  30609\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward -356.3481046566637 , mean_reward:  -328.0415790170677 , time_score:  147 , memory:  31648\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward -730.184220463338 , mean_reward:  -339.9826104182407 , time_score:  417 , memory:  33073\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward -233.83971536110218 , mean_reward:  -344.5007770146549 , time_score:  500 , memory:  35468\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward -264.79304093215876 , mean_reward:  -346.53473123063907 , time_score:  118 , memory:  37049\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward -156.01714741573608 , mean_reward:  -342.64750203732166 , time_score:  500 , memory:  38765\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward -309.7353524800857 , mean_reward:  -349.9642852038796 , time_score:  207 , memory:  39891\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward -183.93607779650523 , mean_reward:  -348.9814765359459 , time_score:  122 , memory:  41312\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward -455.8817139014248 , mean_reward:  -348.9522887470863 , time_score:  127 , memory:  43170\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward -282.04125423236854 , mean_reward:  -351.7200558749122 , time_score:  500 , memory:  44917\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward -665.062523927849 , mean_reward:  -355.85800360432347 , time_score:  327 , memory:  46903\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward -181.41016341324888 , mean_reward:  -357.13371673699794 , time_score:  101 , memory:  48840\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward -302.1585329554389 , mean_reward:  -355.9416591643804 , time_score:  331 , memory:  50554\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward -1.4736720290686378 , mean_reward:  -349.2564769931791 , time_score:  413 , memory:  52565\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward -380.2516427471327 , mean_reward:  -348.349416588511 , time_score:  205 , memory:  54245\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward -292.9319627653763 , mean_reward:  -342.98398984080137 , time_score:  219 , memory:  55873\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward -309.1993615555128 , mean_reward:  -345.0813428076653 , time_score:  297 , memory:  57594\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward -217.476872408688 , mean_reward:  -336.7153601064448 , time_score:  111 , memory:  58776\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward -109.84084693413381 , mean_reward:  -330.6984079908041 , time_score:  197 , memory:  59947\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward -422.8586864433968 , mean_reward:  -334.8724506015777 , time_score:  141 , memory:  61132\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward -342.3568347663951 , mean_reward:  -336.7068513336202 , time_score:  335 , memory:  62360\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward -137.3505801274979 , mean_reward:  -333.36571070260356 , time_score:  257 , memory:  63533\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward -122.23430493666478 , mean_reward:  -319.734146676514 , time_score:  128 , memory:  64456\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward -285.03293394411844 , mean_reward:  -305.7128518929493 , time_score:  139 , memory:  65408\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward -78.01889945505292 , mean_reward:  -287.6595665528399 , time_score:  204 , memory:  66349\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward -197.49087730388945 , mean_reward:  -277.20974631439844 , time_score:  105 , memory:  67270\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward -46.43836579244136 , mean_reward:  -264.7589988126084 , time_score:  146 , memory:  67980\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward -93.20821893036435 , mean_reward:  -257.2105581673974 , time_score:  221 , memory:  68658\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward -522.257332201477 , mean_reward:  -255.39537371757058 , time_score:  113 , memory:  69350\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward -57.18916605220964 , mean_reward:  -244.0486577289518 , time_score:  177 , memory:  70494\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward -204.80969006933668 , mean_reward:  -235.71536331488866 , time_score:  269 , memory:  71961\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward -36.308179654872596 , mean_reward:  -230.91126900005239 , time_score:  500 , memory:  73711\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward -253.21922802464897 , mean_reward:  -223.55433167610815 , time_score:  373 , memory:  75616\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward -103.54785933798323 , mean_reward:  -222.21840508755793 , time_score:  500 , memory:  77363\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward -108.68938559161141 , mean_reward:  -213.19171335845428 , time_score:  265 , memory:  78918\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward -108.4819366175653 , mean_reward:  -202.11282207812695 , time_score:  390 , memory:  81217\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward -37.25771157843728 , mean_reward:  -187.0737321867086 , time_score:  88 , memory:  81960\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward -37.18772323060519 , mean_reward:  -182.06791305854622 , time_score:  258 , memory:  83557\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward -137.79943897671197 , mean_reward:  -173.62722392996955 , time_score:  104 , memory:  84506\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward -16.837382376615267 , mean_reward:  -160.55636815861263 , time_score:  92 , memory:  85013\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward -95.15892496182512 , mean_reward:  -147.87629107655528 , time_score:  251 , memory:  86096\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward 13.694530831045846 , mean_reward:  -147.3568967743384 , time_score:  500 , memory:  87993\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward -88.41329303117246 , mean_reward:  -139.78727731155064 , time_score:  500 , memory:  89765\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward -225.5182798972741 , mean_reward:  -134.08336876232352 , time_score:  500 , memory:  91890\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward -334.7189499769998 , mean_reward:  -137.29617431912277 , time_score:  209 , memory:  93711\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward -192.05036048977843 , mean_reward:  -137.66126902332869 , time_score:  185 , memory:  94864\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward -320.2026088423541 , mean_reward:  -140.45069562005622 , time_score:  49 , memory:  96358\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward -169.5221288704126 , mean_reward:  -142.46072249776708 , time_score:  373 , memory:  97715\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward -192.80851928718945 , mean_reward:  -133.48275584695966 , time_score:  317 , memory:  99854\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward -0.5280960167150255 , mean_reward:  -133.00220519591878 , time_score:  357 , memory:  102211\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward -94.55265964813788 , mean_reward:  -129.18166968286195 , time_score:  500 , memory:  104512\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward -415.3481502236186 , mean_reward:  -132.59629711788403 , time_score:  116 , memory:  106378\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward -253.42941783930846 , mean_reward:  -129.4419032524069 , time_score:  238 , memory:  108217\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward -238.98103060035604 , mean_reward:  -137.37319071509356 , time_score:  155 , memory:  109782\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward -232.0633721396045 , mean_reward:  -145.43365876790534 , time_score:  105 , memory:  110961\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward -530.648421399603 , mean_reward:  -152.03528965143963 , time_score:  274 , memory:  112588\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward -366.9794218204486 , mean_reward:  -162.69471781330162 , time_score:  491 , memory:  114577\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward -124.85139906549084 , mean_reward:  -168.68028546933797 , time_score:  120 , memory:  115911\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward -697.4904954843186 , mean_reward:  -174.00841997994678 , time_score:  181 , memory:  117084\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward -202.4649955559753 , mean_reward:  -178.68560388803004 , time_score:  348 , memory:  118637\n","Episode:  525  , Epsilon:  0.07196434741762824 , Reward -110.5485064178991 , mean_reward:  -181.78982760872898 , time_score:  367 , memory:  120503\n","Episode:  530  , Epsilon:  0.07018314008827135 , Reward -66.41205989868217 , mean_reward:  -180.58364028542167 , time_score:  500 , memory:  122560\n","Episode:  535  , Epsilon:  0.06844601986126451 , Reward 21.347711114742754 , mean_reward:  -181.9340177371836 , time_score:  500 , memory:  124172\n","Episode:  540  , Epsilon:  0.0667518955258533 , Reward -37.78718906972929 , mean_reward:  -177.3595337397157 , time_score:  198 , memory:  126021\n","Episode:  545  , Epsilon:  0.06509970288011008 , Reward -45.52675437495475 , mean_reward:  -174.3913849927425 , time_score:  500 , memory:  127772\n","Episode:  550  , Epsilon:  0.06348840406243188 , Reward -234.9559763683744 , mean_reward:  -177.71854508378155 , time_score:  500 , memory:  129450\n","Episode:  555  , Epsilon:  0.06191698689958447 , Reward -488.53100601305647 , mean_reward:  -181.32125196824887 , time_score:  151 , memory:  130203\n","Episode:  560  , Epsilon:  0.06038446427088321 , Reward -352.1193156588349 , mean_reward:  -181.0059790911031 , time_score:  415 , memory:  132056\n","Episode:  565  , Epsilon:  0.058889873488111255 , Reward -69.35277282291074 , mean_reward:  -180.7573866378445 , time_score:  500 , memory:  133914\n","Episode:  570  , Epsilon:  0.05743227569078546 , Reward -373.9444099697163 , mean_reward:  -196.74839341849327 , time_score:  64 , memory:  135330\n","Episode:  575  , Epsilon:  0.05601075525639029 , Reward -412.36317650159293 , mean_reward:  -208.11356088962282 , time_score:  139 , memory:  136016\n","Episode:  580  , Epsilon:  0.05462441922520914 , Reward -132.59799672620323 , mean_reward:  -199.03730945660038 , time_score:  239 , memory:  138014\n","Episode:  585  , Epsilon:  0.05327239673939179 , Reward -94.09617310247162 , mean_reward:  -198.8421693068819 , time_score:  500 , memory:  140119\n","Episode:  590  , Epsilon:  0.05195383849590569 , Reward -380.77976906283004 , mean_reward:  -188.72604575151325 , time_score:  115 , memory:  141218\n","Episode:  595  , Epsilon:  0.05066791621302729 , Reward -8.716207013432149 , mean_reward:  -173.30476067343884 , time_score:  500 , memory:  143121\n","Episode:  600  , Epsilon:  0.0494138221100385 , Reward -532.6768975311385 , mean_reward:  -172.65832619602696 , time_score:  63 , memory:  144848\n","Episode:  605  , Epsilon:  0.048190768399801194 , Reward -0.5932808272627215 , mean_reward:  -157.81671324130383 , time_score:  500 , memory:  147248\n","Episode:  610  , Epsilon:  0.046997986793891174 , Reward -232.34229518027402 , mean_reward:  -153.47650740252544 , time_score:  500 , memory:  149366\n","Episode:  615  , Epsilon:  0.04583472801998072 , Reward -42.634674718323026 , mean_reward:  -145.09036577514135 , time_score:  456 , memory:  151165\n","Episode:  620  , Epsilon:  0.04470026135116646 , Reward 31.356081446732876 , mean_reward:  -139.3338602851264 , time_score:  500 , memory:  153390\n","Episode:  625  , Epsilon:  0.04359387414694703 , Reward 74.3209621107538 , mean_reward:  -132.25560128466014 , time_score:  500 , memory:  155544\n","Episode:  630  , Epsilon:  0.04251487140556204 , Reward 12.403034897683979 , mean_reward:  -122.50851438901026 , time_score:  500 , memory:  158044\n","Episode:  635  , Epsilon:  0.04146257532741124 , Reward -242.44297893734992 , mean_reward:  -122.12652006494723 , time_score:  500 , memory:  160519\n","Episode:  640  , Epsilon:  0.04043632488927963 , Reward 27.448382258619638 , mean_reward:  -123.84770568568139 , time_score:  500 , memory:  162971\n","Episode:  645  , Epsilon:  0.039435475429100995 , Reward -35.09490633929813 , mean_reward:  -116.93037224765257 , time_score:  500 , memory:  165240\n","Episode:  650  , Epsilon:  0.03845939824099909 , Reward 20.182184153002837 , mean_reward:  -110.83128672396589 , time_score:  500 , memory:  167239\n","Episode:  655  , Epsilon:  0.03750748018035199 , Reward -22.21302311600376 , mean_reward:  -102.53699731902815 , time_score:  500 , memory:  169739\n","Episode:  660  , Epsilon:  0.03657912327863173 , Reward 54.43402983046606 , mean_reward:  -96.44178525561782 , time_score:  500 , memory:  172239\n","Episode:  665  , Epsilon:  0.035673744367776934 , Reward 83.09191211196566 , mean_reward:  -90.31729253331196 , time_score:  500 , memory:  174549\n","Episode:  670  , Epsilon:  0.03479077471386296 , Reward -247.9397213598027 , mean_reward:  -75.12326521704746 , time_score:  491 , memory:  177040\n","Episode:  675  , Epsilon:  0.03392965965983891 , Reward 31.373981079026837 , mean_reward:  -59.336339716562655 , time_score:  500 , memory:  179540\n","Episode:  680  , Epsilon:  0.03308985827710748 , Reward -59.96356633831206 , mean_reward:  -58.833629691392325 , time_score:  227 , memory:  181681\n","Episode:  685  , Epsilon:  0.03227084302572862 , Reward -63.68960011187171 , mean_reward:  -55.36434224080135 , time_score:  500 , memory:  184181\n","Episode:  690  , Epsilon:  0.03147209942303359 , Reward -609.4247260159339 , mean_reward:  -55.99419160101093 , time_score:  118 , memory:  185942\n","Episode:  695  , Epsilon:  0.030693125720441184 , Reward 41.22484528503212 , mean_reward:  -64.54144512477909 , time_score:  500 , memory:  187244\n","Episode:  700  , Epsilon:  0.029933432588273214 , Reward -197.74962452591134 , mean_reward:  -57.88891182734855 , time_score:  474 , memory:  189718\n","Episode:  705  , Epsilon:  0.029192542808371146 , Reward -191.89275992982903 , mean_reward:  -58.52887169694808 , time_score:  429 , memory:  192147\n","Episode:  710  , Epsilon:  0.028469990974320916 , Reward 11.933193960336176 , mean_reward:  -49.73789707179731 , time_score:  500 , memory:  194647\n","Episode:  715  , Epsilon:  0.027765323199097504 , Reward -118.72188903795904 , mean_reward:  -52.064313260463614 , time_score:  151 , memory:  196226\n","Episode:  720  , Epsilon:  0.02707809682994571 , Reward 20.914280019162383 , mean_reward:  -49.31772460873457 , time_score:  500 , memory:  198726\n","Episode:  725  , Epsilon:  0.026407880170317945 , Reward -51.97854644616683 , mean_reward:  -47.143120746397244 , time_score:  222 , memory:  200948\n","Episode:  730  , Epsilon:  0.025754252208694463 , Reward 16.17944939023704 , mean_reward:  -57.59917858494318 , time_score:  500 , memory:  202587\n","Episode:  735  , Epsilon:  0.025116802354115567 , Reward -67.68701031157397 , mean_reward:  -56.28447028139106 , time_score:  500 , memory:  204700\n","Episode:  740  , Epsilon:  0.02449513017825978 , Reward -301.28895093679876 , mean_reward:  -60.834259509291805 , time_score:  234 , memory:  206480\n","Episode:  745  , Epsilon:  0.023888845163905856 , Reward -2.3772621068404476 , mean_reward:  -66.72664749086377 , time_score:  55 , memory:  207498\n","Episode:  750  , Epsilon:  0.023297566459620722 , Reward -7.417715228836729 , mean_reward:  -75.7058651695136 , time_score:  500 , memory:  208787\n","Episode:  755  , Epsilon:  0.022720922640519125 , Reward -296.7570553570737 , mean_reward:  -82.62535846551862 , time_score:  154 , memory:  210242\n","Episode:  760  , Epsilon:  0.022158551474944856 , Reward -239.26255621878477 , mean_reward:  -93.92547443055504 , time_score:  414 , memory:  211991\n","Episode:  765  , Epsilon:  0.021610099696926857 , Reward -146.8480897443714 , mean_reward:  -118.32119878025463 , time_score:  204 , memory:  213704\n","Episode:  770  , Epsilon:  0.021075222784267326 , Reward -2240.154582194233 , mean_reward:  -170.95918895177337 , time_score:  320 , memory:  214837\n","Episode:  775  , Epsilon:  0.020553584742122436 , Reward -2334.0484519973247 , mean_reward:  -211.89382640507978 , time_score:  318 , memory:  216918\n","Episode:  780  , Epsilon:  0.020044857891939702 , Reward -1366.3614329819352 , mean_reward:  -238.898445427352 , time_score:  392 , memory:  218687\n","Episode:  785  , Epsilon:  0.01954872266561937 , Reward -895.086042358161 , mean_reward:  -281.2588334198665 , time_score:  169 , memory:  220059\n","Episode:  790  , Epsilon:  0.019064867404770626 , Reward -487.93585152675865 , mean_reward:  -305.42813699214804 , time_score:  286 , memory:  221834\n","Episode:  795  , Epsilon:  0.018592988164936427 , Reward -1360.6618051844862 , mean_reward:  -399.8441780413325 , time_score:  291 , memory:  223180\n","Episode:  800  , Epsilon:  0.018132788524664028 , Reward -1064.001326578331 , mean_reward:  -438.14176622332775 , time_score:  179 , memory:  223922\n","Episode:  805  , Epsilon:  0.017683979399301233 , Reward -1205.9064192400267 , mean_reward:  -486.56239539141336 , time_score:  236 , memory:  225120\n","Episode:  810  , Epsilon:  0.01724627885940145 , Reward -866.3629291876994 , mean_reward:  -544.8905170146016 , time_score:  241 , memory:  226401\n","Episode:  815  , Epsilon:  0.01681941195362342 , Reward -2521.8307885821273 , mean_reward:  -616.3526658113974 , time_score:  479 , memory:  228016\n","Episode:  820  , Epsilon:  0.0164031105360144 , Reward -621.6719195801069 , mean_reward:  -691.6707332361523 , time_score:  474 , memory:  229844\n","Episode:  825  , Epsilon:  0.015997113097568336 , Reward -1882.3838699907799 , mean_reward:  -786.9468426770742 , time_score:  451 , memory:  231669\n","Episode:  830  , Epsilon:  0.015601164601953134 , Reward -3307.0440143740993 , mean_reward:  -877.9938036503912 , time_score:  496 , memory:  233700\n","Episode:  835  , Epsilon:  0.015215016325303928 , Reward -1152.1770748068905 , mean_reward:  -907.2174272209829 , time_score:  396 , memory:  234957\n","Episode:  840  , Epsilon:  0.014838425699981627 , Reward -1263.1835271387147 , mean_reward:  -957.0035573863074 , time_score:  319 , memory:  236375\n","Episode:  845  , Epsilon:  0.014471156162198668 , Reward -1208.385467588473 , mean_reward:  -1001.3922142117672 , time_score:  338 , memory:  237818\n","Episode:  850  , Epsilon:  0.014112977003416188 , Reward -1703.2155225566582 , mean_reward:  -1068.8242833025483 , time_score:  335 , memory:  239173\n","Episode:  855  , Epsilon:  0.013763663225419333 , Reward -107.41278592238132 , mean_reward:  -1139.056540035369 , time_score:  208 , memory:  240846\n","Episode:  860  , Epsilon:  0.013422995398979608 , Reward -14.622050895253878 , mean_reward:  -1128.608879357141 , time_score:  73 , memory:  241545\n","Episode:  865  , Epsilon:  0.013090759526015528 , Reward -509.3820408321012 , mean_reward:  -1111.409190507706 , time_score:  202 , memory:  242191\n","Episode:  870  , Epsilon:  0.012766746905164949 , Reward -79.0311378397297 , mean_reward:  -1058.3983241092055 , time_score:  287 , memory:  242853\n","Episode:  875  , Epsilon:  0.012450754000684672 , Reward 4.491886122401567 , mean_reward:  -1018.78630257935 , time_score:  108 , memory:  243466\n","Episode:  880  , Epsilon:  0.012142582314594924 , Reward -12.492993859514101 , mean_reward:  -990.2522570150998 , time_score:  135 , memory:  244097\n","Episode:  885  , Epsilon:  0.01184203826198843 , Reward -25.76379866786732 , mean_reward:  -949.3658440133609 , time_score:  122 , memory:  244826\n","Episode:  890  , Epsilon:  0.01154893304942575 , Reward -30.781120430506775 , mean_reward:  -919.392359063771 , time_score:  116 , memory:  245353\n","Episode:  895  , Epsilon:  0.011263082556340478 , Reward -141.19459784967927 , mean_reward:  -822.9840711910042 , time_score:  85 , memory:  245861\n","Episode:  900  , Epsilon:  0.01098430721937979 , Reward -23.51206472225148 , mean_reward:  -785.597796880056 , time_score:  138 , memory:  246567\n","Episode:  905  , Epsilon:  0.01071243191960775 , Reward -145.61991347204858 , mean_reward:  -740.0845874786354 , time_score:  71 , memory:  247072\n","Episode:  910  , Epsilon:  0.010447285872500434 , Reward -242.2745105319517 , mean_reward:  -688.8333683791323 , time_score:  65 , memory:  247735\n","Episode:  915  , Epsilon:  0.010188702520663827 , Reward -109.72030105209602 , mean_reward:  -619.4063140529986 , time_score:  99 , memory:  248176\n","Episode:  920  , Epsilon:  0.01 , Reward -67.43713369347465 , mean_reward:  -553.5310182798503 , time_score:  500 , memory:  249378\n","Episode:  925  , Epsilon:  0.01 , Reward -79.25139447186467 , mean_reward:  -466.55549971790754 , time_score:  500 , memory:  250721\n","Episode:  930  , Epsilon:  0.01 , Reward -269.57862444643956 , mean_reward:  -379.1689027965124 , time_score:  322 , memory:  252783\n","Episode:  935  , Epsilon:  0.01 , Reward -302.0564016071869 , mean_reward:  -352.418868234768 , time_score:  135 , memory:  254918\n","Episode:  940  , Epsilon:  0.01 , Reward -65.54374989550261 , mean_reward:  -297.9320878410154 , time_score:  500 , memory:  257418\n","Episode:  945  , Epsilon:  0.01 , Reward -524.0593729068826 , mean_reward:  -256.5599242634265 , time_score:  491 , memory:  259498\n","Episode:  950  , Epsilon:  0.01 , Reward -749.4522209594029 , mean_reward:  -198.72305037195136 , time_score:  436 , memory:  261659\n","Episode:  955  , Epsilon:  0.01 , Reward -468.120119254585 , mean_reward:  -148.24629162967554 , time_score:  384 , memory:  263205\n","Episode:  960  , Epsilon:  0.01 , Reward -515.7356264876058 , mean_reward:  -198.49669584287068 , time_score:  145 , memory:  265189\n","Episode:  965  , Epsilon:  0.01 , Reward -378.60554200062694 , mean_reward:  -217.7440387536199 , time_score:  150 , memory:  266012\n","Episode:  970  , Epsilon:  0.01 , Reward -506.2568193353308 , mean_reward:  -240.05878917529674 , time_score:  190 , memory:  267043\n","Episode:  975  , Epsilon:  0.01 , Reward -520.2060027954842 , mean_reward:  -262.34577502803967 , time_score:  250 , memory:  268023\n","Episode:  980  , Epsilon:  0.01 , Reward -64.90677491406309 , mean_reward:  -276.3187088135972 , time_score:  119 , memory:  269268\n","Episode:  985  , Epsilon:  0.01 , Reward -444.5928710478762 , mean_reward:  -296.3129221771465 , time_score:  500 , memory:  271127\n","Episode:  990  , Epsilon:  0.01 , Reward -156.0401684287039 , mean_reward:  -307.6126085359651 , time_score:  86 , memory:  271894\n","Episode:  995  , Epsilon:  0.01 , Reward -311.4868334658263 , mean_reward:  -314.6224410263056 , time_score:  380 , memory:  273430\n","Episode:  1000  , Epsilon:  0.01 , Reward -274.52512498687 , mean_reward:  -325.3122194475907 , time_score:  232 , memory:  274297\n","Episode:  1005  , Epsilon:  0.01 , Reward 22.013062586021164 , mean_reward:  -333.00493881634793 , time_score:  70 , memory:  274996\n","Episode:  1010  , Epsilon:  0.01 , Reward -147.83217915701977 , mean_reward:  -334.734585102467 , time_score:  56 , memory:  275817\n","Episode:  1015  , Epsilon:  0.01 , Reward -154.83638086772288 , mean_reward:  -333.1623787967377 , time_score:  144 , memory:  276386\n","Episode:  1020  , Epsilon:  0.01 , Reward -264.2438124524982 , mean_reward:  -339.6449535610583 , time_score:  67 , memory:  276912\n","Episode:  1025  , Epsilon:  0.01 , Reward -190.19830575306761 , mean_reward:  -342.51262082829055 , time_score:  54 , memory:  277239\n","Episode:  1030  , Epsilon:  0.01 , Reward -170.69357331511276 , mean_reward:  -340.5406651215121 , time_score:  58 , memory:  277595\n","Episode:  1035  , Epsilon:  0.01 , Reward -334.222314778375 , mean_reward:  -354.412270803022 , time_score:  60 , memory:  277904\n","Episode:  1040  , Epsilon:  0.01 , Reward -230.44220751558203 , mean_reward:  -367.63731400144434 , time_score:  73 , memory:  278268\n","Episode:  1045  , Epsilon:  0.01 , Reward -293.7097082800465 , mean_reward:  -367.7140599261365 , time_score:  75 , memory:  278707\n","Episode:  1050  , Epsilon:  0.01 , Reward -170.93331880836573 , mean_reward:  -358.58594305314733 , time_score:  65 , memory:  279077\n","Episode:  1055  , Epsilon:  0.01 , Reward -608.8702724288648 , mean_reward:  -347.6063372472415 , time_score:  75 , memory:  279417\n","Episode:  1060  , Epsilon:  0.01 , Reward -57.95379269375045 , mean_reward:  -306.3440248087478 , time_score:  79 , memory:  279745\n","Episode:  1065  , Epsilon:  0.01 , Reward -467.8238625121276 , mean_reward:  -303.3305080028005 , time_score:  73 , memory:  280140\n","Episode:  1070  , Epsilon:  0.01 , Reward -100.53655839483439 , mean_reward:  -289.80598393416153 , time_score:  78 , memory:  280601\n","Episode:  1075  , Epsilon:  0.01 , Reward -426.2645402298186 , mean_reward:  -280.8485073168182 , time_score:  76 , memory:  280999\n","Episode:  1080  , Epsilon:  0.01 , Reward -633.4230638020048 , mean_reward:  -286.55414897211466 , time_score:  70 , memory:  281411\n","Episode:  1085  , Epsilon:  0.01 , Reward -399.7849942230322 , mean_reward:  -286.46896109338803 , time_score:  124 , memory:  281944\n","Episode:  1090  , Epsilon:  0.01 , Reward -166.69330482439292 , mean_reward:  -292.515157948491 , time_score:  179 , memory:  282420\n","Episode:  1095  , Epsilon:  0.01 , Reward -199.9458525919923 , mean_reward:  -288.2091308470514 , time_score:  123 , memory:  283092\n","Episode:  1100  , Epsilon:  0.01 , Reward -545.0389886069844 , mean_reward:  -294.7724415539128 , time_score:  59 , memory:  283700\n","Episode:  1105  , Epsilon:  0.01 , Reward -397.5796873913431 , mean_reward:  -296.7026150688916 , time_score:  123 , memory:  284473\n","Episode:  1110  , Epsilon:  0.01 , Reward -207.2834486473909 , mean_reward:  -302.1742182959183 , time_score:  500 , memory:  286205\n","Episode:  1115  , Epsilon:  0.01 , Reward -47.96485107984295 , mean_reward:  -308.8713582319562 , time_score:  500 , memory:  288300\n","Episode:  1120  , Epsilon:  0.01 , Reward -200.36095166173908 , mean_reward:  -305.81939582133117 , time_score:  417 , memory:  290242\n","Episode:  1125  , Epsilon:  0.01 , Reward -127.04752934101663 , mean_reward:  -301.1225457586607 , time_score:  275 , memory:  291777\n","Episode:  1130  , Epsilon:  0.01 , Reward -182.53222253587109 , mean_reward:  -296.41063061765686 , time_score:  74 , memory:  292860\n","Episode:  1135  , Epsilon:  0.01 , Reward -20.231305852378085 , mean_reward:  -288.34947681533043 , time_score:  197 , memory:  293496\n","Episode:  1140  , Epsilon:  0.01 , Reward -86.40329195147916 , mean_reward:  -279.9896838286449 , time_score:  102 , memory:  294198\n","Episode:  1145  , Epsilon:  0.01 , Reward -129.11968973483374 , mean_reward:  -275.5406818085837 , time_score:  181 , memory:  294872\n","Episode:  1150  , Epsilon:  0.01 , Reward -292.5857479325136 , mean_reward:  -274.1238558375355 , time_score:  312 , memory:  296292\n","Episode:  1155  , Epsilon:  0.01 , Reward -191.63275083352613 , mean_reward:  -263.3699537907865 , time_score:  352 , memory:  297503\n","Episode:  1160  , Epsilon:  0.01 , Reward -898.2964078047747 , mean_reward:  -273.68713694555623 , time_score:  484 , memory:  299987\n","Episode:  1165  , Epsilon:  0.01 , Reward -462.5881738666821 , mean_reward:  -296.9495754700798 , time_score:  500 , memory:  302240\n","Episode:  1170  , Epsilon:  0.01 , Reward -621.7000080991784 , mean_reward:  -310.0097382850531 , time_score:  176 , memory:  303650\n","Episode:  1175  , Epsilon:  0.01 , Reward -647.3542280100162 , mean_reward:  -339.3597298448092 , time_score:  290 , memory:  305137\n","Episode:  1180  , Epsilon:  0.01 , Reward -315.1202158983241 , mean_reward:  -339.95889149067875 , time_score:  319 , memory:  306573\n","Episode:  1185  , Epsilon:  0.01 , Reward -1750.8663596316599 , mean_reward:  -357.87307778882473 , time_score:  478 , memory:  307988\n","Episode:  1190  , Epsilon:  0.01 , Reward -573.1417936531634 , mean_reward:  -360.1933590049673 , time_score:  195 , memory:  309116\n","Episode:  1195  , Epsilon:  0.01 , Reward -366.97448677260746 , mean_reward:  -399.285312281631 , time_score:  230 , memory:  310463\n","Episode:  1200  , Epsilon:  0.01 , Reward -1170.8667470077282 , mean_reward:  -427.2750833543468 , time_score:  337 , memory:  311838\n","Episode:  1205  , Epsilon:  0.01 , Reward -322.95390002157706 , mean_reward:  -461.52351817841753 , time_score:  85 , memory:  312644\n","Episode:  1210  , Epsilon:  0.01 , Reward -787.177080634993 , mean_reward:  -470.14324700411004 , time_score:  180 , memory:  313356\n","Episode:  1215  , Epsilon:  0.01 , Reward -604.7237752332567 , mean_reward:  -482.6070449097877 , time_score:  208 , memory:  314009\n","Episode:  1220  , Epsilon:  0.01 , Reward -663.8370504526822 , mean_reward:  -505.7192825736617 , time_score:  138 , memory:  314721\n","Episode:  1225  , Epsilon:  0.01 , Reward -657.7676418837484 , mean_reward:  -534.319896122847 , time_score:  150 , memory:  315480\n","Episode:  1230  , Epsilon:  0.01 , Reward -844.1555510363559 , mean_reward:  -565.4429302443049 , time_score:  188 , memory:  316223\n","Episode:  1235  , Epsilon:  0.01 , Reward -851.0720282870948 , mean_reward:  -596.2891199769647 , time_score:  205 , memory:  316996\n","Episode:  1240  , Epsilon:  0.01 , Reward -329.0287002000488 , mean_reward:  -617.1629262584813 , time_score:  85 , memory:  317608\n","Episode:  1245  , Epsilon:  0.01 , Reward -791.1239468017528 , mean_reward:  -652.444023106271 , time_score:  147 , memory:  318362\n","Episode:  1250  , Epsilon:  0.01 , Reward -809.833063234751 , mean_reward:  -672.8058008282392 , time_score:  297 , memory:  319361\n","Episode:  1255  , Epsilon:  0.01 , Reward -287.3423505556276 , mean_reward:  -687.3885118526724 , time_score:  167 , memory:  320299\n","Episode:  1260  , Epsilon:  0.01 , Reward -579.9585532512754 , mean_reward:  -721.6766358086882 , time_score:  243 , memory:  321602\n","Episode:  1265  , Epsilon:  0.01 , Reward -431.0966810957511 , mean_reward:  -695.4863736491889 , time_score:  147 , memory:  322443\n","Episode:  1270  , Epsilon:  0.01 , Reward -1020.2905809382121 , mean_reward:  -696.3401716418758 , time_score:  349 , memory:  323507\n","Episode:  1275  , Epsilon:  0.01 , Reward -939.1736841357683 , mean_reward:  -687.5597548175132 , time_score:  338 , memory:  324774\n","Episode:  1280  , Epsilon:  0.01 , Reward -608.6950895717966 , mean_reward:  -733.7935342835434 , time_score:  296 , memory:  326468\n","Episode:  1285  , Epsilon:  0.01 , Reward -515.4267811045081 , mean_reward:  -731.6009471632829 , time_score:  287 , memory:  328151\n","Episode:  1290  , Epsilon:  0.01 , Reward -917.4285195125849 , mean_reward:  -764.5933913910175 , time_score:  484 , memory:  329954\n","Episode:  1295  , Epsilon:  0.01 , Reward -1620.5424016943393 , mean_reward:  -777.7123274361005 , time_score:  500 , memory:  332154\n","Episode:  1300  , Epsilon:  0.01 , Reward -1354.0029230081768 , mean_reward:  -763.9801495169448 , time_score:  271 , memory:  334106\n","Episode:  1305  , Epsilon:  0.01 , Reward -453.0215199751423 , mean_reward:  -742.4141901939291 , time_score:  500 , memory:  335867\n","Episode:  1310  , Epsilon:  0.01 , Reward -69.98222725809406 , mean_reward:  -752.012170625278 , time_score:  500 , memory:  337870\n","Episode:  1315  , Epsilon:  0.01 , Reward -598.6548016259937 , mean_reward:  -762.3750886061845 , time_score:  492 , memory:  339663\n","Episode:  1320  , Epsilon:  0.01 , Reward -353.7335568066775 , mean_reward:  -751.1910347097416 , time_score:  500 , memory:  342163\n","Episode:  1325  , Epsilon:  0.01 , Reward -2804.326339638529 , mean_reward:  -776.9168026262795 , time_score:  461 , memory:  344061\n","Episode:  1330  , Epsilon:  0.01 , Reward -278.3855283111147 , mean_reward:  -767.9236252869277 , time_score:  122 , memory:  345068\n","Episode:  1335  , Epsilon:  0.01 , Reward -102.62289560100724 , mean_reward:  -750.9786714799043 , time_score:  500 , memory:  346601\n","Episode:  1340  , Epsilon:  0.01 , Reward -919.4280184105388 , mean_reward:  -742.8606859055482 , time_score:  495 , memory:  347599\n","Episode:  1345  , Epsilon:  0.01 , Reward -1209.668817969947 , mean_reward:  -724.6229935342476 , time_score:  426 , memory:  349591\n","Episode:  1350  , Epsilon:  0.01 , Reward -115.28295599249999 , mean_reward:  -703.5364010383975 , time_score:  60 , memory:  351475\n","Episode:  1355  , Epsilon:  0.01 , Reward -142.10953914399866 , mean_reward:  -702.4492658153458 , time_score:  500 , memory:  353517\n","Episode:  1360  , Epsilon:  0.01 , Reward -342.3733074755934 , mean_reward:  -676.8503845392959 , time_score:  431 , memory:  355063\n","Episode:  1365  , Epsilon:  0.01 , Reward -36.76532925181297 , mean_reward:  -668.7810341634399 , time_score:  219 , memory:  356405\n","Episode:  1370  , Epsilon:  0.01 , Reward -60.618308681745575 , mean_reward:  -650.7792769064895 , time_score:  500 , memory:  357696\n","Episode:  1375  , Epsilon:  0.01 , Reward -175.10335201121995 , mean_reward:  -635.1144955962031 , time_score:  148 , memory:  358685\n","Episode:  1380  , Epsilon:  0.01 , Reward -223.63472504704566 , mean_reward:  -582.2595875422369 , time_score:  115 , memory:  359756\n","Episode:  1385  , Epsilon:  0.01 , Reward -1.1423752973351782 , mean_reward:  -553.5489029176744 , time_score:  228 , memory:  360619\n","Episode:  1390  , Epsilon:  0.01 , Reward -158.55536125658057 , mean_reward:  -508.62614475651475 , time_score:  130 , memory:  361953\n","Episode:  1395  , Epsilon:  0.01 , Reward -98.61214009993478 , mean_reward:  -452.9081898597795 , time_score:  206 , memory:  362666\n","Episode:  1400  , Epsilon:  0.01 , Reward -217.96044498124832 , mean_reward:  -431.52323435925916 , time_score:  335 , memory:  364023\n","Episode:  1405  , Epsilon:  0.01 , Reward -249.43625864270015 , mean_reward:  -421.6974515914745 , time_score:  327 , memory:  365155\n","Episode:  1410  , Epsilon:  0.01 , Reward -351.5481122203264 , mean_reward:  -410.3719562717935 , time_score:  52 , memory:  365978\n","Episode:  1415  , Epsilon:  0.01 , Reward -221.65082765470305 , mean_reward:  -388.6292315948356 , time_score:  52 , memory:  366785\n","Episode:  1420  , Epsilon:  0.01 , Reward -375.4319470058723 , mean_reward:  -384.28837624621525 , time_score:  72 , memory:  367356\n","Episode:  1425  , Epsilon:  0.01 , Reward -298.0329964068168 , mean_reward:  -340.82837742268225 , time_score:  51 , memory:  368038\n","Episode:  1430  , Epsilon:  0.01 , Reward -360.9228567479338 , mean_reward:  -324.08069814981906 , time_score:  153 , memory:  368608\n","Episode:  1435  , Epsilon:  0.01 , Reward -230.90708493914738 , mean_reward:  -314.95870361314644 , time_score:  103 , memory:  369021\n","Episode:  1440  , Epsilon:  0.01 , Reward -433.04007540844407 , mean_reward:  -312.54067994077786 , time_score:  73 , memory:  369476\n","Episode:  1445  , Epsilon:  0.01 , Reward -195.75030382094727 , mean_reward:  -310.7444226085292 , time_score:  164 , memory:  369967\n","Episode:  1450  , Epsilon:  0.01 , Reward -107.61407291177287 , mean_reward:  -311.69609933474214 , time_score:  52 , memory:  370704\n","Episode:  1455  , Epsilon:  0.01 , Reward -406.48866496532656 , mean_reward:  -307.77085154538236 , time_score:  72 , memory:  371853\n","Episode:  1460  , Epsilon:  0.01 , Reward -662.3809146571228 , mean_reward:  -298.5687649857965 , time_score:  113 , memory:  372846\n","Episode:  1465  , Epsilon:  0.01 , Reward -539.6503392866728 , mean_reward:  -308.3096259796693 , time_score:  110 , memory:  373542\n","Episode:  1470  , Epsilon:  0.01 , Reward -441.5770782361051 , mean_reward:  -317.4738598536529 , time_score:  199 , memory:  374504\n","Episode:  1475  , Epsilon:  0.01 , Reward -49.32234615977651 , mean_reward:  -312.3731286508314 , time_score:  213 , memory:  375224\n","Episode:  1480  , Epsilon:  0.01 , Reward -192.53820993779647 , mean_reward:  -308.698314836385 , time_score:  111 , memory:  376048\n","Episode:  1485  , Epsilon:  0.01 , Reward -107.36374556997416 , mean_reward:  -312.1273655457615 , time_score:  53 , memory:  376398\n","Episode:  1490  , Epsilon:  0.01 , Reward -129.01471940428866 , mean_reward:  -310.53984459046694 , time_score:  190 , memory:  377046\n","Episode:  1495  , Epsilon:  0.01 , Reward -156.93227297374233 , mean_reward:  -315.83521039460425 , time_score:  54 , memory:  377797\n","Episode:  1500  , Epsilon:  0.01 , Reward -249.7873182338631 , mean_reward:  -321.8126651707336 , time_score:  78 , memory:  378143\n","Episode:  1505  , Epsilon:  0.01 , Reward -332.39285162948966 , mean_reward:  -317.7181146594476 , time_score:  81 , memory:  378725\n","Episode:  1510  , Epsilon:  0.01 , Reward -303.7829760142213 , mean_reward:  -309.2120586144784 , time_score:  63 , memory:  379099\n","Episode:  1515  , Epsilon:  0.01 , Reward -290.9873275163177 , mean_reward:  -307.454687440699 , time_score:  77 , memory:  379453\n","Episode:  1520  , Epsilon:  0.01 , Reward -572.1410966133731 , mean_reward:  -311.96936976117837 , time_score:  65 , memory:  379885\n","Episode:  1525  , Epsilon:  0.01 , Reward -140.66830143801437 , mean_reward:  -305.59152402419056 , time_score:  69 , memory:  380462\n","Episode:  1530  , Epsilon:  0.01 , Reward -90.16105166062346 , mean_reward:  -305.9731217995743 , time_score:  85 , memory:  381041\n","Episode:  1535  , Epsilon:  0.01 , Reward -366.8894678396389 , mean_reward:  -306.095986530233 , time_score:  77 , memory:  381559\n","Episode:  1540  , Epsilon:  0.01 , Reward -120.57470024637385 , mean_reward:  -299.97966194935873 , time_score:  153 , memory:  382120\n","Episode:  1545  , Epsilon:  0.01 , Reward -293.386807187215 , mean_reward:  -290.6451504611858 , time_score:  336 , memory:  383117\n","Episode:  1550  , Epsilon:  0.01 , Reward -716.0589595790627 , mean_reward:  -303.0636873087697 , time_score:  104 , memory:  384034\n","Episode:  1555  , Epsilon:  0.01 , Reward -531.7803492569057 , mean_reward:  -308.4124024909799 , time_score:  99 , memory:  384915\n","Episode:  1560  , Epsilon:  0.01 , Reward -513.2156970750045 , mean_reward:  -306.68082153338565 , time_score:  231 , memory:  386107\n","Episode:  1565  , Epsilon:  0.01 , Reward -440.74490519066944 , mean_reward:  -302.3254443450981 , time_score:  115 , memory:  387651\n","Episode:  1570  , Epsilon:  0.01 , Reward -590.1550866107443 , mean_reward:  -307.5567659748016 , time_score:  183 , memory:  388804\n","Episode:  1575  , Epsilon:  0.01 , Reward -375.1744310877011 , mean_reward:  -318.43205251666046 , time_score:  74 , memory:  389808\n","Episode:  1580  , Epsilon:  0.01 , Reward -425.9068262464778 , mean_reward:  -332.54133189624935 , time_score:  344 , memory:  390947\n","Episode:  1585  , Epsilon:  0.01 , Reward -327.8711355402778 , mean_reward:  -346.0286226986232 , time_score:  106 , memory:  391797\n","Episode:  1590  , Epsilon:  0.01 , Reward -478.18239117827227 , mean_reward:  -358.43294219395966 , time_score:  69 , memory:  392290\n","Episode:  1595  , Epsilon:  0.01 , Reward -391.0667199017875 , mean_reward:  -368.0298638443526 , time_score:  123 , memory:  392851\n","Episode:  1600  , Epsilon:  0.01 , Reward -368.9331579019071 , mean_reward:  -370.6677938586696 , time_score:  86 , memory:  393334\n","Episode:  1605  , Epsilon:  0.01 , Reward -192.06635385447936 , mean_reward:  -378.48788100861225 , time_score:  88 , memory:  393891\n","Episode:  1610  , Epsilon:  0.01 , Reward -470.8993310066198 , mean_reward:  -388.52110875050187 , time_score:  62 , memory:  394387\n","Episode:  1615  , Epsilon:  0.01 , Reward -326.106318226389 , mean_reward:  -397.4050592441483 , time_score:  70 , memory:  394826\n","Episode:  1620  , Epsilon:  0.01 , Reward -568.3736716648502 , mean_reward:  -396.22361187283457 , time_score:  75 , memory:  395236\n","Episode:  1625  , Epsilon:  0.01 , Reward -304.9079918106337 , mean_reward:  -408.4358469113666 , time_score:  67 , memory:  395652\n","Episode:  1630  , Epsilon:  0.01 , Reward -314.0154587185004 , mean_reward:  -415.3336181088331 , time_score:  101 , memory:  396096\n","Episode:  1635  , Epsilon:  0.01 , Reward -484.7455656811966 , mean_reward:  -423.48106624132225 , time_score:  140 , memory:  396597\n","Episode:  1640  , Epsilon:  0.01 , Reward -528.164635075746 , mean_reward:  -429.4396294165983 , time_score:  74 , memory:  396966\n","Episode:  1645  , Epsilon:  0.01 , Reward -573.1757865407345 , mean_reward:  -439.68453846342777 , time_score:  69 , memory:  397366\n","Episode:  1650  , Epsilon:  0.01 , Reward -19.52370344762174 , mean_reward:  -420.5466771405562 , time_score:  75 , memory:  397726\n","Episode:  1655  , Epsilon:  0.01 , Reward -365.00428358593615 , mean_reward:  -410.5260492227073 , time_score:  63 , memory:  398149\n","Episode:  1660  , Epsilon:  0.01 , Reward -331.9148934530332 , mean_reward:  -411.23107548272776 , time_score:  87 , memory:  398674\n","Episode:  1665  , Epsilon:  0.01 , Reward -662.6286045449442 , mean_reward:  -415.82542075747557 , time_score:  77 , memory:  399113\n","Episode:  1670  , Epsilon:  0.01 , Reward -317.5294584065957 , mean_reward:  -413.68792338690554 , time_score:  70 , memory:  399767\n","Episode:  1675  , Epsilon:  0.01 , Reward -220.75257417852768 , mean_reward:  -405.1679597027053 , time_score:  51 , memory:  400237\n","Episode:  1680  , Epsilon:  0.01 , Reward -563.8505451505586 , mean_reward:  -400.39114078349826 , time_score:  71 , memory:  400649\n","Episode:  1685  , Epsilon:  0.01 , Reward -210.08203564509267 , mean_reward:  -390.2260078170982 , time_score:  75 , memory:  401220\n","Episode:  1690  , Epsilon:  0.01 , Reward -298.5390499270117 , mean_reward:  -379.66792570417124 , time_score:  74 , memory:  401633\n","Episode:  1695  , Epsilon:  0.01 , Reward -420.1192411689358 , mean_reward:  -370.56851013868953 , time_score:  104 , memory:  402143\n","Episode:  1700  , Epsilon:  0.01 , Reward -107.34121714938892 , mean_reward:  -366.2904454180093 , time_score:  134 , memory:  402589\n","Episode:  1705  , Epsilon:  0.01 , Reward -195.5072125694305 , mean_reward:  -355.6207083812236 , time_score:  62 , memory:  402962\n","Episode:  1710  , Epsilon:  0.01 , Reward -349.08703776622696 , mean_reward:  -348.9502940750977 , time_score:  123 , memory:  403391\n","Episode:  1715  , Epsilon:  0.01 , Reward -218.1051479128451 , mean_reward:  -339.04169698574117 , time_score:  85 , memory:  403828\n","Episode:  1720  , Epsilon:  0.01 , Reward -346.12893604020184 , mean_reward:  -329.82946824420577 , time_score:  93 , memory:  404233\n","Episode:  1725  , Epsilon:  0.01 , Reward -411.987060074448 , mean_reward:  -323.17461416717964 , time_score:  57 , memory:  404666\n","Episode:  1730  , Epsilon:  0.01 , Reward -461.6516290163785 , mean_reward:  -323.79503255429483 , time_score:  350 , memory:  405589\n","Episode:  1735  , Epsilon:  0.01 , Reward -695.9132074011171 , mean_reward:  -324.939945674163 , time_score:  236 , memory:  406646\n","Episode:  1740  , Epsilon:  0.01 , Reward -124.90958413236618 , mean_reward:  -328.2555290544398 , time_score:  155 , memory:  407654\n","Episode:  1745  , Epsilon:  0.01 , Reward -510.1408361956451 , mean_reward:  -326.41135215144544 , time_score:  299 , memory:  409121\n","Episode:  1750  , Epsilon:  0.01 , Reward -345.98937613183864 , mean_reward:  -343.3138014548683 , time_score:  295 , memory:  410972\n","Episode:  1755  , Epsilon:  0.01 , Reward -435.2257810015809 , mean_reward:  -373.929948695657 , time_score:  213 , memory:  412285\n","Episode:  1760  , Epsilon:  0.01 , Reward -613.9841376384392 , mean_reward:  -375.0834787277697 , time_score:  270 , memory:  413046\n","Episode:  1765  , Epsilon:  0.01 , Reward -815.4621987117232 , mean_reward:  -391.1598394042015 , time_score:  261 , memory:  414454\n","Episode:  1770  , Epsilon:  0.01 , Reward -406.76615548578616 , mean_reward:  -398.5280409690866 , time_score:  401 , memory:  415898\n","Episode:  1775  , Epsilon:  0.01 , Reward -848.5826772512502 , mean_reward:  -407.6159003805495 , time_score:  395 , memory:  417480\n","Episode:  1780  , Epsilon:  0.01 , Reward -230.70833940125493 , mean_reward:  -401.0073025220214 , time_score:  52 , memory:  418654\n","Episode:  1785  , Epsilon:  0.01 , Reward -567.9597765022354 , mean_reward:  -409.84685287618146 , time_score:  297 , memory:  420312\n","Episode:  1790  , Epsilon:  0.01 , Reward -270.1713389550986 , mean_reward:  -433.3646758124638 , time_score:  500 , memory:  422545\n","Episode:  1795  , Epsilon:  0.01 , Reward -898.119465832521 , mean_reward:  -470.28861215468095 , time_score:  418 , memory:  424447\n","Episode:  1800  , Epsilon:  0.01 , Reward -935.4054298827175 , mean_reward:  -490.76564926835846 , time_score:  451 , memory:  425841\n","Episode:  1805  , Epsilon:  0.01 , Reward -1156.0716531244955 , mean_reward:  -530.3732590973501 , time_score:  488 , memory:  427512\n","Episode:  1810  , Epsilon:  0.01 , Reward -735.8768798926191 , mean_reward:  -568.1363566284947 , time_score:  180 , memory:  428972\n","Episode:  1815  , Epsilon:  0.01 , Reward -949.5617170754467 , mean_reward:  -614.5277270355892 , time_score:  301 , memory:  430612\n","Episode:  1820  , Epsilon:  0.01 , Reward -917.8940172478734 , mean_reward:  -666.2778658107251 , time_score:  278 , memory:  431984\n","Episode:  1825  , Epsilon:  0.01 , Reward -2044.181086083744 , mean_reward:  -699.3270389687622 , time_score:  329 , memory:  433493\n","Episode:  1830  , Epsilon:  0.01 , Reward -1697.4848119856572 , mean_reward:  -730.7629498849311 , time_score:  306 , memory:  435035\n","Episode:  1835  , Epsilon:  0.01 , Reward -547.1083539660216 , mean_reward:  -754.3444884850262 , time_score:  500 , memory:  436854\n","Episode:  1840  , Epsilon:  0.01 , Reward -272.34115349336065 , mean_reward:  -766.0270084749781 , time_score:  500 , memory:  438445\n","Episode:  1845  , Epsilon:  0.01 , Reward -496.418302088392 , mean_reward:  -788.7827260948039 , time_score:  333 , memory:  440216\n","Episode:  1850  , Epsilon:  0.01 , Reward -571.9252775401264 , mean_reward:  -790.1010546937896 , time_score:  441 , memory:  442068\n","Episode:  1855  , Epsilon:  0.01 , Reward -472.13473120638713 , mean_reward:  -768.3275516401476 , time_score:  261 , memory:  443477\n","Episode:  1860  , Epsilon:  0.01 , Reward -293.8524551687676 , mean_reward:  -770.4610887363971 , time_score:  98 , memory:  444934\n","Episode:  1865  , Epsilon:  0.01 , Reward -492.4051741703319 , mean_reward:  -757.5495940384776 , time_score:  500 , memory:  446598\n","Episode:  1870  , Epsilon:  0.01 , Reward -1407.6801761357485 , mean_reward:  -759.6476793859798 , time_score:  376 , memory:  448754\n","Episode:  1875  , Epsilon:  0.01 , Reward -373.7745624711688 , mean_reward:  -753.3629150310393 , time_score:  279 , memory:  450237\n","Episode:  1880  , Epsilon:  0.01 , Reward -549.2612952706703 , mean_reward:  -769.2276630945022 , time_score:  251 , memory:  451764\n","Episode:  1885  , Epsilon:  0.01 , Reward -501.16895539236805 , mean_reward:  -767.5508775951384 , time_score:  430 , memory:  453300\n","Episode:  1890  , Epsilon:  0.01 , Reward -405.20145515178626 , mean_reward:  -750.3361293051739 , time_score:  303 , memory:  454035\n","Episode:  1895  , Epsilon:  0.01 , Reward -103.56217148914664 , mean_reward:  -715.536269846128 , time_score:  169 , memory:  455596\n","Episode:  1900  , Epsilon:  0.01 , Reward -474.29428837796075 , mean_reward:  -693.009075962147 , time_score:  276 , memory:  456531\n","Episode:  1905  , Epsilon:  0.01 , Reward -92.33891178484924 , mean_reward:  -655.1648989893426 , time_score:  149 , memory:  457653\n","Episode:  1910  , Epsilon:  0.01 , Reward -345.06089305790175 , mean_reward:  -614.114339499553 , time_score:  256 , memory:  458617\n","Episode:  1915  , Epsilon:  0.01 , Reward -402.06752707010696 , mean_reward:  -564.7068989747928 , time_score:  492 , memory:  460024\n","Episode:  1920  , Epsilon:  0.01 , Reward -446.33626812473153 , mean_reward:  -524.5390262461054 , time_score:  500 , memory:  461516\n","Episode:  1925  , Epsilon:  0.01 , Reward -204.7440513330081 , mean_reward:  -505.2860736054011 , time_score:  500 , memory:  463398\n","Episode:  1930  , Epsilon:  0.01 , Reward -628.6448764560224 , mean_reward:  -473.20894662917965 , time_score:  271 , memory:  465150\n","Episode:  1935  , Epsilon:  0.01 , Reward -259.3571552333458 , mean_reward:  -445.381664730406 , time_score:  85 , memory:  466032\n","Episode:  1940  , Epsilon:  0.01 , Reward -483.4746504379759 , mean_reward:  -426.0649508256091 , time_score:  88 , memory:  466439\n","Episode:  1945  , Epsilon:  0.01 , Reward -643.479690400378 , mean_reward:  -402.7573889554463 , time_score:  232 , memory:  467142\n","Episode:  1950  , Epsilon:  0.01 , Reward -281.12250351926787 , mean_reward:  -393.71860975299256 , time_score:  82 , memory:  467511\n","Episode:  1955  , Epsilon:  0.01 , Reward -321.2156515774805 , mean_reward:  -387.50782991140187 , time_score:  306 , memory:  469081\n","Episode:  1960  , Epsilon:  0.01 , Reward -53.74162774653223 , mean_reward:  -376.72653356481385 , time_score:  132 , memory:  470106\n","Episode:  1965  , Epsilon:  0.01 , Reward -333.94062909379494 , mean_reward:  -362.6416671036007 , time_score:  73 , memory:  470522\n","Episode:  1970  , Epsilon:  0.01 , Reward -336.0223950186221 , mean_reward:  -344.95536142222045 , time_score:  111 , memory:  471086\n","Episode:  1975  , Epsilon:  0.01 , Reward -547.1882301658615 , mean_reward:  -355.85185616097255 , time_score:  80 , memory:  471553\n","Episode:  1980  , Epsilon:  0.01 , Reward -380.07806324153165 , mean_reward:  -343.9551172856943 , time_score:  74 , memory:  471912\n","Episode:  1985  , Epsilon:  0.01 , Reward -516.1224391484674 , mean_reward:  -345.2100889171257 , time_score:  60 , memory:  472308\n","Episode:  1990  , Epsilon:  0.01 , Reward -365.1491070029125 , mean_reward:  -349.2050167401196 , time_score:  67 , memory:  472713\n","Episode:  1995  , Epsilon:  0.01 , Reward -298.21727628409894 , mean_reward:  -352.40055257773696 , time_score:  101 , memory:  473115\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QzXeEPyZkZx5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmu7jobCkZ0S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}