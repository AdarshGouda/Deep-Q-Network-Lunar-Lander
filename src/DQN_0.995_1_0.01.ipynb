{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_0.995_1_0.01.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"mount_file_id":"1wSff9KP4qeOoAR8CMqQl0sUr4LgJMT4e","authorship_tag":"ABX9TyPzKHAXOYWsBqmPHndybJHK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624389568477,"user_tz":360,"elapsed":802,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4d75c3a5-0a8c-4bbb-aa15-af916d6f396a"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sRtQsqz4QLkP"},"source":["#COMPLETED"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624737537724,"user_tz":360,"elapsed":9674,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"6b7deaf6-54ad-447a-9958-3dae86d2b912"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting box2d-py\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 6.6MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vq1HYdz1sP4d"},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8"},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/1.0_0.01/1p0_0p01.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/1.0_0.01/1p0_0p01.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624752691510,"user_tz":360,"elapsed":15125842,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"215eff32-057f-498a-9536-02bfc92a1709"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 1.0, lr=0.01)\n","df = dqn.play()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -107.56986044874597 , mean_reward:  -107.56986044874597 , time_score:  81 , memory:  81\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -256.4140843166911 , mean_reward:  -198.3388729584251 , time_score:  66 , memory:  490\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -124.91003919683644 , mean_reward:  -163.17841225686914 , time_score:  79 , memory:  939\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -136.2589745721972 , mean_reward:  -144.54835298803664 , time_score:  87 , memory:  1406\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward -199.15902079153778 , mean_reward:  -144.26034353268915 , time_score:  109 , memory:  1915\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -9.135933877517331 , mean_reward:  -146.22234803668528 , time_score:  130 , memory:  2494\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -108.77653346897434 , mean_reward:  -157.2877990097332 , time_score:  93 , memory:  2958\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -72.89762838510053 , mean_reward:  -148.59516024986368 , time_score:  102 , memory:  3369\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -94.70074310688886 , mean_reward:  -146.42898705937597 , time_score:  107 , memory:  3942\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -178.72953580223052 , mean_reward:  -144.10929649607667 , time_score:  124 , memory:  4554\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -139.12802161866108 , mean_reward:  -141.9204519540134 , time_score:  81 , memory:  5095\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -163.50377952605737 , mean_reward:  -138.01174831039467 , time_score:  164 , memory:  5630\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -129.37166757049812 , mean_reward:  -134.051261022213 , time_score:  130 , memory:  6187\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -63.995954867865066 , mean_reward:  -128.73417009263656 , time_score:  74 , memory:  6726\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -95.08803410372502 , mean_reward:  -126.95897261423976 , time_score:  121 , memory:  7402\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward -54.32820264365351 , mean_reward:  -123.02757066175603 , time_score:  124 , memory:  8077\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -72.60745203120764 , mean_reward:  -118.2756548293747 , time_score:  156 , memory:  8702\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -45.486147226435634 , mean_reward:  -116.31563461602227 , time_score:  118 , memory:  9396\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -38.945092210115334 , mean_reward:  -114.00355599569407 , time_score:  123 , memory:  9953\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -33.72553716846613 , mean_reward:  -114.0950749017788 , time_score:  112 , memory:  10614\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward -37.6299213239741 , mean_reward:  -111.34240887748827 , time_score:  139 , memory:  11190\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -192.6527785167667 , mean_reward:  -103.77186970895346 , time_score:  189 , memory:  12003\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward -38.53943423182953 , mean_reward:  -102.4090341600568 , time_score:  118 , memory:  13112\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward -122.48590908178997 , mean_reward:  -101.95142568709569 , time_score:  215 , memory:  14106\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -314.51373092111146 , mean_reward:  -100.48853437677201 , time_score:  97 , memory:  14769\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward 6.301626496500219 , mean_reward:  -99.88157156662872 , time_score:  144 , memory:  15663\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -261.45108612567253 , mean_reward:  -100.96329732168586 , time_score:  106 , memory:  16392\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -126.77713825904988 , mean_reward:  -105.84784279454631 , time_score:  197 , memory:  17151\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -77.31176868350441 , mean_reward:  -103.1920069903956 , time_score:  237 , memory:  18314\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -182.07504298346592 , mean_reward:  -100.35795723330901 , time_score:  437 , memory:  19427\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward 3.0116701085032673 , mean_reward:  -96.67604466110458 , time_score:  158 , memory:  20372\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward -480.1977696651858 , mean_reward:  -103.08001879948101 , time_score:  121 , memory:  21341\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward -40.47294174489322 , mean_reward:  -104.78239874612478 , time_score:  121 , memory:  21808\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward -38.053753577939666 , mean_reward:  -112.08801598522312 , time_score:  160 , memory:  22781\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward -12.348907564710771 , mean_reward:  -114.77308028244974 , time_score:  204 , memory:  24330\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward -232.04572063466182 , mean_reward:  -116.39112538810373 , time_score:  328 , memory:  25777\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward -289.71214698907966 , mean_reward:  -120.05493308877465 , time_score:  206 , memory:  27611\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward -43.45702816391672 , mean_reward:  -117.98566921256577 , time_score:  117 , memory:  29097\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward -416.0789030417186 , mean_reward:  -121.63346517339737 , time_score:  221 , memory:  30855\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward -162.980054248849 , mean_reward:  -127.13727299111238 , time_score:  85 , memory:  32010\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward -605.6903604992381 , mean_reward:  -139.55537129293856 , time_score:  319 , memory:  32760\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward -212.30262518886934 , mean_reward:  -142.2977407050821 , time_score:  243 , memory:  34692\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward 26.724239782313475 , mean_reward:  -143.9573309971439 , time_score:  500 , memory:  36374\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward 59.78224268213775 , mean_reward:  -138.87980040593857 , time_score:  500 , memory:  38255\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward -112.35632301042439 , mean_reward:  -138.97560864759748 , time_score:  285 , memory:  40540\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward -150.6877006460477 , mean_reward:  -137.3598902387624 , time_score:  309 , memory:  42391\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward -144.704504914329 , mean_reward:  -127.50397213462239 , time_score:  348 , memory:  44065\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward -33.87133606464095 , mean_reward:  -124.3389593440434 , time_score:  500 , memory:  45764\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward -295.0104343587428 , mean_reward:  -129.48439554383606 , time_score:  407 , memory:  47826\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward -301.8475594410986 , mean_reward:  -134.86473778768277 , time_score:  157 , memory:  48854\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward -360.87846860088524 , mean_reward:  -138.75766092197665 , time_score:  192 , memory:  49949\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward -233.03741293720836 , mean_reward:  -136.45713494670446 , time_score:  298 , memory:  51445\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward -107.39645614627773 , mean_reward:  -137.88012996222886 , time_score:  263 , memory:  52753\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward -82.15742468920011 , mean_reward:  -133.68518020181864 , time_score:  108 , memory:  54536\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward -142.38468221761826 , mean_reward:  -134.68451863215364 , time_score:  315 , memory:  55922\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward -297.35029527764806 , mean_reward:  -142.25643865988295 , time_score:  397 , memory:  57254\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward 56.549442741474415 , mean_reward:  -144.97293833326705 , time_score:  500 , memory:  58729\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward -170.52711543836594 , mean_reward:  -151.65644347685654 , time_score:  243 , memory:  60095\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward -183.64690964210286 , mean_reward:  -147.9861124267265 , time_score:  158 , memory:  61211\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward -279.2281661289159 , mean_reward:  -139.7967734780562 , time_score:  444 , memory:  63655\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward -71.43391373052685 , mean_reward:  -130.49575543413002 , time_score:  86 , memory:  64599\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward -363.257497195451 , mean_reward:  -136.74624294564182 , time_score:  106 , memory:  65047\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward -13.856767863786464 , mean_reward:  -137.5868523112191 , time_score:  500 , memory:  66448\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward -222.9092557036128 , mean_reward:  -154.88567891787568 , time_score:  291 , memory:  68042\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward -25.452923158257054 , mean_reward:  -152.21450095338488 , time_score:  500 , memory:  70285\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward 88.14869651059823 , mean_reward:  -147.70535262052223 , time_score:  500 , memory:  72785\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward 23.59908680105681 , mean_reward:  -148.64609298511886 , time_score:  500 , memory:  74701\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward 99.19860004644174 , mean_reward:  -143.60558602091757 , time_score:  500 , memory:  76875\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward -69.01030596722322 , mean_reward:  -135.09392727188066 , time_score:  500 , memory:  79035\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward 4.761178270774678 , mean_reward:  -126.98071751066838 , time_score:  500 , memory:  80954\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward 114.49297958668971 , mean_reward:  -117.94929914098506 , time_score:  500 , memory:  83454\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward -127.89288830464247 , mean_reward:  -111.34967910970309 , time_score:  500 , memory:  85925\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward -122.71972860440387 , mean_reward:  -114.9813409590113 , time_score:  500 , memory:  88301\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward 45.25044669682177 , mean_reward:  -117.63449878491784 , time_score:  500 , memory:  90268\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward 30.570841468543946 , mean_reward:  -112.6477273619278 , time_score:  500 , memory:  92618\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward 55.36002943279973 , mean_reward:  -99.6441985571104 , time_score:  500 , memory:  95118\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward -37.10109619050564 , mean_reward:  -90.80107451919795 , time_score:  500 , memory:  97409\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward -5.897505681067951 , mean_reward:  -83.2425395847123 , time_score:  500 , memory:  99909\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward 54.25495635035703 , mean_reward:  -76.99946844554346 , time_score:  500 , memory:  102409\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward 108.34381091880749 , mean_reward:  -73.93589279289223 , time_score:  500 , memory:  104779\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward -150.198550542744 , mean_reward:  -72.17050932317962 , time_score:  98 , memory:  106450\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward 12.114150552339842 , mean_reward:  -64.58616095782288 , time_score:  500 , memory:  107894\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward 97.20792418906042 , mean_reward:  -53.39455157024811 , time_score:  500 , memory:  110394\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward -263.89785588172344 , mean_reward:  -39.93553807823972 , time_score:  319 , memory:  112713\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward 76.11891753724889 , mean_reward:  -32.147918820647284 , time_score:  500 , memory:  115213\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward 134.13846153003593 , mean_reward:  -28.962453900121105 , time_score:  500 , memory:  117713\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward -53.238426340465466 , mean_reward:  -24.042620495117536 , time_score:  500 , memory:  120213\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward -742.379722095357 , mean_reward:  -35.201735006490146 , time_score:  211 , memory:  121980\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward -497.44412611277414 , mean_reward:  -56.90314675018185 , time_score:  82 , memory:  122704\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward -428.93145689051914 , mean_reward:  -76.76382412625969 , time_score:  75 , memory:  123151\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward -592.7727157106988 , mean_reward:  -115.9665951607606 , time_score:  88 , memory:  123775\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward -194.65143972229438 , mean_reward:  -130.65862218738772 , time_score:  103 , memory:  124376\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward -434.0203206433117 , mean_reward:  -138.33942409244236 , time_score:  226 , memory:  124950\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward -440.4601828860235 , mean_reward:  -163.74509807745045 , time_score:  269 , memory:  126057\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward -359.43370987457337 , mean_reward:  -175.82533149853566 , time_score:  282 , memory:  127402\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward -626.9572221590938 , mean_reward:  -195.61238813697292 , time_score:  179 , memory:  128644\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward -441.05545136669343 , mean_reward:  -215.1086583742794 , time_score:  103 , memory:  129813\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward -655.1354954449699 , mean_reward:  -241.3553392038227 , time_score:  444 , memory:  131375\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward -938.3564198071406 , mean_reward:  -266.7069212815821 , time_score:  390 , memory:  132715\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward -780.9456223120164 , mean_reward:  -292.4135283820871 , time_score:  180 , memory:  133512\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward -642.6437784316988 , mean_reward:  -311.02346711395813 , time_score:  144 , memory:  134208\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward -441.82445568350204 , mean_reward:  -337.0145506542483 , time_score:  275 , memory:  135062\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward -724.5553506943944 , mean_reward:  -368.097912563934 , time_score:  209 , memory:  136141\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward -392.08751430947063 , mean_reward:  -383.25277468878886 , time_score:  500 , memory:  137629\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward 72.8779542491341 , mean_reward:  -411.811645767388 , time_score:  500 , memory:  139125\n","Episode:  525  , Epsilon:  0.07196434741762824 , Reward -269.7361809623254 , mean_reward:  -443.65578666784296 , time_score:  150 , memory:  141030\n","Episode:  530  , Epsilon:  0.07018314008827135 , Reward -112.11372039001108 , mean_reward:  -457.1035772674591 , time_score:  500 , memory:  142998\n","Episode:  535  , Epsilon:  0.06844601986126451 , Reward -318.51074752417446 , mean_reward:  -458.6532028968502 , time_score:  444 , memory:  144376\n","Episode:  540  , Epsilon:  0.0667518955258533 , Reward -304.22099754820135 , mean_reward:  -444.1299717571279 , time_score:  366 , memory:  146500\n","Episode:  545  , Epsilon:  0.06509970288011008 , Reward -83.60388265327614 , mean_reward:  -431.9017380639348 , time_score:  207 , memory:  148544\n","Episode:  550  , Epsilon:  0.06348840406243188 , Reward -33.11176638391576 , mean_reward:  -400.7949749382291 , time_score:  500 , memory:  150860\n","Episode:  555  , Epsilon:  0.06191698689958447 , Reward -967.4862067818352 , mean_reward:  -403.4003150313216 , time_score:  217 , memory:  152878\n","Episode:  560  , Epsilon:  0.06038446427088321 , Reward -76.8043698540911 , mean_reward:  -395.9452725599263 , time_score:  500 , memory:  155015\n","Episode:  565  , Epsilon:  0.058889873488111255 , Reward -29.269687022723794 , mean_reward:  -368.55442135929525 , time_score:  500 , memory:  157515\n","Episode:  570  , Epsilon:  0.05743227569078546 , Reward -14.980749910167546 , mean_reward:  -355.42511893239424 , time_score:  500 , memory:  160015\n","Episode:  575  , Epsilon:  0.05601075525639029 , Reward -375.6227620643411 , mean_reward:  -349.8572704602582 , time_score:  284 , memory:  162262\n","Episode:  580  , Epsilon:  0.05462441922520914 , Reward -248.04141465327092 , mean_reward:  -337.44917980619795 , time_score:  459 , memory:  164721\n","Episode:  585  , Epsilon:  0.05327239673939179 , Reward -10.166900986464352 , mean_reward:  -310.7566428358817 , time_score:  500 , memory:  167221\n","Episode:  590  , Epsilon:  0.05195383849590569 , Reward -170.14364517888416 , mean_reward:  -291.19264083341585 , time_score:  500 , memory:  169721\n","Episode:  595  , Epsilon:  0.05066791621302729 , Reward -102.50195163132763 , mean_reward:  -267.044125765401 , time_score:  500 , memory:  172221\n","Episode:  600  , Epsilon:  0.0494138221100385 , Reward -115.68244455450889 , mean_reward:  -249.8022199640384 , time_score:  500 , memory:  174397\n","Episode:  605  , Epsilon:  0.048190768399801194 , Reward 47.447000268566356 , mean_reward:  -219.56655907356685 , time_score:  500 , memory:  176897\n","Episode:  610  , Epsilon:  0.046997986793891174 , Reward 14.260733473310195 , mean_reward:  -190.49451719272486 , time_score:  500 , memory:  179397\n","Episode:  615  , Epsilon:  0.04583472801998072 , Reward -194.20859577216072 , mean_reward:  -174.0843989588637 , time_score:  362 , memory:  181678\n","Episode:  620  , Epsilon:  0.04470026135116646 , Reward 76.20214929144974 , mean_reward:  -148.41648478401646 , time_score:  500 , memory:  184178\n","Episode:  625  , Epsilon:  0.04359387414694703 , Reward 27.60850885398838 , mean_reward:  -117.7004128820476 , time_score:  500 , memory:  186678\n","Episode:  630  , Epsilon:  0.04251487140556204 , Reward -73.85089685767414 , mean_reward:  -105.29124762519727 , time_score:  443 , memory:  189121\n","Episode:  635  , Epsilon:  0.04146257532741124 , Reward 80.93856043770344 , mean_reward:  -93.51155375851096 , time_score:  500 , memory:  191163\n","Episode:  640  , Epsilon:  0.04043632488927963 , Reward 71.64313342992627 , mean_reward:  -84.59728352156442 , time_score:  500 , memory:  193653\n","Episode:  645  , Epsilon:  0.039435475429100995 , Reward 122.40871185213425 , mean_reward:  -75.20034694448013 , time_score:  500 , memory:  196153\n","Episode:  650  , Epsilon:  0.03845939824099909 , Reward -37.16046363602544 , mean_reward:  -67.35404277580989 , time_score:  500 , memory:  198653\n","Episode:  655  , Epsilon:  0.03750748018035199 , Reward 10.25792172150275 , mean_reward:  -49.264593476259016 , time_score:  500 , memory:  200743\n","Episode:  660  , Epsilon:  0.03657912327863173 , Reward -397.0511685590152 , mean_reward:  -43.718554231116656 , time_score:  167 , memory:  202608\n","Episode:  665  , Epsilon:  0.035673744367776934 , Reward -306.01738710546323 , mean_reward:  -56.16051259289335 , time_score:  130 , memory:  203335\n","Episode:  670  , Epsilon:  0.03479077471386296 , Reward -482.25542620849666 , mean_reward:  -74.7998938452577 , time_score:  74 , memory:  203761\n","Episode:  675  , Epsilon:  0.03392965965983891 , Reward -249.50625146718536 , mean_reward:  -81.21377886862749 , time_score:  76 , memory:  204501\n","Episode:  680  , Epsilon:  0.03308985827710748 , Reward -2433.8903833915956 , mean_reward:  -133.6082018287217 , time_score:  230 , memory:  205417\n","Episode:  685  , Epsilon:  0.03227084302572862 , Reward -865.6509187786435 , mean_reward:  -173.38815774049198 , time_score:  293 , memory:  206395\n","Episode:  690  , Epsilon:  0.03147209942303359 , Reward -628.1781473274859 , mean_reward:  -214.1697367308683 , time_score:  80 , memory:  206940\n","Episode:  695  , Epsilon:  0.030693125720441184 , Reward -1382.9046174514826 , mean_reward:  -253.6637419703206 , time_score:  150 , memory:  207726\n","Episode:  700  , Epsilon:  0.029933432588273214 , Reward -287.7421559251968 , mean_reward:  -282.19637982067235 , time_score:  245 , memory:  208740\n","Episode:  705  , Epsilon:  0.029192542808371146 , Reward -1126.853963206179 , mean_reward:  -319.0998885075367 , time_score:  139 , memory:  209782\n","Episode:  710  , Epsilon:  0.028469990974320916 , Reward -319.22966178771526 , mean_reward:  -351.80648124835716 , time_score:  91 , memory:  210338\n","Episode:  715  , Epsilon:  0.027765323199097504 , Reward -218.14089826771766 , mean_reward:  -376.3218719158561 , time_score:  107 , memory:  211152\n","Episode:  720  , Epsilon:  0.02707809682994571 , Reward -662.365302582528 , mean_reward:  -399.16001022742864 , time_score:  471 , memory:  213171\n","Episode:  725  , Epsilon:  0.026407880170317945 , Reward -66.5867455763006 , mean_reward:  -411.9375601883536 , time_score:  500 , memory:  215502\n","Episode:  730  , Epsilon:  0.025754252208694463 , Reward 160.90487246988792 , mean_reward:  -417.6804752266387 , time_score:  500 , memory:  217951\n","Episode:  735  , Epsilon:  0.025116802354115567 , Reward -23.621653592841714 , mean_reward:  -419.2722145779735 , time_score:  500 , memory:  220221\n","Episode:  740  , Epsilon:  0.02449513017825978 , Reward 37.41835135138943 , mean_reward:  -425.88444871638853 , time_score:  500 , memory:  222392\n","Episode:  745  , Epsilon:  0.023888845163905856 , Reward -254.94561266474378 , mean_reward:  -433.8055313631423 , time_score:  410 , memory:  223736\n","Episode:  750  , Epsilon:  0.023297566459620722 , Reward -474.429987825354 , mean_reward:  -468.9635703050265 , time_score:  123 , memory:  224709\n","Episode:  755  , Epsilon:  0.022720922640519125 , Reward -368.2425219517453 , mean_reward:  -484.1841414505112 , time_score:  341 , memory:  226202\n","Episode:  760  , Epsilon:  0.022158551474944856 , Reward -149.04158251665143 , mean_reward:  -490.3561522331558 , time_score:  500 , memory:  228318\n","Episode:  765  , Epsilon:  0.021610099696926857 , Reward -1076.509535850366 , mean_reward:  -492.5388505955076 , time_score:  290 , memory:  230479\n","Episode:  770  , Epsilon:  0.021075222784267326 , Reward -215.70479210029822 , mean_reward:  -481.39508838026495 , time_score:  500 , memory:  232802\n","Episode:  775  , Epsilon:  0.020553584742122436 , Reward -908.0422467835239 , mean_reward:  -477.90692140259665 , time_score:  266 , memory:  234367\n","Episode:  780  , Epsilon:  0.020044857891939702 , Reward 146.9537298416122 , mean_reward:  -422.5835389083927 , time_score:  442 , memory:  236773\n","Episode:  785  , Epsilon:  0.01954872266561937 , Reward -140.90835718161486 , mean_reward:  -386.71220261960826 , time_score:  284 , memory:  238795\n","Episode:  790  , Epsilon:  0.019064867404770626 , Reward -7.216314498992559 , mean_reward:  -347.4927629847172 , time_score:  500 , memory:  241202\n","Episode:  795  , Epsilon:  0.018592988164936427 , Reward -2847.0553566561493 , mean_reward:  -352.65241894792484 , time_score:  348 , memory:  243002\n","Episode:  800  , Epsilon:  0.018132788524664028 , Reward -510.8427106538391 , mean_reward:  -337.8834742092635 , time_score:  65 , memory:  243446\n","Episode:  805  , Epsilon:  0.017683979399301233 , Reward -374.30375985214175 , mean_reward:  -329.6466035237822 , time_score:  61 , memory:  243930\n","Episode:  810  , Epsilon:  0.01724627885940145 , Reward -730.9079097720704 , mean_reward:  -336.26269836389156 , time_score:  77 , memory:  244354\n","Episode:  815  , Epsilon:  0.01681941195362342 , Reward -1413.123918750444 , mean_reward:  -359.56823547501267 , time_score:  163 , memory:  244895\n","Episode:  820  , Epsilon:  0.0164031105360144 , Reward -661.1979640220331 , mean_reward:  -386.5694155006843 , time_score:  101 , memory:  245704\n","Episode:  825  , Epsilon:  0.015997113097568336 , Reward -5785.902472766631 , mean_reward:  -471.86999361891304 , time_score:  438 , memory:  246720\n","Episode:  830  , Epsilon:  0.015601164601953134 , Reward -1230.8299649170222 , mean_reward:  -532.1805835691881 , time_score:  176 , memory:  247579\n","Episode:  835  , Epsilon:  0.015215016325303928 , Reward -863.8419500492649 , mean_reward:  -569.3065300416351 , time_score:  302 , memory:  248355\n","Episode:  840  , Epsilon:  0.014838425699981627 , Reward -440.1024802279235 , mean_reward:  -590.6605262180009 , time_score:  183 , memory:  249185\n","Episode:  845  , Epsilon:  0.014471156162198668 , Reward -1060.238163129377 , mean_reward:  -615.7758002971037 , time_score:  122 , memory:  249868\n","Episode:  850  , Epsilon:  0.014112977003416188 , Reward -550.5717003254174 , mean_reward:  -619.8298285106712 , time_score:  76 , memory:  250806\n","Episode:  855  , Epsilon:  0.013763663225419333 , Reward -200.7356516171884 , mean_reward:  -617.7355350941739 , time_score:  127 , memory:  251415\n","Episode:  860  , Epsilon:  0.013422995398979608 , Reward -139.9670739588193 , mean_reward:  -618.6208338649508 , time_score:  128 , memory:  251957\n","Episode:  865  , Epsilon:  0.013090759526015528 , Reward -525.9375381219692 , mean_reward:  -614.5871495437096 , time_score:  205 , memory:  252754\n","Episode:  870  , Epsilon:  0.012766746905164949 , Reward -484.30431176717497 , mean_reward:  -625.0856356661312 , time_score:  95 , memory:  253351\n","Episode:  875  , Epsilon:  0.012450754000684672 , Reward -520.0432138145447 , mean_reward:  -640.0062721898237 , time_score:  179 , memory:  254229\n","Episode:  880  , Epsilon:  0.012142582314594924 , Reward -325.5437409352528 , mean_reward:  -660.3097968502572 , time_score:  134 , memory:  255085\n","Episode:  885  , Epsilon:  0.01184203826198843 , Reward -329.8802789290112 , mean_reward:  -673.5086525753615 , time_score:  101 , memory:  255814\n","Episode:  890  , Epsilon:  0.01154893304942575 , Reward -291.57007746848865 , mean_reward:  -690.104706577401 , time_score:  148 , memory:  256646\n","Episode:  895  , Epsilon:  0.011263082556340478 , Reward -479.97901499031695 , mean_reward:  -669.5963325577841 , time_score:  68 , memory:  257005\n","Episode:  900  , Epsilon:  0.01098430721937979 , Reward -270.31882364039825 , mean_reward:  -682.8473474433666 , time_score:  53 , memory:  257482\n","Episode:  905  , Epsilon:  0.01071243191960775 , Reward -819.5359678554988 , mean_reward:  -683.1585602948768 , time_score:  125 , memory:  258108\n","Episode:  910  , Epsilon:  0.010447285872500434 , Reward -546.6296839662248 , mean_reward:  -668.8315570104517 , time_score:  250 , memory:  258870\n","Episode:  915  , Epsilon:  0.010188702520663827 , Reward -599.2619681461706 , mean_reward:  -653.4552391166638 , time_score:  137 , memory:  259912\n","Episode:  920  , Epsilon:  0.01 , Reward -334.7776716975077 , mean_reward:  -629.4336106517748 , time_score:  341 , memory:  261276\n","Episode:  925  , Epsilon:  0.01 , Reward -765.4767835561314 , mean_reward:  -561.9334639984232 , time_score:  335 , memory:  262155\n","Episode:  930  , Epsilon:  0.01 , Reward -429.07501713475904 , mean_reward:  -520.9321156353925 , time_score:  126 , memory:  263535\n","Episode:  935  , Epsilon:  0.01 , Reward -865.2253923983101 , mean_reward:  -508.4361581104672 , time_score:  131 , memory:  264516\n","Episode:  940  , Epsilon:  0.01 , Reward -390.78224226189127 , mean_reward:  -512.181833423047 , time_score:  190 , memory:  265444\n","Episode:  945  , Epsilon:  0.01 , Reward -551.7925827413494 , mean_reward:  -523.4396837638119 , time_score:  75 , memory:  266049\n","Episode:  950  , Epsilon:  0.01 , Reward -1659.7766301735458 , mean_reward:  -527.3464451462812 , time_score:  148 , memory:  266574\n","Episode:  955  , Epsilon:  0.01 , Reward -986.5300283485017 , mean_reward:  -552.7842980128931 , time_score:  132 , memory:  267134\n","Episode:  960  , Epsilon:  0.01 , Reward -431.93219041441705 , mean_reward:  -577.0461782313049 , time_score:  78 , memory:  267965\n","Episode:  965  , Epsilon:  0.01 , Reward -471.1551006584538 , mean_reward:  -587.4367274910652 , time_score:  82 , memory:  268422\n","Episode:  970  , Epsilon:  0.01 , Reward -756.4778113830794 , mean_reward:  -597.1123712068617 , time_score:  97 , memory:  268866\n","Episode:  975  , Epsilon:  0.01 , Reward -706.596919664665 , mean_reward:  -597.756897433887 , time_score:  160 , memory:  269437\n","Episode:  980  , Epsilon:  0.01 , Reward -680.3591981437522 , mean_reward:  -607.1266123090626 , time_score:  82 , memory:  269968\n","Episode:  985  , Epsilon:  0.01 , Reward -590.4434791460271 , mean_reward:  -616.4008404933821 , time_score:  87 , memory:  270404\n","Episode:  990  , Epsilon:  0.01 , Reward -568.9523122917441 , mean_reward:  -622.577586923186 , time_score:  92 , memory:  270878\n","Episode:  995  , Epsilon:  0.01 , Reward -486.67310528508665 , mean_reward:  -622.1293725336823 , time_score:  120 , memory:  271303\n","Episode:  1000  , Epsilon:  0.01 , Reward -789.2433958525213 , mean_reward:  -625.7590795586667 , time_score:  114 , memory:  271951\n","Episode:  1005  , Epsilon:  0.01 , Reward -580.4798854484386 , mean_reward:  -624.1907210864581 , time_score:  92 , memory:  272462\n","Episode:  1010  , Epsilon:  0.01 , Reward -1179.4292227664464 , mean_reward:  -649.2584418350623 , time_score:  173 , memory:  273183\n","Episode:  1015  , Epsilon:  0.01 , Reward -694.0657553414085 , mean_reward:  -646.6959784710208 , time_score:  78 , memory:  273784\n","Episode:  1020  , Epsilon:  0.01 , Reward -634.3199568928546 , mean_reward:  -668.919598561652 , time_score:  124 , memory:  274441\n","Episode:  1025  , Epsilon:  0.01 , Reward -409.9379186982493 , mean_reward:  -664.8840061843027 , time_score:  96 , memory:  274904\n","Episode:  1030  , Epsilon:  0.01 , Reward -863.9579174650562 , mean_reward:  -681.233693693013 , time_score:  176 , memory:  275546\n","Episode:  1035  , Epsilon:  0.01 , Reward -808.1036876033546 , mean_reward:  -700.8236716256245 , time_score:  119 , memory:  276682\n","Episode:  1040  , Epsilon:  0.01 , Reward -485.89055812588833 , mean_reward:  -701.4429541461186 , time_score:  324 , memory:  278040\n","Episode:  1045  , Epsilon:  0.01 , Reward -627.6635658271958 , mean_reward:  -692.2737236036455 , time_score:  500 , memory:  279263\n","Episode:  1050  , Epsilon:  0.01 , Reward -1112.853177066752 , mean_reward:  -698.6488645374582 , time_score:  500 , memory:  280625\n","Episode:  1055  , Epsilon:  0.01 , Reward -883.4471368648291 , mean_reward:  -706.2767832533942 , time_score:  500 , memory:  282487\n","Episode:  1060  , Epsilon:  0.01 , Reward -174.96918486714776 , mean_reward:  -709.4901489075568 , time_score:  110 , memory:  284138\n","Episode:  1065  , Epsilon:  0.01 , Reward -336.16567071636956 , mean_reward:  -709.4698039254127 , time_score:  128 , memory:  285006\n","Episode:  1070  , Epsilon:  0.01 , Reward -283.96408255191665 , mean_reward:  -703.8021862346947 , time_score:  180 , memory:  286782\n","Episode:  1075  , Epsilon:  0.01 , Reward -445.4339847087563 , mean_reward:  -695.2392407773897 , time_score:  232 , memory:  288602\n","Episode:  1080  , Epsilon:  0.01 , Reward -420.039380735939 , mean_reward:  -681.2650256013715 , time_score:  123 , memory:  289749\n","Episode:  1085  , Epsilon:  0.01 , Reward -436.83610208302247 , mean_reward:  -682.5979470608772 , time_score:  117 , memory:  290837\n","Episode:  1090  , Epsilon:  0.01 , Reward -126.23381693207186 , mean_reward:  -683.5605289581309 , time_score:  186 , memory:  291665\n","Episode:  1095  , Epsilon:  0.01 , Reward -172.19633444191618 , mean_reward:  -678.4031993280518 , time_score:  89 , memory:  292754\n","Episode:  1100  , Epsilon:  0.01 , Reward -1879.430416058862 , mean_reward:  -696.4968487973288 , time_score:  241 , memory:  294025\n","Episode:  1105  , Epsilon:  0.01 , Reward -1279.8588293024636 , mean_reward:  -719.6326871503172 , time_score:  335 , memory:  295578\n","Episode:  1110  , Epsilon:  0.01 , Reward -138.94858372519644 , mean_reward:  -714.0808787334116 , time_score:  205 , memory:  296958\n","Episode:  1115  , Epsilon:  0.01 , Reward -58.06249880332166 , mean_reward:  -698.5730444454474 , time_score:  500 , memory:  298244\n","Episode:  1120  , Epsilon:  0.01 , Reward -448.6693482493969 , mean_reward:  -665.2693999182826 , time_score:  409 , memory:  300575\n","Episode:  1125  , Epsilon:  0.01 , Reward -236.77190648581993 , mean_reward:  -648.5219279292387 , time_score:  500 , memory:  303075\n","Episode:  1130  , Epsilon:  0.01 , Reward -111.78490460491719 , mean_reward:  -614.8134121563141 , time_score:  500 , memory:  305199\n","Episode:  1135  , Epsilon:  0.01 , Reward -192.90884359728253 , mean_reward:  -579.0827794172218 , time_score:  500 , memory:  307569\n","Episode:  1140  , Epsilon:  0.01 , Reward -172.726814670621 , mean_reward:  -560.8375319319779 , time_score:  500 , memory:  309488\n","Episode:  1145  , Epsilon:  0.01 , Reward -80.5538412199386 , mean_reward:  -537.1114898025843 , time_score:  500 , memory:  311771\n","Episode:  1150  , Epsilon:  0.01 , Reward -151.6862691762606 , mean_reward:  -497.88800028212694 , time_score:  422 , memory:  312995\n","Episode:  1155  , Epsilon:  0.01 , Reward -96.58968035724801 , mean_reward:  -458.44011225405535 , time_score:  73 , memory:  313625\n","Episode:  1160  , Epsilon:  0.01 , Reward -186.44189472781227 , mean_reward:  -425.9558253671582 , time_score:  164 , memory:  314084\n","Episode:  1165  , Epsilon:  0.01 , Reward -85.61146176446914 , mean_reward:  -405.162813550222 , time_score:  466 , memory:  315080\n","Episode:  1170  , Epsilon:  0.01 , Reward -189.50941961761936 , mean_reward:  -386.87547600856 , time_score:  500 , memory:  317149\n","Episode:  1175  , Epsilon:  0.01 , Reward -304.8087970740998 , mean_reward:  -373.115841007122 , time_score:  87 , memory:  318647\n","Episode:  1180  , Epsilon:  0.01 , Reward -636.8545418127078 , mean_reward:  -374.41716390369237 , time_score:  186 , memory:  319230\n","Episode:  1185  , Epsilon:  0.01 , Reward -420.74015266184153 , mean_reward:  -385.2502430172419 , time_score:  106 , memory:  319940\n","Episode:  1190  , Epsilon:  0.01 , Reward -512.8278278934106 , mean_reward:  -378.7745099008363 , time_score:  117 , memory:  320425\n","Episode:  1195  , Epsilon:  0.01 , Reward -917.9363144028681 , mean_reward:  -393.3625293765318 , time_score:  104 , memory:  320960\n","Episode:  1200  , Epsilon:  0.01 , Reward -1390.0439667554542 , mean_reward:  -380.54116108308233 , time_score:  135 , memory:  321543\n","Episode:  1205  , Epsilon:  0.01 , Reward -817.0674035985685 , mean_reward:  -364.0945404260617 , time_score:  117 , memory:  322195\n","Episode:  1210  , Epsilon:  0.01 , Reward -798.5592443142749 , mean_reward:  -367.0542696110931 , time_score:  79 , memory:  322813\n","Episode:  1215  , Epsilon:  0.01 , Reward -377.94104270888744 , mean_reward:  -399.90221037768555 , time_score:  91 , memory:  323496\n","Episode:  1220  , Epsilon:  0.01 , Reward -1044.227596060548 , mean_reward:  -438.3571238162526 , time_score:  129 , memory:  324287\n","Episode:  1225  , Epsilon:  0.01 , Reward -417.06223603264607 , mean_reward:  -461.26477463089014 , time_score:  88 , memory:  324784\n","Episode:  1230  , Epsilon:  0.01 , Reward -368.5980641377524 , mean_reward:  -478.2603809441225 , time_score:  127 , memory:  325364\n","Episode:  1235  , Epsilon:  0.01 , Reward -171.03013393167748 , mean_reward:  -485.31491983129854 , time_score:  97 , memory:  325788\n","Episode:  1240  , Epsilon:  0.01 , Reward -610.6355716413826 , mean_reward:  -496.45760272351805 , time_score:  145 , memory:  326289\n","Episode:  1245  , Epsilon:  0.01 , Reward -499.9461963842302 , mean_reward:  -528.1916289269933 , time_score:  112 , memory:  327012\n","Episode:  1250  , Epsilon:  0.01 , Reward -544.4304338430593 , mean_reward:  -550.1503698283825 , time_score:  100 , memory:  327669\n","Episode:  1255  , Epsilon:  0.01 , Reward -1360.8419197226021 , mean_reward:  -588.8718051805945 , time_score:  163 , memory:  328318\n","Episode:  1260  , Epsilon:  0.01 , Reward -125.34351946843304 , mean_reward:  -614.1054315758452 , time_score:  119 , memory:  328969\n","Episode:  1265  , Epsilon:  0.01 , Reward -2740.3214464744 , mean_reward:  -652.6323755505675 , time_score:  242 , memory:  329576\n","Episode:  1270  , Epsilon:  0.01 , Reward -607.1834082462218 , mean_reward:  -673.0527132303779 , time_score:  78 , memory:  330055\n","Episode:  1275  , Epsilon:  0.01 , Reward -491.0798235055998 , mean_reward:  -706.3827756871228 , time_score:  118 , memory:  330812\n","Episode:  1280  , Epsilon:  0.01 , Reward -356.5193114770128 , mean_reward:  -711.465792615941 , time_score:  125 , memory:  331383\n","Episode:  1285  , Epsilon:  0.01 , Reward -782.983589627973 , mean_reward:  -716.2831694105603 , time_score:  148 , memory:  332147\n","Episode:  1290  , Epsilon:  0.01 , Reward -762.4267039987487 , mean_reward:  -740.1983423957224 , time_score:  166 , memory:  332871\n","Episode:  1295  , Epsilon:  0.01 , Reward -1626.8964406721598 , mean_reward:  -748.9789354428341 , time_score:  291 , memory:  333723\n","Episode:  1300  , Epsilon:  0.01 , Reward -328.11637584673235 , mean_reward:  -734.2482913313188 , time_score:  102 , memory:  334253\n","Episode:  1305  , Epsilon:  0.01 , Reward -162.17071560153693 , mean_reward:  -736.528765979385 , time_score:  76 , memory:  335028\n","Episode:  1310  , Epsilon:  0.01 , Reward -337.94231437894666 , mean_reward:  -720.8183312089589 , time_score:  148 , memory:  336055\n","Episode:  1315  , Epsilon:  0.01 , Reward -469.0983878897464 , mean_reward:  -706.8577446595932 , time_score:  61 , memory:  336518\n","Episode:  1320  , Epsilon:  0.01 , Reward -308.99289454142956 , mean_reward:  -703.1338793914584 , time_score:  53 , memory:  337237\n","Episode:  1325  , Epsilon:  0.01 , Reward -413.8213339376616 , mean_reward:  -721.033500148935 , time_score:  263 , memory:  338014\n","Episode:  1330  , Epsilon:  0.01 , Reward -392.3590120692654 , mean_reward:  -719.8855619607169 , time_score:  85 , memory:  338684\n","Episode:  1335  , Epsilon:  0.01 , Reward -436.4422304504809 , mean_reward:  -724.4059680880187 , time_score:  237 , memory:  339384\n","Episode:  1340  , Epsilon:  0.01 , Reward -324.01279876450263 , mean_reward:  -718.8547561452923 , time_score:  60 , memory:  340110\n","Episode:  1345  , Epsilon:  0.01 , Reward -516.3756072776798 , mean_reward:  -713.4727943779097 , time_score:  138 , memory:  340789\n","Episode:  1350  , Epsilon:  0.01 , Reward -811.6918768304458 , mean_reward:  -722.830575663951 , time_score:  187 , memory:  341485\n","Episode:  1355  , Epsilon:  0.01 , Reward -130.50964176895155 , mean_reward:  -703.5287727301107 , time_score:  79 , memory:  342198\n","Episode:  1360  , Epsilon:  0.01 , Reward -776.897897425052 , mean_reward:  -710.1503594335423 , time_score:  138 , memory:  342989\n","Episode:  1365  , Epsilon:  0.01 , Reward -879.3515838777384 , mean_reward:  -723.301720600212 , time_score:  131 , memory:  343776\n","Episode:  1370  , Epsilon:  0.01 , Reward -52.43456120368033 , mean_reward:  -732.6877570837121 , time_score:  165 , memory:  344542\n","Episode:  1375  , Epsilon:  0.01 , Reward -612.5884844814807 , mean_reward:  -714.854583711526 , time_score:  83 , memory:  345038\n","Episode:  1380  , Epsilon:  0.01 , Reward -1005.3436671394032 , mean_reward:  -714.8113267499408 , time_score:  195 , memory:  345766\n","Episode:  1385  , Epsilon:  0.01 , Reward -400.98090362623014 , mean_reward:  -684.897755387855 , time_score:  140 , memory:  346365\n","Episode:  1390  , Epsilon:  0.01 , Reward -901.9649219642413 , mean_reward:  -664.3003600161012 , time_score:  166 , memory:  347318\n","Episode:  1395  , Epsilon:  0.01 , Reward -510.0570344576541 , mean_reward:  -654.9354805060029 , time_score:  100 , memory:  348278\n","Episode:  1400  , Epsilon:  0.01 , Reward -972.0374643647284 , mean_reward:  -663.1114692957495 , time_score:  216 , memory:  349365\n","Episode:  1405  , Epsilon:  0.01 , Reward -819.6612846499784 , mean_reward:  -657.4732564649781 , time_score:  221 , memory:  350364\n","Episode:  1410  , Epsilon:  0.01 , Reward -302.60293686423245 , mean_reward:  -657.7080352389683 , time_score:  224 , memory:  351418\n","Episode:  1415  , Epsilon:  0.01 , Reward -494.2840161084881 , mean_reward:  -644.1586103083064 , time_score:  154 , memory:  352324\n","Episode:  1420  , Epsilon:  0.01 , Reward -928.8040362649363 , mean_reward:  -624.6874889743518 , time_score:  371 , memory:  353390\n","Episode:  1425  , Epsilon:  0.01 , Reward -750.0001489588208 , mean_reward:  -603.1758549214466 , time_score:  85 , memory:  353782\n","Episode:  1430  , Epsilon:  0.01 , Reward -509.3948281540266 , mean_reward:  -601.4287968650963 , time_score:  94 , memory:  354259\n","Episode:  1435  , Epsilon:  0.01 , Reward -583.051944275348 , mean_reward:  -603.3554213979297 , time_score:  113 , memory:  354681\n","Episode:  1440  , Epsilon:  0.01 , Reward -673.7974508070213 , mean_reward:  -622.3060942506189 , time_score:  203 , memory:  355435\n","Episode:  1445  , Epsilon:  0.01 , Reward -442.2756254588127 , mean_reward:  -628.2885301961135 , time_score:  118 , memory:  356139\n","Episode:  1450  , Epsilon:  0.01 , Reward -666.9506820729737 , mean_reward:  -608.5783011233664 , time_score:  105 , memory:  356718\n","Episode:  1455  , Epsilon:  0.01 , Reward -1029.9020203465705 , mean_reward:  -609.9241608439369 , time_score:  304 , memory:  357372\n","Episode:  1460  , Epsilon:  0.01 , Reward -2287.7172688719334 , mean_reward:  -611.2176942196554 , time_score:  200 , memory:  358102\n","Episode:  1465  , Epsilon:  0.01 , Reward -513.2494929808392 , mean_reward:  -593.1753165601039 , time_score:  154 , memory:  359287\n","Episode:  1470  , Epsilon:  0.01 , Reward -1227.2068778054647 , mean_reward:  -592.0161605073358 , time_score:  295 , memory:  360455\n","Episode:  1475  , Epsilon:  0.01 , Reward -969.2888886154682 , mean_reward:  -594.9105427621408 , time_score:  213 , memory:  361047\n","Episode:  1480  , Epsilon:  0.01 , Reward -748.6731655610098 , mean_reward:  -615.5872578421632 , time_score:  179 , memory:  362125\n","Episode:  1485  , Epsilon:  0.01 , Reward -758.743326994806 , mean_reward:  -633.5276998766562 , time_score:  185 , memory:  362955\n","Episode:  1490  , Epsilon:  0.01 , Reward -740.0918874706805 , mean_reward:  -632.277829284183 , time_score:  121 , memory:  363977\n","Episode:  1495  , Epsilon:  0.01 , Reward -709.670497616257 , mean_reward:  -637.9554830673122 , time_score:  94 , memory:  365105\n","Episode:  1500  , Epsilon:  0.01 , Reward -682.8639591636095 , mean_reward:  -632.567730658706 , time_score:  275 , memory:  366972\n","Episode:  1505  , Epsilon:  0.01 , Reward -407.94329107558514 , mean_reward:  -642.9369079696799 , time_score:  154 , memory:  368518\n","Episode:  1510  , Epsilon:  0.01 , Reward -408.28339591755184 , mean_reward:  -639.7853964377751 , time_score:  213 , memory:  369764\n","Episode:  1515  , Epsilon:  0.01 , Reward -214.54061789979076 , mean_reward:  -645.3033245965893 , time_score:  154 , memory:  371319\n","Episode:  1520  , Epsilon:  0.01 , Reward -1449.80414252008 , mean_reward:  -655.5671150416459 , time_score:  372 , memory:  372910\n","Episode:  1525  , Epsilon:  0.01 , Reward -528.8259121875111 , mean_reward:  -661.3164904716259 , time_score:  90 , memory:  373913\n","Episode:  1530  , Epsilon:  0.01 , Reward -1547.531787272851 , mean_reward:  -673.195966885673 , time_score:  183 , memory:  374686\n","Episode:  1535  , Epsilon:  0.01 , Reward -801.3827126773629 , mean_reward:  -671.028671762857 , time_score:  210 , memory:  375516\n","Episode:  1540  , Epsilon:  0.01 , Reward -528.6660554241417 , mean_reward:  -674.1663476513068 , time_score:  254 , memory:  377243\n","Episode:  1545  , Epsilon:  0.01 , Reward -713.5477600758662 , mean_reward:  -662.2568580687697 , time_score:  368 , memory:  378404\n","Episode:  1550  , Epsilon:  0.01 , Reward -698.0040288532268 , mean_reward:  -679.687441547179 , time_score:  129 , memory:  379770\n","Episode:  1555  , Epsilon:  0.01 , Reward -515.0459156438159 , mean_reward:  -682.6401114565206 , time_score:  66 , memory:  380307\n","Episode:  1560  , Epsilon:  0.01 , Reward -636.8154636978434 , mean_reward:  -672.8448861448045 , time_score:  88 , memory:  381039\n","Episode:  1565  , Epsilon:  0.01 , Reward -591.1173541148646 , mean_reward:  -666.724648747828 , time_score:  98 , memory:  382186\n","Episode:  1570  , Epsilon:  0.01 , Reward -247.48437938330025 , mean_reward:  -645.0592349063097 , time_score:  138 , memory:  383526\n","Episode:  1575  , Epsilon:  0.01 , Reward -57.444510877100136 , mean_reward:  -626.2710161360632 , time_score:  274 , memory:  384279\n","Episode:  1580  , Epsilon:  0.01 , Reward -208.42842658980626 , mean_reward:  -585.1077268730461 , time_score:  111 , memory:  384948\n","Episode:  1585  , Epsilon:  0.01 , Reward -1064.0254261303337 , mean_reward:  -579.9065879685842 , time_score:  239 , memory:  385805\n","Episode:  1590  , Epsilon:  0.01 , Reward -1856.0782075603313 , mean_reward:  -624.2963899023594 , time_score:  170 , memory:  386752\n","Episode:  1595  , Epsilon:  0.01 , Reward -1411.025445900079 , mean_reward:  -659.8884281875359 , time_score:  148 , memory:  387679\n","Episode:  1600  , Epsilon:  0.01 , Reward -338.4890590100372 , mean_reward:  -663.0987191770233 , time_score:  245 , memory:  388684\n","Episode:  1605  , Epsilon:  0.01 , Reward -1019.1580117606169 , mean_reward:  -668.3954909603335 , time_score:  108 , memory:  389311\n","Episode:  1610  , Epsilon:  0.01 , Reward -1000.3004310458424 , mean_reward:  -689.8719623998821 , time_score:  97 , memory:  389933\n","Episode:  1615  , Epsilon:  0.01 , Reward -479.7416630706762 , mean_reward:  -706.2891808817092 , time_score:  233 , memory:  390788\n","Episode:  1620  , Epsilon:  0.01 , Reward -571.9611423430197 , mean_reward:  -691.2696946387001 , time_score:  73 , memory:  391271\n","Episode:  1625  , Epsilon:  0.01 , Reward -89.88087897478324 , mean_reward:  -671.1913932714685 , time_score:  121 , memory:  391949\n","Episode:  1630  , Epsilon:  0.01 , Reward -536.8540675539602 , mean_reward:  -656.4923656451408 , time_score:  91 , memory:  392436\n","Episode:  1635  , Epsilon:  0.01 , Reward -574.528923715621 , mean_reward:  -656.4835368361994 , time_score:  91 , memory:  392888\n","Episode:  1640  , Epsilon:  0.01 , Reward -277.91815173768293 , mean_reward:  -635.048654369552 , time_score:  92 , memory:  393369\n","Episode:  1645  , Epsilon:  0.01 , Reward -496.4840003637696 , mean_reward:  -621.3526869019418 , time_score:  97 , memory:  393798\n","Episode:  1650  , Epsilon:  0.01 , Reward -1406.175905867591 , mean_reward:  -618.4765118681582 , time_score:  145 , memory:  394342\n","Episode:  1655  , Epsilon:  0.01 , Reward -609.7144185141619 , mean_reward:  -614.9415351083893 , time_score:  108 , memory:  394894\n","Episode:  1660  , Epsilon:  0.01 , Reward -601.6152296312441 , mean_reward:  -615.5657408837783 , time_score:  83 , memory:  395454\n","Episode:  1665  , Epsilon:  0.01 , Reward -246.9067479199974 , mean_reward:  -610.7070619210514 , time_score:  115 , memory:  396003\n","Episode:  1670  , Epsilon:  0.01 , Reward -685.8696115814689 , mean_reward:  -633.7563407320459 , time_score:  94 , memory:  396635\n","Episode:  1675  , Epsilon:  0.01 , Reward -339.7110209248474 , mean_reward:  -643.2390721674768 , time_score:  85 , memory:  397115\n","Episode:  1680  , Epsilon:  0.01 , Reward -570.2572068298546 , mean_reward:  -661.0018992738406 , time_score:  175 , memory:  397902\n","Episode:  1685  , Epsilon:  0.01 , Reward -682.2977075765766 , mean_reward:  -675.9515274525822 , time_score:  124 , memory:  398877\n","Episode:  1690  , Epsilon:  0.01 , Reward -1004.9337661581495 , mean_reward:  -663.0344088007557 , time_score:  193 , memory:  400057\n","Episode:  1695  , Epsilon:  0.01 , Reward -394.58190771860836 , mean_reward:  -640.4238664734253 , time_score:  141 , memory:  400947\n","Episode:  1700  , Epsilon:  0.01 , Reward -255.77720595524806 , mean_reward:  -653.1103540788375 , time_score:  219 , memory:  402149\n","Episode:  1705  , Epsilon:  0.01 , Reward -221.04224259396662 , mean_reward:  -685.2462000398831 , time_score:  173 , memory:  403518\n","Episode:  1710  , Epsilon:  0.01 , Reward -635.7080623184071 , mean_reward:  -664.3820829376342 , time_score:  129 , memory:  404596\n","Episode:  1715  , Epsilon:  0.01 , Reward -584.1749082665676 , mean_reward:  -652.1438862310227 , time_score:  93 , memory:  405149\n","Episode:  1720  , Epsilon:  0.01 , Reward -368.3779260630725 , mean_reward:  -646.9459894136405 , time_score:  89 , memory:  405812\n","Episode:  1725  , Epsilon:  0.01 , Reward -1606.4844473186383 , mean_reward:  -668.4866468619622 , time_score:  204 , memory:  406466\n","Episode:  1730  , Epsilon:  0.01 , Reward -482.90591534738496 , mean_reward:  -672.6180825936459 , time_score:  71 , memory:  406980\n","Episode:  1735  , Epsilon:  0.01 , Reward -663.5941150515469 , mean_reward:  -681.1445989892214 , time_score:  81 , memory:  407404\n","Episode:  1740  , Epsilon:  0.01 , Reward -358.98978937759864 , mean_reward:  -689.9639756911087 , time_score:  109 , memory:  407820\n","Episode:  1745  , Epsilon:  0.01 , Reward -566.4779588052456 , mean_reward:  -706.9881751936794 , time_score:  79 , memory:  408234\n","Episode:  1750  , Epsilon:  0.01 , Reward -484.1345088605935 , mean_reward:  -695.7479934984639 , time_score:  69 , memory:  408598\n","Episode:  1755  , Epsilon:  0.01 , Reward -373.68451709120177 , mean_reward:  -690.4699530794609 , time_score:  84 , memory:  409145\n","Episode:  1760  , Epsilon:  0.01 , Reward -295.6755404017673 , mean_reward:  -683.5928106621056 , time_score:  86 , memory:  409669\n","Episode:  1765  , Epsilon:  0.01 , Reward -509.11115609822093 , mean_reward:  -678.1980958794675 , time_score:  74 , memory:  410060\n","Episode:  1770  , Epsilon:  0.01 , Reward -597.341944900598 , mean_reward:  -667.6450116145741 , time_score:  65 , memory:  410497\n","Episode:  1775  , Epsilon:  0.01 , Reward -616.3823751073869 , mean_reward:  -673.0944308291321 , time_score:  106 , memory:  410929\n","Episode:  1780  , Epsilon:  0.01 , Reward -620.1257736550762 , mean_reward:  -679.3155048172927 , time_score:  94 , memory:  411475\n","Episode:  1785  , Epsilon:  0.01 , Reward -520.2232472215541 , mean_reward:  -671.1598922851169 , time_score:  98 , memory:  412006\n","Episode:  1790  , Epsilon:  0.01 , Reward -572.1084493449797 , mean_reward:  -667.152457263067 , time_score:  64 , memory:  412627\n","Episode:  1795  , Epsilon:  0.01 , Reward -689.2824205295915 , mean_reward:  -644.0528193560841 , time_score:  77 , memory:  413048\n","Episode:  1800  , Epsilon:  0.01 , Reward -602.8903321552859 , mean_reward:  -641.4357609535876 , time_score:  63 , memory:  413543\n","Episode:  1805  , Epsilon:  0.01 , Reward -700.3256038477431 , mean_reward:  -590.309360842219 , time_score:  126 , memory:  413921\n","Episode:  1810  , Epsilon:  0.01 , Reward -940.0687220007385 , mean_reward:  -598.5747289119743 , time_score:  97 , memory:  414321\n","Episode:  1815  , Epsilon:  0.01 , Reward -448.5652434074761 , mean_reward:  -590.1695740629116 , time_score:  62 , memory:  414626\n","Episode:  1820  , Epsilon:  0.01 , Reward -660.8076884325571 , mean_reward:  -600.06109288935 , time_score:  100 , memory:  415031\n","Episode:  1825  , Epsilon:  0.01 , Reward -362.57471276505805 , mean_reward:  -589.5409201856343 , time_score:  51 , memory:  415373\n","Episode:  1830  , Epsilon:  0.01 , Reward -529.6027397172721 , mean_reward:  -595.1065159763444 , time_score:  80 , memory:  415722\n","Episode:  1835  , Epsilon:  0.01 , Reward -942.4863586687571 , mean_reward:  -598.4907866191423 , time_score:  89 , memory:  416091\n","Episode:  1840  , Epsilon:  0.01 , Reward -468.6810262466288 , mean_reward:  -596.6555344204963 , time_score:  63 , memory:  416399\n","Episode:  1845  , Epsilon:  0.01 , Reward -991.6064113758741 , mean_reward:  -591.2261495002101 , time_score:  89 , memory:  416755\n","Episode:  1850  , Epsilon:  0.01 , Reward -679.8718444842807 , mean_reward:  -601.7293447089647 , time_score:  77 , memory:  417130\n","Episode:  1855  , Epsilon:  0.01 , Reward -347.09770373367064 , mean_reward:  -603.7262067783366 , time_score:  53 , memory:  417423\n","Episode:  1860  , Epsilon:  0.01 , Reward -747.3414980717702 , mean_reward:  -604.0323652686757 , time_score:  72 , memory:  417730\n","Episode:  1865  , Epsilon:  0.01 , Reward -788.0204322739689 , mean_reward:  -606.1497951277463 , time_score:  84 , memory:  418069\n","Episode:  1870  , Epsilon:  0.01 , Reward -357.55323684749516 , mean_reward:  -608.1736958447597 , time_score:  52 , memory:  418436\n","Episode:  1875  , Epsilon:  0.01 , Reward -685.560082879023 , mean_reward:  -612.4987707692652 , time_score:  76 , memory:  418812\n","Episode:  1880  , Epsilon:  0.01 , Reward -802.7074547388086 , mean_reward:  -609.2252728577572 , time_score:  75 , memory:  419125\n","Episode:  1885  , Epsilon:  0.01 , Reward -482.8483894719377 , mean_reward:  -606.4355860970766 , time_score:  63 , memory:  419485\n","Episode:  1890  , Epsilon:  0.01 , Reward -567.2088999886705 , mean_reward:  -579.7389801986872 , time_score:  68 , memory:  419822\n","Episode:  1895  , Epsilon:  0.01 , Reward -389.695413427439 , mean_reward:  -578.7251118416725 , time_score:  62 , memory:  420157\n","Episode:  1900  , Epsilon:  0.01 , Reward -498.224610071372 , mean_reward:  -566.5398944480196 , time_score:  102 , memory:  420539\n","Episode:  1905  , Epsilon:  0.01 , Reward -200.75734236913425 , mean_reward:  -567.953362926472 , time_score:  54 , memory:  420874\n","Episode:  1910  , Epsilon:  0.01 , Reward -342.2383595678275 , mean_reward:  -554.9655074509559 , time_score:  56 , memory:  421178\n","Episode:  1915  , Epsilon:  0.01 , Reward -460.85691076893227 , mean_reward:  -560.4400484914795 , time_score:  62 , memory:  421542\n","Episode:  1920  , Epsilon:  0.01 , Reward -572.6775977483912 , mean_reward:  -554.0686138202474 , time_score:  64 , memory:  421847\n","Episode:  1925  , Epsilon:  0.01 , Reward -315.56857209407247 , mean_reward:  -552.5968284748556 , time_score:  55 , memory:  422168\n","Episode:  1930  , Epsilon:  0.01 , Reward -156.24772768283816 , mean_reward:  -534.1028924721289 , time_score:  79 , memory:  422532\n","Episode:  1935  , Epsilon:  0.01 , Reward -225.39258932457693 , mean_reward:  -510.39163660098404 , time_score:  62 , memory:  422899\n","Episode:  1940  , Epsilon:  0.01 , Reward -171.39177510008582 , mean_reward:  -497.21733450459584 , time_score:  64 , memory:  423285\n","Episode:  1945  , Epsilon:  0.01 , Reward -202.21376301818486 , mean_reward:  -481.16977885768995 , time_score:  64 , memory:  423694\n","Episode:  1950  , Epsilon:  0.01 , Reward -112.21964242818532 , mean_reward:  -458.7963814378984 , time_score:  53 , memory:  424065\n","Episode:  1955  , Epsilon:  0.01 , Reward -269.2983487388158 , mean_reward:  -446.7104358747522 , time_score:  86 , memory:  424467\n","Episode:  1960  , Epsilon:  0.01 , Reward -272.2336394286339 , mean_reward:  -434.0920335966504 , time_score:  72 , memory:  424803\n","Episode:  1965  , Epsilon:  0.01 , Reward -235.30003800228187 , mean_reward:  -419.92977141418555 , time_score:  88 , memory:  425184\n","Episode:  1970  , Epsilon:  0.01 , Reward -232.87643250495978 , mean_reward:  -401.83454196766644 , time_score:  65 , memory:  425502\n","Episode:  1975  , Epsilon:  0.01 , Reward -515.0331528193975 , mean_reward:  -386.1308174657564 , time_score:  65 , memory:  425850\n","Episode:  1980  , Epsilon:  0.01 , Reward -618.2712276392866 , mean_reward:  -379.6196830420959 , time_score:  69 , memory:  426209\n","Episode:  1985  , Epsilon:  0.01 , Reward -374.05558906760064 , mean_reward:  -367.9417338398107 , time_score:  55 , memory:  426556\n","Episode:  1990  , Epsilon:  0.01 , Reward -392.87384478020147 , mean_reward:  -370.0306607968679 , time_score:  56 , memory:  426909\n","Episode:  1995  , Epsilon:  0.01 , Reward -1069.561598511451 , mean_reward:  -374.2194163059763 , time_score:  106 , memory:  427286\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dmu7jobCkZ0S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}