{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"ANN32.csv\"\n",
    "        self.model_filename = \"ANN32.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(16, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(16, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "              with open(self.csv_filename, 'a') as f:\n",
    "                self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "              \n",
    "              self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "\n",
    "              self.Q_model.save(self.model_filename)\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "                self.ep *= self.ep_decay\n",
    "            else:\n",
    "                self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return R, R_moving\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -117.01417660525377 , mean_reward:  -117.01417660525377 , time_score:  76 , memory:  76\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -115.38391599678269 , mean_reward:  -154.5254724286229 , time_score:  70 , memory:  434\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -125.22801503324241 , mean_reward:  -148.93702399798337 , time_score:  97 , memory:  909\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -167.80405175998357 , mean_reward:  -152.11681590695628 , time_score:  119 , memory:  1395\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -118.16255451732071 , mean_reward:  -151.5251498788259 , time_score:  110 , memory:  1920\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -247.35437020828948 , mean_reward:  -165.77150226518955 , time_score:  131 , memory:  2541\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -424.69755186206874 , mean_reward:  -176.2334058327467 , time_score:  124 , memory:  3098\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -256.074825307951 , mean_reward:  -182.64084455703198 , time_score:  64 , memory:  3541\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -220.82988807028374 , mean_reward:  -186.36641080405636 , time_score:  112 , memory:  4008\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -206.12977945813347 , mean_reward:  -180.47324170244664 , time_score:  112 , memory:  4552\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward -138.94291893809594 , mean_reward:  -179.35788918391503 , time_score:  99 , memory:  5123\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -204.38420024892764 , mean_reward:  -179.86509976938603 , time_score:  83 , memory:  5691\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -132.69409256248136 , mean_reward:  -178.8273845396084 , time_score:  94 , memory:  6170\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -207.27197508842528 , mean_reward:  -177.94101696963799 , time_score:  104 , memory:  6740\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -113.23558261315387 , mean_reward:  -175.7697580959566 , time_score:  79 , memory:  7247\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -213.05383812353358 , mean_reward:  -181.6660954034271 , time_score:  186 , memory:  7851\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward -117.88179999625319 , mean_reward:  -175.63440957910555 , time_score:  62 , memory:  8509\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -36.25053841201119 , mean_reward:  -171.23992285782475 , time_score:  83 , memory:  9289\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -224.2390833290432 , mean_reward:  -172.40280242058085 , time_score:  85 , memory:  9875\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -87.83984633222977 , mean_reward:  -169.79750798788714 , time_score:  279 , memory:  10608\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -216.0022394908876 , mean_reward:  -171.4506160405017 , time_score:  109 , memory:  11254\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -333.37830953777507 , mean_reward:  -175.4155013498925 , time_score:  141 , memory:  12055\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -153.53841678716896 , mean_reward:  -179.39531691211232 , time_score:  100 , memory:  12813\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -223.1221370640476 , mean_reward:  -178.60069667272776 , time_score:  145 , memory:  14001\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -136.47343108077047 , mean_reward:  -178.77431034047274 , time_score:  120 , memory:  14905\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -68.97363516246276 , mean_reward:  -175.06788568013144 , time_score:  138 , memory:  15736\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -284.34799959690827 , mean_reward:  -174.62187536136364 , time_score:  225 , memory:  16568\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -156.44937781342134 , mean_reward:  -172.27053370520613 , time_score:  108 , memory:  17250\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward -199.16082951133055 , mean_reward:  -167.58467677522134 , time_score:  187 , memory:  18084\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward -364.5689977399073 , mean_reward:  -171.9656559813494 , time_score:  225 , memory:  19179\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -324.62888588515875 , mean_reward:  -176.74170787951599 , time_score:  101 , memory:  20224\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -210.20906925806923 , mean_reward:  -175.43556620516208 , time_score:  242 , memory:  21442\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -88.17496929265599 , mean_reward:  -174.24649045799237 , time_score:  133 , memory:  22395\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward -166.11040900375457 , mean_reward:  -179.93801837610184 , time_score:  491 , memory:  24198\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -239.05703671149155 , mean_reward:  -183.84178262179165 , time_score:  188 , memory:  25437\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward -198.54523835480205 , mean_reward:  -178.85013384002247 , time_score:  357 , memory:  26641\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward -133.85970579966204 , mean_reward:  -184.25582900291545 , time_score:  104 , memory:  27952\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -368.4412101714365 , mean_reward:  -191.42825678376485 , time_score:  458 , memory:  29605\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward -97.93945142864393 , mean_reward:  -190.348356027337 , time_score:  144 , memory:  31358\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -69.2279165200699 , mean_reward:  -190.89382462695193 , time_score:  500 , memory:  33115\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -9.700871028466432 , mean_reward:  -184.1453867250736 , time_score:  500 , memory:  34962\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward -104.99515229313398 , mean_reward:  -180.00053135472822 , time_score:  500 , memory:  37004\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -127.38027679636001 , mean_reward:  -176.98139330800552 , time_score:  248 , memory:  38774\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward -24.310578791489053 , mean_reward:  -172.98040741129557 , time_score:  500 , memory:  41080\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward -14.512503504734184 , mean_reward:  -175.57949851840877 , time_score:  500 , memory:  42976\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward 8.6341599693856 , mean_reward:  -173.39086018168203 , time_score:  500 , memory:  45177\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward 1.2394020705784354 , mean_reward:  -164.9420856069708 , time_score:  500 , memory:  47546\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward -21.414209737544837 , mean_reward:  -156.74121893583597 , time_score:  500 , memory:  49925\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward -28.184374488942805 , mean_reward:  -152.00498302523155 , time_score:  500 , memory:  52368\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward -25.59358455331119 , mean_reward:  -141.18261368540294 , time_score:  500 , memory:  54868\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward -72.73293170788435 , mean_reward:  -130.0102997527893 , time_score:  500 , memory:  57368\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward -14.206305457829231 , mean_reward:  -129.56640948911735 , time_score:  363 , memory:  59213\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward -20.752001098484943 , mean_reward:  -122.86570484735462 , time_score:  500 , memory:  61713\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward 21.583460386374885 , mean_reward:  -108.85303541865727 , time_score:  500 , memory:  64213\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward 10.242876328828398 , mean_reward:  -96.79416428154116 , time_score:  500 , memory:  66713\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward -5.821680937892421 , mean_reward:  -87.74558017953593 , time_score:  500 , memory:  69213\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward -31.8508265999854 , mean_reward:  -77.61612858495283 , time_score:  500 , memory:  71713\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward -61.11310967740897 , mean_reward:  -68.10592236750155 , time_score:  500 , memory:  73860\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward -6.286577370315297 , mean_reward:  -59.90257072488159 , time_score:  500 , memory:  76360\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward 59.42318445337557 , mean_reward:  -50.50297187544501 , time_score:  500 , memory:  78860\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward -9.607639119539737 , mean_reward:  -47.11593779346644 , time_score:  500 , memory:  81360\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward 73.09946429451871 , mean_reward:  -37.792140168052676 , time_score:  500 , memory:  83860\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward -25.49821122417628 , mean_reward:  -29.024935279670316 , time_score:  500 , memory:  86360\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward 54.22308456966363 , mean_reward:  -24.899450506675276 , time_score:  500 , memory:  88860\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward 35.16884379456195 , mean_reward:  -13.544388464998601 , time_score:  500 , memory:  91360\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward -8.772341868961428 , mean_reward:  -6.525548645848885 , time_score:  500 , memory:  93860\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward -17.784568006193325 , mean_reward:  -5.08160789639308 , time_score:  500 , memory:  96360\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward -30.05092872483114 , mean_reward:  -4.859725980369702 , time_score:  500 , memory:  98860\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward 12.578674742759313 , mean_reward:  -3.3420961500410193 , time_score:  500 , memory:  101360\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward -36.25482273381864 , mean_reward:  -3.796411306210885 , time_score:  500 , memory:  103860\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward 21.741318941714397 , mean_reward:  -1.4268943997517056 , time_score:  500 , memory:  106360\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward 17.396215961332906 , mean_reward:  6.8245569938839505 , time_score:  500 , memory:  108860\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward -4.021415880105025 , mean_reward:  7.518982921021433 , time_score:  500 , memory:  111360\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward -26.93953680492067 , mean_reward:  7.489924441994491 , time_score:  500 , memory:  113860\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward 45.712450528900575 , mean_reward:  8.189069818756332 , time_score:  500 , memory:  116360\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward 26.434607320063776 , mean_reward:  8.021149546731863 , time_score:  500 , memory:  118860\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward 39.330760614248014 , mean_reward:  7.499799130943204 , time_score:  500 , memory:  121360\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward 2.4509200603859713 , mean_reward:  10.910132788820064 , time_score:  500 , memory:  123860\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward 3.2975938750728417 , mean_reward:  10.970769205387512 , time_score:  500 , memory:  126360\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward 1.5866432519051001 , mean_reward:  9.048941543126398 , time_score:  500 , memory:  128860\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward -40.215100461295336 , mean_reward:  9.476128685466898 , time_score:  500 , memory:  131360\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward 35.25056437992576 , mean_reward:  7.192624161214927 , time_score:  500 , memory:  133860\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward 8.840981636090532 , mean_reward:  7.304720268484741 , time_score:  500 , memory:  136360\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward 58.07724248911059 , mean_reward:  6.322407728856128 , time_score:  500 , memory:  138860\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward 33.80327135985475 , mean_reward:  5.411195410642115 , time_score:  500 , memory:  141360\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward -33.81070379818492 , mean_reward:  3.8999902822067316 , time_score:  500 , memory:  143860\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward 41.20166952729098 , mean_reward:  5.727128654098951 , time_score:  500 , memory:  146360\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward 6.093825131857125 , mean_reward:  6.154960645077579 , time_score:  500 , memory:  148860\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward -4.760461590297556 , mean_reward:  6.312326449262144 , time_score:  500 , memory:  151360\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward -12.590464900228053 , mean_reward:  7.624768001536188 , time_score:  500 , memory:  153860\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward 32.26512600843065 , mean_reward:  8.105635118076124 , time_score:  500 , memory:  156360\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward 21.873446897183115 , mean_reward:  8.56464578060454 , time_score:  500 , memory:  158860\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward 51.2872143468544 , mean_reward:  10.17980678110137 , time_score:  500 , memory:  161360\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward -23.36035023543698 , mean_reward:  10.67872949069409 , time_score:  500 , memory:  163860\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward 36.461515296030015 , mean_reward:  9.224515926695716 , time_score:  500 , memory:  166360\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward -5.286853744263966 , mean_reward:  8.497746714876767 , time_score:  500 , memory:  168860\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward 2.335880229375264 , mean_reward:  9.833656058384019 , time_score:  500 , memory:  171360\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward 57.236098580981476 , mean_reward:  10.66131517399258 , time_score:  500 , memory:  173860\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward -5.842354216071275 , mean_reward:  11.991294387180751 , time_score:  500 , memory:  176360\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward -1.8929448639044832 , mean_reward:  11.374769709434513 , time_score:  500 , memory:  178860\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward 48.04262238586762 , mean_reward:  7.590092243949622 , time_score:  500 , memory:  181217\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward -1.5332125350461077 , mean_reward:  8.647121388183972 , time_score:  500 , memory:  183717\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward -30.48867419335491 , mean_reward:  7.6228505027830655 , time_score:  500 , memory:  186217\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward -21.178791166071647 , mean_reward:  7.210936328275283 , time_score:  500 , memory:  188717\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward -39.433838803180464 , mean_reward:  8.078939181804614 , time_score:  500 , memory:  191111\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward -9.16009817107281 , mean_reward:  7.809594708068078 , time_score:  500 , memory:  193611\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward 57.61272855076426 , mean_reward:  8.2092887476525 , time_score:  500 , memory:  196111\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward -10.320620929932188 , mean_reward:  7.721520929830142 , time_score:  500 , memory:  198611\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward -21.94622867430661 , mean_reward:  7.621711289245634 , time_score:  500 , memory:  200951\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward 96.18530054603323 , mean_reward:  9.889608941489119 , time_score:  500 , memory:  203451\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward 19.954147126593007 , mean_reward:  9.36320027976861 , time_score:  500 , memory:  205951\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward -43.26119359146121 , mean_reward:  7.763954609347811 , time_score:  500 , memory:  208451\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward 4.241800523937981 , mean_reward:  6.57861940893291 , time_score:  500 , memory:  210951\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward 24.18023204481616 , mean_reward:  7.077056818683599 , time_score:  500 , memory:  213451\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward 53.87222185388431 , mean_reward:  8.654228226776398 , time_score:  500 , memory:  215951\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward 54.119551715708255 , mean_reward:  9.968994091345005 , time_score:  500 , memory:  218451\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward 15.597624806951233 , mean_reward:  9.400861170158349 , time_score:  500 , memory:  220951\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward 49.35802520415217 , mean_reward:  9.414697974401047 , time_score:  500 , memory:  223451\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward 43.10502800226523 , mean_reward:  9.53458045304105 , time_score:  500 , memory:  225951\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward 20.691690795407908 , mean_reward:  10.313113964578802 , time_score:  500 , memory:  228451\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward 41.805892781497846 , mean_reward:  14.337975845585806 , time_score:  500 , memory:  230951\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward 23.77861332360607 , mean_reward:  15.477589059231184 , time_score:  500 , memory:  233451\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward 31.55000683648425 , mean_reward:  16.3906774132679 , time_score:  500 , memory:  235951\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward 29.11853230366936 , mean_reward:  17.94768613130104 , time_score:  500 , memory:  238451\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward -10.363701115959778 , mean_reward:  18.172671271167356 , time_score:  500 , memory:  240951\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward 11.549927582486136 , mean_reward:  19.57657099792622 , time_score:  500 , memory:  243451\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward -4.920262302065179 , mean_reward:  19.052839341157018 , time_score:  500 , memory:  245951\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward -34.43853012916753 , mean_reward:  19.666755921101927 , time_score:  500 , memory:  248451\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward 10.602626836807652 , mean_reward:  19.338840573199015 , time_score:  500 , memory:  250951\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward 46.48079132684661 , mean_reward:  18.065370072071502 , time_score:  500 , memory:  253451\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward 81.41469600012735 , mean_reward:  19.12049901020852 , time_score:  500 , memory:  255951\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward 47.92341289005269 , mean_reward:  20.309510268927035 , time_score:  500 , memory:  258451\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 28.020708591732728 , mean_reward:  21.524772183404274 , time_score:  500 , memory:  260951\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward 80.04173826384385 , mean_reward:  22.061425748942202 , time_score:  500 , memory:  263451\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward 24.8623875630807 , mean_reward:  21.18644220674379 , time_score:  500 , memory:  265951\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward 0.09902667401674581 , mean_reward:  22.00886158094959 , time_score:  500 , memory:  268451\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward 83.50834916146212 , mean_reward:  22.931871000962577 , time_score:  500 , memory:  270951\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward -19.324328001958886 , mean_reward:  23.447672651796005 , time_score:  500 , memory:  273451\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward 69.09793299731872 , mean_reward:  23.633037794160725 , time_score:  500 , memory:  275951\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward -5.957835085981756 , mean_reward:  22.792797563051426 , time_score:  455 , memory:  278406\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward 47.951899706925595 , mean_reward:  23.67764141743318 , time_score:  500 , memory:  280906\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward 58.31978057252038 , mean_reward:  23.529877980140256 , time_score:  500 , memory:  283406\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward -442.70347604131325 , mean_reward:  20.028347564601994 , time_score:  500 , memory:  285906\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 110.30093953125154 , mean_reward:  19.918578782223193 , time_score:  500 , memory:  288406\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward 36.5825980455331 , mean_reward:  21.443675742576517 , time_score:  500 , memory:  290906\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 56.515569990709686 , mean_reward:  20.168431739630204 , time_score:  500 , memory:  293406\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward 68.22626484749674 , mean_reward:  22.640490602848644 , time_score:  500 , memory:  295906\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward -291.55025673352225 , mean_reward:  20.777319293480232 , time_score:  500 , memory:  298406\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 64.5168359401539 , mean_reward:  22.293561855678135 , time_score:  500 , memory:  300906\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward -41.888295963783804 , mean_reward:  21.0264598087019 , time_score:  500 , memory:  303406\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward 2.675308059687297 , mean_reward:  22.465174871968465 , time_score:  500 , memory:  305906\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward 88.97857476368497 , mean_reward:  22.794431602642913 , time_score:  500 , memory:  308406\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward -26.10306138343985 , mean_reward:  22.320194012060533 , time_score:  500 , memory:  310906\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward -63.21051259553481 , mean_reward:  22.368013342127835 , time_score:  500 , memory:  313406\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward 102.29746315496111 , mean_reward:  22.90341984715141 , time_score:  500 , memory:  315906\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward 44.17627454849139 , mean_reward:  19.62401520630496 , time_score:  500 , memory:  318406\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward 46.09278231643432 , mean_reward:  19.791517610963357 , time_score:  500 , memory:  320906\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward 63.33915537104021 , mean_reward:  18.077282148282844 , time_score:  500 , memory:  323406\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward -50.644246981927324 , mean_reward:  13.658870557873998 , time_score:  500 , memory:  325906\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 86.56873095359444 , mean_reward:  17.91966884927527 , time_score:  500 , memory:  328406\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward 39.82813070970709 , mean_reward:  16.73092725283748 , time_score:  500 , memory:  330906\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward -21.773500086766227 , mean_reward:  16.738892168549906 , time_score:  500 , memory:  333406\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward 71.7113840612176 , mean_reward:  20.89797160606935 , time_score:  500 , memory:  335906\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward 52.86395618836722 , mean_reward:  24.492654240987118 , time_score:  500 , memory:  338406\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward 112.90237050651997 , mean_reward:  25.16717507524155 , time_score:  500 , memory:  340906\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward -38.984188422078866 , mean_reward:  26.436165244737097 , time_score:  500 , memory:  343406\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward 1.2143712246175251 , mean_reward:  24.504945074465937 , time_score:  500 , memory:  345906\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward -5.992189044113202 , mean_reward:  26.94296489467947 , time_score:  500 , memory:  348309\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward 97.77521182951708 , mean_reward:  24.743606471964203 , time_score:  500 , memory:  350674\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward 73.24230212063216 , mean_reward:  27.61008630427997 , time_score:  500 , memory:  353174\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward 89.63167479573598 , mean_reward:  26.324177726587 , time_score:  500 , memory:  355674\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward 110.03262799661552 , mean_reward:  27.22587004183004 , time_score:  500 , memory:  358144\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward 82.83624973379827 , mean_reward:  25.781787747132782 , time_score:  500 , memory:  360644\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward 124.561543703044 , mean_reward:  26.57475611021838 , time_score:  500 , memory:  363144\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward -157.8355279002131 , mean_reward:  22.44592543129298 , time_score:  257 , memory:  365185\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward -50.57624768173661 , mean_reward:  17.62100645652936 , time_score:  500 , memory:  367387\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward -146.4458872628465 , mean_reward:  12.826893830461017 , time_score:  352 , memory:  369431\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward -187.97893857581124 , mean_reward:  9.095399314744498 , time_score:  488 , memory:  371831\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward -186.25115996354918 , mean_reward:  9.31138433597231 , time_score:  442 , memory:  373967\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward 29.4088082927508 , mean_reward:  5.535086501267548 , time_score:  500 , memory:  376301\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward -44.478107744490245 , mean_reward:  1.226475140568124 , time_score:  500 , memory:  378520\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward 31.792556157295834 , mean_reward:  -0.6087873581102948 , time_score:  500 , memory:  380962\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward -137.32808825062753 , mean_reward:  -7.5936968286670545 , time_score:  280 , memory:  382764\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward -47.10098820369403 , mean_reward:  -16.76589590920833 , time_score:  500 , memory:  385096\n",
      "Episode:  920  , Epsilon:  0.01 , Reward -22.569269852532752 , mean_reward:  -18.53932457248058 , time_score:  500 , memory:  387596\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 84.23419096207618 , mean_reward:  -17.252160156728994 , time_score:  500 , memory:  390096\n",
      "Episode:  930  , Epsilon:  0.01 , Reward 88.14375582503027 , mean_reward:  -18.234023816624386 , time_score:  500 , memory:  392521\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 7.6148828783011835 , mean_reward:  -22.077169852747197 , time_score:  500 , memory:  394787\n",
      "Episode:  940  , Epsilon:  0.01 , Reward -49.59606500026279 , mean_reward:  -24.955487760168364 , time_score:  500 , memory:  397115\n",
      "Episode:  945  , Epsilon:  0.01 , Reward -30.882301418392224 , mean_reward:  -26.72106985544107 , time_score:  500 , memory:  399536\n",
      "Episode:  950  , Epsilon:  0.01 , Reward -72.59369548652785 , mean_reward:  -32.35543485411811 , time_score:  500 , memory:  401927\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 8.457095412678232 , mean_reward:  -34.87533817276675 , time_score:  500 , memory:  404327\n",
      "Episode:  960  , Epsilon:  0.01 , Reward -33.768855201681674 , mean_reward:  -34.98501068050764 , time_score:  500 , memory:  406827\n",
      "Episode:  965  , Epsilon:  0.01 , Reward -195.63410601304278 , mean_reward:  -43.85980764780047 , time_score:  377 , memory:  409085\n",
      "Episode:  970  , Epsilon:  0.01 , Reward -58.93138710647339 , mean_reward:  -44.72698382304886 , time_score:  500 , memory:  411475\n",
      "Episode:  975  , Epsilon:  0.01 , Reward -151.50575761329722 , mean_reward:  -45.12210171313254 , time_score:  337 , memory:  413599\n",
      "Episode:  980  , Epsilon:  0.01 , Reward -167.2182616136571 , mean_reward:  -49.91029572392515 , time_score:  397 , memory:  415646\n",
      "Episode:  985  , Epsilon:  0.01 , Reward -141.76921043495597 , mean_reward:  -50.801231335432014 , time_score:  360 , memory:  417768\n",
      "Episode:  990  , Epsilon:  0.01 , Reward -44.76324358059756 , mean_reward:  -52.98425092969504 , time_score:  500 , memory:  419893\n",
      "Episode:  995  , Epsilon:  0.01 , Reward -148.63805771735045 , mean_reward:  -56.40582918270388 , time_score:  408 , memory:  422246\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward -164.76537180047 , mean_reward:  -58.4780892041532 , time_score:  467 , memory:  424585\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward -285.2029407939912 , mean_reward:  -66.2681169571076 , time_score:  492 , memory:  426620\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward -50.35124059642261 , mean_reward:  -63.06155764917899 , time_score:  500 , memory:  429120\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward -120.93872624163556 , mean_reward:  -65.83793114932294 , time_score:  330 , memory:  431137\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward -114.44790183020297 , mean_reward:  -72.4135302412318 , time_score:  354 , memory:  433222\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward -146.8690894416051 , mean_reward:  -80.51746305378367 , time_score:  440 , memory:  435164\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward -133.82455217877597 , mean_reward:  -88.07352048106462 , time_score:  377 , memory:  436842\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward -8.606976191740564 , mean_reward:  -89.24546928726488 , time_score:  500 , memory:  439106\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward -133.92654578454142 , mean_reward:  -92.97235147133968 , time_score:  201 , memory:  440633\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward -120.67429597689244 , mean_reward:  -100.96906664377693 , time_score:  330 , memory:  442209\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward -128.7946541336538 , mean_reward:  -103.74022527480648 , time_score:  245 , memory:  443625\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward -131.33730477998347 , mean_reward:  -110.50256757818508 , time_score:  373 , memory:  445103\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward -189.70200277525214 , mean_reward:  -117.95205611090395 , time_score:  328 , memory:  446735\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward -36.667169103750695 , mean_reward:  -114.71140074110606 , time_score:  500 , memory:  449046\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward -111.24570243367326 , mean_reward:  -114.11246592200042 , time_score:  411 , memory:  451293\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward -141.33137684438378 , mean_reward:  -112.69873694729776 , time_score:  341 , memory:  453363\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward -12.495666110229527 , mean_reward:  -109.99858965970681 , time_score:  500 , memory:  455429\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward -38.659212969688305 , mean_reward:  -109.10971880397868 , time_score:  500 , memory:  457890\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward -19.223505922106284 , mean_reward:  -107.54849319980985 , time_score:  500 , memory:  460103\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward -142.01161762829474 , mean_reward:  -111.74622303349139 , time_score:  373 , memory:  461925\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward -166.52099441283602 , mean_reward:  -114.10218620027176 , time_score:  244 , memory:  463415\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward -128.8967791046299 , mean_reward:  -112.6767383617988 , time_score:  401 , memory:  464977\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward -139.3509946179998 , mean_reward:  -115.11584735425444 , time_score:  374 , memory:  467011\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 8.80884086006475 , mean_reward:  -111.89746624560263 , time_score:  500 , memory:  468850\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward -20.34481763226998 , mean_reward:  -111.87937205512583 , time_score:  500 , memory:  471072\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward -138.49683361556194 , mean_reward:  -110.9714698992995 , time_score:  332 , memory:  473022\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward -205.08676238915535 , mean_reward:  -109.48416924725102 , time_score:  446 , memory:  474934\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward -40.56644129741028 , mean_reward:  -106.92207944848427 , time_score:  500 , memory:  477434\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward -47.502356940212664 , mean_reward:  -100.56817295897773 , time_score:  500 , memory:  479934\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward -38.98498309673796 , mean_reward:  -96.9037879528722 , time_score:  500 , memory:  482434\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward -23.378625862129848 , mean_reward:  -90.85237957685007 , time_score:  500 , memory:  484934\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward -24.000219834436155 , mean_reward:  -84.22406373711664 , time_score:  500 , memory:  487434\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward -52.42912805472563 , mean_reward:  -78.53945506973935 , time_score:  500 , memory:  489934\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward -27.962778546785316 , mean_reward:  -77.3540996091406 , time_score:  500 , memory:  492434\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward -39.84923713749395 , mean_reward:  -76.01877520316276 , time_score:  500 , memory:  494934\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward -33.831081514458425 , mean_reward:  -72.32987555852705 , time_score:  500 , memory:  497434\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward -72.22536821980464 , mean_reward:  -69.40489915155334 , time_score:  500 , memory:  499934\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward -4.174465867903929 , mean_reward:  -67.23500709623133 , time_score:  500 , memory:  502434\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward -68.36048441807988 , mean_reward:  -64.58440524957797 , time_score:  500 , memory:  504934\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward -34.168922007639566 , mean_reward:  -58.62190301883481 , time_score:  500 , memory:  507434\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward -69.44116659733646 , mean_reward:  -52.66812014482632 , time_score:  500 , memory:  509934\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward -42.793147845847 , mean_reward:  -47.11987878435016 , time_score:  500 , memory:  512434\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward -21.48154750278687 , mean_reward:  -42.98648055070102 , time_score:  500 , memory:  514934\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward 6.6423565178939805 , mean_reward:  -40.554014826306876 , time_score:  500 , memory:  517434\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward -50.35922453027757 , mean_reward:  -37.27151017827166 , time_score:  500 , memory:  519934\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward -27.606794959058217 , mean_reward:  -34.28495228755677 , time_score:  500 , memory:  522434\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward -35.405854294317834 , mean_reward:  -30.200131329377573 , time_score:  500 , memory:  524934\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward -18.31001114044778 , mean_reward:  -28.76882042226775 , time_score:  500 , memory:  527434\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward -24.960135152729585 , mean_reward:  -28.86434698922395 , time_score:  500 , memory:  529934\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 7.750242038191899 , mean_reward:  -25.75987474164391 , time_score:  500 , memory:  532434\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 19.75786722234528 , mean_reward:  -24.49941565265187 , time_score:  500 , memory:  534934\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward 28.407744542752845 , mean_reward:  -24.455930834743555 , time_score:  500 , memory:  537434\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward -24.80423190393229 , mean_reward:  -23.129176984364886 , time_score:  500 , memory:  539934\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 19.006867713846482 , mean_reward:  -20.794444320645738 , time_score:  500 , memory:  542434\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward -29.15808219628017 , mean_reward:  -21.1343435747427 , time_score:  500 , memory:  544934\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward -11.74553686403757 , mean_reward:  -19.828284303319958 , time_score:  500 , memory:  547434\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward -21.009873036301737 , mean_reward:  -18.309458986658175 , time_score:  500 , memory:  549934\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward -70.43372808607135 , mean_reward:  -18.26280778778728 , time_score:  500 , memory:  552434\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward -14.38199099098882 , mean_reward:  -17.918559935893192 , time_score:  500 , memory:  554934\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward 2.8011883529212285 , mean_reward:  -17.06611016445747 , time_score:  500 , memory:  557434\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward -53.59154752611029 , mean_reward:  -15.797744435910486 , time_score:  500 , memory:  559934\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward 18.912964635516374 , mean_reward:  -15.064400817788155 , time_score:  500 , memory:  562434\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward 23.533580713372206 , mean_reward:  -13.981045850934532 , time_score:  500 , memory:  564934\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward -53.38880899194035 , mean_reward:  -13.213547550666547 , time_score:  500 , memory:  567434\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 4.1548347009239635 , mean_reward:  -11.350139963984637 , time_score:  500 , memory:  569934\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward -22.654295482733744 , mean_reward:  -9.89927345138657 , time_score:  500 , memory:  572434\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward -50.893866588379076 , mean_reward:  -8.247987186308727 , time_score:  500 , memory:  574934\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward 0.0487624348372373 , mean_reward:  -8.468449533314192 , time_score:  500 , memory:  577434\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward 41.40812851613365 , mean_reward:  -7.2982801615018005 , time_score:  500 , memory:  579934\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward 23.312470009770923 , mean_reward:  -7.29148145217902 , time_score:  500 , memory:  582434\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward -14.964731112714361 , mean_reward:  -7.939906734562454 , time_score:  500 , memory:  584934\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward 14.73323629470252 , mean_reward:  -7.045079456158208 , time_score:  500 , memory:  587434\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward 14.21575494278232 , mean_reward:  -6.403824939547027 , time_score:  500 , memory:  589934\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward -7.239292750304796 , mean_reward:  -8.974803503484193 , time_score:  500 , memory:  592434\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward -8.924698916284187 , mean_reward:  -7.939352680101772 , time_score:  500 , memory:  594934\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward -32.801258803343714 , mean_reward:  -9.096329330494694 , time_score:  500 , memory:  597434\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward -123.52400106993146 , mean_reward:  -10.085402920232697 , time_score:  500 , memory:  599934\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward -21.41028601422072 , mean_reward:  -10.63158215998645 , time_score:  500 , memory:  602434\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward -43.762273318013506 , mean_reward:  -10.874741676681426 , time_score:  500 , memory:  604934\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward -4.380961249247389 , mean_reward:  -12.330976701614322 , time_score:  500 , memory:  607434\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward -2.5119460448338304 , mean_reward:  -12.022460742585844 , time_score:  500 , memory:  609934\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward 1.6224832909027194 , mean_reward:  -11.494044630938506 , time_score:  500 , memory:  612434\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward -4.123076342921309 , mean_reward:  -11.45065621970645 , time_score:  500 , memory:  614934\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward 16.139658888913846 , mean_reward:  -11.906511887524072 , time_score:  500 , memory:  617434\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward -0.3089829407814402 , mean_reward:  -12.561372405610536 , time_score:  500 , memory:  619934\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward 18.319559231338957 , mean_reward:  -11.243891743601875 , time_score:  500 , memory:  622434\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward 0.914630617978045 , mean_reward:  -11.975967184412669 , time_score:  500 , memory:  624934\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward 20.927883322144186 , mean_reward:  -11.976051998784806 , time_score:  500 , memory:  627434\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward -34.7015221195486 , mean_reward:  -12.612009025606232 , time_score:  500 , memory:  629934\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward -20.60951280763759 , mean_reward:  -14.091704569211565 , time_score:  500 , memory:  632434\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward 26.941258222356904 , mean_reward:  -13.997794098083418 , time_score:  500 , memory:  634934\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward -20.222185540320538 , mean_reward:  -14.921777476648376 , time_score:  500 , memory:  637434\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward -23.39400511980132 , mean_reward:  -15.88771317436982 , time_score:  500 , memory:  639934\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward 1.8442728168953717 , mean_reward:  -13.927624685570631 , time_score:  500 , memory:  642434\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward -44.20031841353368 , mean_reward:  -13.186276396859903 , time_score:  500 , memory:  644934\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward 5.584703422373792 , mean_reward:  -12.588618309840989 , time_score:  500 , memory:  647434\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward 20.19061682972935 , mean_reward:  -11.48956762535076 , time_score:  500 , memory:  649934\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward 46.106726843131256 , mean_reward:  -9.882963537869347 , time_score:  500 , memory:  652434\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward 7.984878634128464 , mean_reward:  -10.25123480290341 , time_score:  500 , memory:  654934\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward 2.7804653078841266 , mean_reward:  -10.248984414673455 , time_score:  500 , memory:  657434\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward -42.16492882831441 , mean_reward:  -10.349778074399305 , time_score:  500 , memory:  659934\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward -33.185491662165546 , mean_reward:  -10.555437371407027 , time_score:  500 , memory:  662434\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward 22.98206281551225 , mean_reward:  -11.497864584478918 , time_score:  500 , memory:  664934\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward -5.150010055841742 , mean_reward:  -10.408644511276284 , time_score:  500 , memory:  667434\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward 8.114065790332017 , mean_reward:  -9.643398604187285 , time_score:  500 , memory:  669934\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward -21.62035054920611 , mean_reward:  -11.787055746514806 , time_score:  500 , memory:  672434\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward 12.828464820742884 , mean_reward:  -10.114657328764004 , time_score:  500 , memory:  674934\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward -22.230022439205033 , mean_reward:  -9.726654089964974 , time_score:  500 , memory:  677434\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward 18.219851451256282 , mean_reward:  -8.678104657716803 , time_score:  500 , memory:  679934\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward 10.942330416274269 , mean_reward:  -8.346926611853219 , time_score:  500 , memory:  682434\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward 7.456655005216767 , mean_reward:  -7.455616602168217 , time_score:  500 , memory:  684934\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward -19.980543434435553 , mean_reward:  -6.453202941210626 , time_score:  500 , memory:  687434\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward -13.065130105247679 , mean_reward:  -5.511196801944189 , time_score:  500 , memory:  689934\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward 39.74535057829739 , mean_reward:  -5.301726132927164 , time_score:  500 , memory:  692434\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward 6.000423013101868 , mean_reward:  -4.829275007613648 , time_score:  500 , memory:  694934\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward -25.94627530031063 , mean_reward:  -4.038387616213666 , time_score:  500 , memory:  697434\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward -31.785993740535112 , mean_reward:  -4.164598779341065 , time_score:  500 , memory:  699934\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward 25.46450872845976 , mean_reward:  -4.170228915988735 , time_score:  500 , memory:  702434\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward 8.150568466760493 , mean_reward:  -2.5112672473604736 , time_score:  500 , memory:  704934\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward -6.39711481933572 , mean_reward:  -1.0342059101864662 , time_score:  500 , memory:  707434\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward -16.2001965706208 , mean_reward:  -1.3865475524156339 , time_score:  500 , memory:  709934\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward -2.9532585302936925 , mean_reward:  -1.2037336934457503 , time_score:  500 , memory:  712434\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward 12.22891922017028 , mean_reward:  -0.6423650646785485 , time_score:  500 , memory:  714934\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward 5.257364753946538 , mean_reward:  0.4714329876523879 , time_score:  500 , memory:  717434\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward -13.762980371857777 , mean_reward:  0.08961577185532628 , time_score:  500 , memory:  719934\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward -11.323356376804817 , mean_reward:  1.5577714375178322 , time_score:  500 , memory:  722434\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward -31.087913619204116 , mean_reward:  1.242581130978428 , time_score:  500 , memory:  724934\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward -12.670286214426762 , mean_reward:  1.5389576530774365 , time_score:  500 , memory:  727434\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward 45.28931590485237 , mean_reward:  1.3427496264888341 , time_score:  500 , memory:  729934\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward 8.513394403042286 , mean_reward:  2.6650992137376743 , time_score:  500 , memory:  732434\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 24.875340387087515 , mean_reward:  3.0097061803524 , time_score:  500 , memory:  734934\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward -31.99278389530858 , mean_reward:  2.137939066062741 , time_score:  500 , memory:  737434\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward -17.793654346134556 , mean_reward:  1.5239868302506914 , time_score:  500 , memory:  739934\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward 9.161612009806161 , mean_reward:  1.9659771995717616 , time_score:  500 , memory:  742434\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward -0.3592677116088119 , mean_reward:  2.4038257453332763 , time_score:  500 , memory:  744934\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward -13.809266500088318 , mean_reward:  2.1054473712803543 , time_score:  500 , memory:  747434\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward -1.5637946445487938 , mean_reward:  2.4810638506779212 , time_score:  500 , memory:  749934\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward -12.438862154431368 , mean_reward:  2.082423792046386 , time_score:  500 , memory:  752434\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward -17.414030403596026 , mean_reward:  1.860296668664074 , time_score:  500 , memory:  754934\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward -19.581532472111853 , mean_reward:  1.5428321095295678 , time_score:  500 , memory:  757434\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward -19.537937675828783 , mean_reward:  2.8965060005520593 , time_score:  500 , memory:  759934\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward -28.113496364135425 , mean_reward:  2.5152199114308194 , time_score:  500 , memory:  762434\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward -47.10769982230938 , mean_reward:  1.8827349047101123 , time_score:  500 , memory:  764934\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward 49.187843322622655 , mean_reward:  1.4628258347073986 , time_score:  500 , memory:  767434\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward -0.7479926042740102 , mean_reward:  1.9488917663483927 , time_score:  500 , memory:  769934\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward 16.874775179422656 , mean_reward:  1.0994312463631573 , time_score:  500 , memory:  772434\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward -11.2374468378086 , mean_reward:  0.1808189670964974 , time_score:  500 , memory:  774934\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward 4.821524184411959 , mean_reward:  0.32819277248942996 , time_score:  500 , memory:  777434\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward -46.26599928329972 , mean_reward:  -1.3301691380860847 , time_score:  500 , memory:  779934\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward -26.197418924384685 , mean_reward:  -2.987108318794203 , time_score:  500 , memory:  782434\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward 18.139640945399655 , mean_reward:  -4.400376759852574 , time_score:  500 , memory:  784934\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward -32.06139904792368 , mean_reward:  -4.019163222021637 , time_score:  500 , memory:  787434\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward 57.685144182205704 , mean_reward:  -2.1715471697613293 , time_score:  500 , memory:  789934\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward 11.28880759861057 , mean_reward:  -3.1996374503500986 , time_score:  500 , memory:  792122\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward -16.221390048956003 , mean_reward:  -5.719222226873065 , time_score:  500 , memory:  794238\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward -81.77633230856658 , mean_reward:  -8.012463932538955 , time_score:  171 , memory:  796065\n",
      "Episode:  1780  , Epsilon:  0.01 , Reward -137.40648637120375 , mean_reward:  -9.753189715635356 , time_score:  144 , memory:  797916\n",
      "Episode:  1785  , Epsilon:  0.01 , Reward -99.45399772907533 , mean_reward:  -12.313636256971856 , time_score:  175 , memory:  799496\n",
      "Episode:  1790  , Epsilon:  0.01 , Reward 26.821043342439662 , mean_reward:  -11.828196007075853 , time_score:  500 , memory:  801996\n",
      "Episode:  1795  , Epsilon:  0.01 , Reward -83.11581613303684 , mean_reward:  -13.505653849122496 , time_score:  259 , memory:  803636\n",
      "Episode:  1800  , Epsilon:  0.01 , Reward 10.490266011395255 , mean_reward:  -15.44501036061178 , time_score:  500 , memory:  805908\n",
      "Episode:  1805  , Epsilon:  0.01 , Reward -4.273926447184977 , mean_reward:  -15.796483727578705 , time_score:  500 , memory:  808038\n",
      "Episode:  1810  , Epsilon:  0.01 , Reward -46.36693554319416 , mean_reward:  -15.468022097883507 , time_score:  500 , memory:  810538\n",
      "Episode:  1815  , Epsilon:  0.01 , Reward 47.69953102436583 , mean_reward:  -16.638520577364897 , time_score:  500 , memory:  812487\n",
      "Episode:  1820  , Epsilon:  0.01 , Reward -6.319088439714112 , mean_reward:  -19.193042875058506 , time_score:  500 , memory:  814339\n",
      "Episode:  1825  , Epsilon:  0.01 , Reward -11.910066786526517 , mean_reward:  -21.70571627439792 , time_score:  500 , memory:  816193\n",
      "Episode:  1830  , Epsilon:  0.01 , Reward -145.6246694652969 , mean_reward:  -26.550539258294684 , time_score:  219 , memory:  817962\n",
      "Episode:  1835  , Epsilon:  0.01 , Reward -33.418961590553906 , mean_reward:  -31.709450348054737 , time_score:  500 , memory:  819761\n",
      "Episode:  1840  , Epsilon:  0.01 , Reward -22.08224199365305 , mean_reward:  -31.238854717363274 , time_score:  500 , memory:  822007\n",
      "Episode:  1845  , Epsilon:  0.01 , Reward -18.49287068149189 , mean_reward:  -34.13904272547072 , time_score:  500 , memory:  823707\n",
      "Episode:  1850  , Epsilon:  0.01 , Reward 26.02124875769998 , mean_reward:  -35.0730739516526 , time_score:  500 , memory:  825818\n",
      "Episode:  1855  , Epsilon:  0.01 , Reward -13.399570802958731 , mean_reward:  -38.123240419968276 , time_score:  500 , memory:  827573\n",
      "Episode:  1860  , Epsilon:  0.01 , Reward -5.735546626608025 , mean_reward:  -40.919393316066206 , time_score:  500 , memory:  829739\n",
      "Episode:  1865  , Epsilon:  0.01 , Reward -30.083116904177015 , mean_reward:  -41.70802559964909 , time_score:  500 , memory:  831873\n",
      "Episode:  1870  , Epsilon:  0.01 , Reward -29.95557906636772 , mean_reward:  -39.790233379725734 , time_score:  500 , memory:  834373\n",
      "Episode:  1875  , Epsilon:  0.01 , Reward -114.72649120568697 , mean_reward:  -42.33480747965304 , time_score:  181 , memory:  835747\n",
      "Episode:  1880  , Epsilon:  0.01 , Reward 16.29815801294181 , mean_reward:  -43.28554364011122 , time_score:  500 , memory:  838043\n",
      "Episode:  1885  , Epsilon:  0.01 , Reward 57.50870959242424 , mean_reward:  -40.992487106867586 , time_score:  500 , memory:  840212\n",
      "Episode:  1890  , Epsilon:  0.01 , Reward -33.7204805826585 , mean_reward:  -44.638591073637926 , time_score:  500 , memory:  842144\n",
      "Episode:  1895  , Epsilon:  0.01 , Reward -114.73230802341494 , mean_reward:  -45.60014886745956 , time_score:  352 , memory:  844133\n",
      "Episode:  1900  , Epsilon:  0.01 , Reward -100.00840148303422 , mean_reward:  -47.42039754700367 , time_score:  151 , memory:  845855\n",
      "Episode:  1905  , Epsilon:  0.01 , Reward -2.3432497075989343 , mean_reward:  -48.432832931918256 , time_score:  500 , memory:  847658\n",
      "Episode:  1910  , Epsilon:  0.01 , Reward -14.989707619038164 , mean_reward:  -50.43194680045359 , time_score:  500 , memory:  850076\n",
      "Episode:  1915  , Epsilon:  0.01 , Reward -22.67396427541209 , mean_reward:  -51.674760428981756 , time_score:  500 , memory:  852014\n",
      "Episode:  1920  , Epsilon:  0.01 , Reward 15.492529333218544 , mean_reward:  -50.6703442858798 , time_score:  500 , memory:  854132\n",
      "Episode:  1925  , Epsilon:  0.01 , Reward -8.817752813642288 , mean_reward:  -49.028276344427034 , time_score:  500 , memory:  856246\n",
      "Episode:  1930  , Epsilon:  0.01 , Reward 70.43231601458496 , mean_reward:  -43.78439538784668 , time_score:  500 , memory:  858746\n",
      "Episode:  1935  , Epsilon:  0.01 , Reward 16.514381067419908 , mean_reward:  -40.28691483599472 , time_score:  500 , memory:  861241\n",
      "Episode:  1940  , Epsilon:  0.01 , Reward 17.385274937258696 , mean_reward:  -41.57090401127803 , time_score:  500 , memory:  863700\n",
      "Episode:  1945  , Epsilon:  0.01 , Reward -2.7747584950103485 , mean_reward:  -36.87777179668151 , time_score:  500 , memory:  866200\n",
      "Episode:  1950  , Epsilon:  0.01 , Reward 10.060075503447074 , mean_reward:  -39.53488398305814 , time_score:  500 , memory:  868141\n",
      "Episode:  1955  , Epsilon:  0.01 , Reward -158.95960555295144 , mean_reward:  -41.12422688344703 , time_score:  118 , memory:  869558\n",
      "Episode:  1960  , Epsilon:  0.01 , Reward 1.2146977965839028 , mean_reward:  -40.80784232571634 , time_score:  500 , memory:  871752\n",
      "Episode:  1965  , Epsilon:  0.01 , Reward -9.7033368981451 , mean_reward:  -40.90040993744465 , time_score:  500 , memory:  873887\n",
      "Episode:  1970  , Epsilon:  0.01 , Reward 25.28930531172681 , mean_reward:  -40.525944946394155 , time_score:  500 , memory:  876006\n",
      "Episode:  1975  , Epsilon:  0.01 , Reward -10.80007845026022 , mean_reward:  -35.52276127331557 , time_score:  500 , memory:  878506\n",
      "Episode:  1980  , Epsilon:  0.01 , Reward 1.511667799798099 , mean_reward:  -33.27620417923344 , time_score:  500 , memory:  880669\n",
      "Episode:  1985  , Epsilon:  0.01 , Reward -19.695217467287527 , mean_reward:  -31.829914198463214 , time_score:  500 , memory:  883169\n",
      "Episode:  1990  , Epsilon:  0.01 , Reward -50.06637261179165 , mean_reward:  -30.688653768766386 , time_score:  500 , memory:  885343\n",
      "Episode:  1995  , Epsilon:  0.01 , Reward -20.6580227427985 , mean_reward:  -30.058210165632726 , time_score:  500 , memory:  887144\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.001, lr=0.0005)\n",
    "R, R_moving = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
