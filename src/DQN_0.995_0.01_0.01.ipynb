{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_0.995_0.01_0.01.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1M1wXjs2vhYJBUd8W8vnhLoaNHwXldrvT","authorship_tag":"ABX9TyOjxN28OwBRvQshOoeBQPcr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624385880165,"user_tz":360,"elapsed":34110,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4903c42e-8fec-433d-bb83-13eeba9fcc0a"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624737765554,"user_tz":360,"elapsed":8803,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"0a89e3c5-5de0-46a0-c241-22775cf112cb"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting box2d-py\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 2.9MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-gEn1F90m1Xw","executionInfo":{"status":"ok","timestamp":1624737765556,"user_tz":360,"elapsed":4,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8","executionInfo":{"status":"ok","timestamp":1624737766008,"user_tz":360,"elapsed":455,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.01_0.01/0p01_0p01.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.01_0.01/0p01_0p01.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","   "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624750192977,"user_tz":360,"elapsed":12426971,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4d30aeaf-2a71-4a43-eb94-61d1f5360acb"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1, epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.01, lr=0.01)\n","df = dqn.play()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -187.03444767810754 , mean_reward:  -187.03444767810754 , time_score:  86 , memory:  86\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -127.12968572161053 , mean_reward:  -153.48877182026902 , time_score:  99 , memory:  470\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -73.5140450617404 , mean_reward:  -146.4303280979885 , time_score:  70 , memory:  841\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -343.212594188346 , mean_reward:  -149.812844983957 , time_score:  95 , memory:  1304\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward -58.36523133933663 , mean_reward:  -146.31461233247998 , time_score:  83 , memory:  1793\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -286.58406475423146 , mean_reward:  -148.76677656678686 , time_score:  89 , memory:  2233\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -468.7353704967169 , mean_reward:  -160.2207771893113 , time_score:  113 , memory:  2704\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -120.64088102690826 , mean_reward:  -159.3790670214736 , time_score:  88 , memory:  3157\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -71.91450075658528 , mean_reward:  -148.1258544312695 , time_score:  97 , memory:  3583\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -83.97887701471222 , mean_reward:  -144.4150470834086 , time_score:  77 , memory:  3974\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -113.61416336321615 , mean_reward:  -143.87747148678568 , time_score:  113 , memory:  4476\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -96.84576100696489 , mean_reward:  -139.09537187498898 , time_score:  101 , memory:  4913\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -85.0238314116856 , mean_reward:  -143.7680655211923 , time_score:  112 , memory:  5448\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -11.168671324767075 , mean_reward:  -142.2366470012182 , time_score:  122 , memory:  6018\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -71.32722508931298 , mean_reward:  -139.10915089358207 , time_score:  107 , memory:  6496\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward -129.77632331148564 , mean_reward:  -138.2327568855378 , time_score:  123 , memory:  7175\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -111.15100349635043 , mean_reward:  -133.79483781524036 , time_score:  114 , memory:  7845\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -95.3195854529913 , mean_reward:  -132.93065227354057 , time_score:  146 , memory:  8442\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -44.278692581514065 , mean_reward:  -130.6671554273704 , time_score:  110 , memory:  9017\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -45.379044786489054 , mean_reward:  -125.9480962583221 , time_score:  79 , memory:  9588\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward -91.76849977412797 , mean_reward:  -124.21926816220628 , time_score:  123 , memory:  10373\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -81.70547893311672 , mean_reward:  -121.75654778068859 , time_score:  160 , memory:  11127\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward 33.83369309962123 , mean_reward:  -118.64827681522753 , time_score:  111 , memory:  11738\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward 18.999788091084298 , mean_reward:  -113.13378069361579 , time_score:  135 , memory:  12551\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -36.80928107562588 , mean_reward:  -109.24125287215156 , time_score:  194 , memory:  13255\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward -110.60723058266082 , mean_reward:  -106.43455145194488 , time_score:  137 , memory:  14174\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -226.14368505885548 , mean_reward:  -100.5386665787763 , time_score:  109 , memory:  14912\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -6.082535033809862 , mean_reward:  -92.626917357733 , time_score:  145 , memory:  15543\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -14.69947297814592 , mean_reward:  -93.81819277233897 , time_score:  139 , memory:  16307\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -154.43462264829566 , mean_reward:  -92.79614895542345 , time_score:  351 , memory:  17186\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward -141.31911923363305 , mean_reward:  -89.08500764716975 , time_score:  119 , memory:  17714\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward -34.41426446565153 , mean_reward:  -88.2451644444054 , time_score:  94 , memory:  18646\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward -101.51027899753669 , mean_reward:  -88.84483839959135 , time_score:  124 , memory:  19492\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward -45.97445104113463 , mean_reward:  -85.93524552690384 , time_score:  96 , memory:  20250\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward -49.58355201166857 , mean_reward:  -81.90574133550653 , time_score:  131 , memory:  20885\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward -39.0063140412729 , mean_reward:  -77.94740510446896 , time_score:  159 , memory:  21970\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward -214.6692984135881 , mean_reward:  -81.46788970497133 , time_score:  480 , memory:  23309\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward -73.85742394656968 , mean_reward:  -81.27122029801316 , time_score:  134 , memory:  24304\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward -9.490068045178988 , mean_reward:  -78.61542018000995 , time_score:  500 , memory:  25834\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward -173.46565121064413 , mean_reward:  -79.53624124590715 , time_score:  475 , memory:  27295\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward 49.3589518389809 , mean_reward:  -73.09892306994732 , time_score:  500 , memory:  29390\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward -65.82815904028276 , mean_reward:  -68.7989486072407 , time_score:  457 , memory:  31179\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward 66.16689234010936 , mean_reward:  -64.03046921137044 , time_score:  500 , memory:  33000\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward -200.37442195853336 , mean_reward:  -62.73302244733358 , time_score:  218 , memory:  34698\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward -54.602754419320064 , mean_reward:  -60.08477398274958 , time_score:  397 , memory:  36989\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward 14.994586359943298 , mean_reward:  -56.30922577855652 , time_score:  113 , memory:  38402\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward 90.10653306941131 , mean_reward:  -50.56704897862218 , time_score:  500 , memory:  39996\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward 28.359248867594545 , mean_reward:  -54.04427609754558 , time_score:  156 , memory:  41345\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward -11.807021906820601 , mean_reward:  -50.97516321317602 , time_score:  500 , memory:  43713\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward 95.2568334301995 , mean_reward:  -47.71710523605478 , time_score:  500 , memory:  45246\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward 66.1304334428889 , mean_reward:  -43.34369701074442 , time_score:  500 , memory:  47746\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward -269.75255549612405 , mean_reward:  -48.325164507200896 , time_score:  261 , memory:  49730\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward 32.4201505831777 , mean_reward:  -40.361783524879 , time_score:  93 , memory:  51575\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward -3.915387795656682 , mean_reward:  -38.750129911483874 , time_score:  500 , memory:  53581\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward -69.20547367232413 , mean_reward:  -40.320426322727116 , time_score:  500 , memory:  56081\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward 74.37599112691466 , mean_reward:  -36.03407993372207 , time_score:  500 , memory:  58581\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward 54.97632301483229 , mean_reward:  -33.380640931919345 , time_score:  500 , memory:  60396\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward 19.784552486460502 , mean_reward:  -27.27511494987037 , time_score:  500 , memory:  62810\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward -157.67374352359275 , mean_reward:  -25.227731670787442 , time_score:  181 , memory:  64991\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward -1.4540723780211111 , mean_reward:  -20.37233591904833 , time_score:  500 , memory:  67491\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward -241.9465769561041 , mean_reward:  -22.032409505321 , time_score:  323 , memory:  69814\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward 141.83954127008823 , mean_reward:  -22.85881531345152 , time_score:  500 , memory:  71888\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward -33.00160934422205 , mean_reward:  -21.617240005549185 , time_score:  187 , memory:  74075\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward -339.0180017601308 , mean_reward:  -26.201536734340927 , time_score:  195 , memory:  75910\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward -0.45321150474666183 , mean_reward:  -30.882803718183386 , time_score:  98 , memory:  76966\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward -44.20961840705514 , mean_reward:  -28.831208570017495 , time_score:  166 , memory:  78816\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward -102.91981306512918 , mean_reward:  -29.73964705589097 , time_score:  314 , memory:  80746\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward 25.111192209327626 , mean_reward:  -29.681702772856962 , time_score:  112 , memory:  82362\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward -211.94632613161343 , mean_reward:  -37.625838274949764 , time_score:  179 , memory:  83792\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward 47.54485082543862 , mean_reward:  -42.561045209791104 , time_score:  500 , memory:  85602\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward -67.86012900450115 , mean_reward:  -51.78605392611599 , time_score:  268 , memory:  87472\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward -2.0173297121487224 , mean_reward:  -47.08597608831242 , time_score:  157 , memory:  89251\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward -5.301187934232959 , mean_reward:  -49.7307713612285 , time_score:  500 , memory:  90811\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward 0.5177651077082857 , mean_reward:  -48.61396444444813 , time_score:  167 , memory:  92631\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward -70.55567457667499 , mean_reward:  -52.539821171503846 , time_score:  221 , memory:  94203\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward -25.442064097007755 , mean_reward:  -59.581113058169315 , time_score:  500 , memory:  95817\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward 1.8627076639301094 , mean_reward:  -58.23601757869008 , time_score:  234 , memory:  97464\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward -47.80242123666636 , mean_reward:  -63.778691137103216 , time_score:  218 , memory:  98763\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward -111.50044675501125 , mean_reward:  -67.22066586305904 , time_score:  264 , memory:  100165\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward -34.16128275639956 , mean_reward:  -72.96449804729646 , time_score:  249 , memory:  101697\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward -192.33053559601854 , mean_reward:  -77.15141444001193 , time_score:  178 , memory:  103338\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward -1.964479781679274 , mean_reward:  -76.30609536006115 , time_score:  500 , memory:  105467\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward -300.3396006111395 , mean_reward:  -82.21799055378688 , time_score:  259 , memory:  107357\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward -146.48436799221525 , mean_reward:  -83.61302563136888 , time_score:  135 , memory:  108861\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward 4.1842843371507 , mean_reward:  -86.82085743095344 , time_score:  255 , memory:  109827\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward -32.30127119158266 , mean_reward:  -89.31008218690152 , time_score:  218 , memory:  110554\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward -235.62522331967708 , mean_reward:  -98.78092665976978 , time_score:  246 , memory:  111215\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward -220.06650701119355 , mean_reward:  -100.77307357407616 , time_score:  349 , memory:  113370\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward 18.630306959823614 , mean_reward:  -98.8791803084228 , time_score:  114 , memory:  114028\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward -6.0624170245939695 , mean_reward:  -102.12931691212272 , time_score:  57 , memory:  114531\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward -17.39902970650533 , mean_reward:  -105.88351181893444 , time_score:  122 , memory:  115219\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward 5.075052204021645 , mean_reward:  -104.50522650744507 , time_score:  81 , memory:  116448\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward -79.4985369325637 , mean_reward:  -101.90314002726062 , time_score:  90 , memory:  117032\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward -201.1203137103073 , mean_reward:  -106.92015848556198 , time_score:  308 , memory:  118161\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward -295.7893899564826 , mean_reward:  -107.35705631938461 , time_score:  64 , memory:  118972\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward -149.1321254755544 , mean_reward:  -110.0969761289325 , time_score:  86 , memory:  119373\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward -198.46606670865475 , mean_reward:  -116.68656705501509 , time_score:  284 , memory:  121133\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward -88.65695577493872 , mean_reward:  -114.50251472360146 , time_score:  136 , memory:  122002\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward -120.21362978239418 , mean_reward:  -116.96071651915783 , time_score:  62 , memory:  122583\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward -933.1625008920767 , mean_reward:  -134.32404574652836 , time_score:  159 , memory:  123343\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward -110.96691457680085 , mean_reward:  -142.4470870811648 , time_score:  66 , memory:  123731\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward -545.9651821518402 , mean_reward:  -158.16587695800214 , time_score:  100 , memory:  124138\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward -105.65159178555574 , mean_reward:  -162.09500674191497 , time_score:  100 , memory:  124570\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward -82.36555078013555 , mean_reward:  -161.58417572858738 , time_score:  52 , memory:  124943\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward -42.249681642866335 , mean_reward:  -159.17596122337514 , time_score:  95 , memory:  125389\n","Episode:  525  , Epsilon:  0.07196434741762824 , Reward -203.30657994474893 , mean_reward:  -164.02197375647404 , time_score:  65 , memory:  125825\n","Episode:  530  , Epsilon:  0.07018314008827135 , Reward -111.02503984573332 , mean_reward:  -160.6007115289009 , time_score:  61 , memory:  126172\n","Episode:  535  , Epsilon:  0.06844601986126451 , Reward -123.41183593819463 , mean_reward:  -162.5203352123164 , time_score:  86 , memory:  126535\n","Episode:  540  , Epsilon:  0.0667518955258533 , Reward -139.2513296839883 , mean_reward:  -168.567577797562 , time_score:  125 , memory:  126993\n","Episode:  545  , Epsilon:  0.06509970288011008 , Reward -151.24193826330458 , mean_reward:  -171.75687152602586 , time_score:  78 , memory:  127457\n","Episode:  550  , Epsilon:  0.06348840406243188 , Reward -375.01326439126444 , mean_reward:  -174.23104996887926 , time_score:  68 , memory:  127852\n","Episode:  555  , Epsilon:  0.06191698689958447 , Reward -381.78174081745766 , mean_reward:  -189.1277088282902 , time_score:  141 , memory:  128358\n","Episode:  560  , Epsilon:  0.06038446427088321 , Reward -481.5169658506195 , mean_reward:  -209.13450369402983 , time_score:  62 , memory:  128855\n","Episode:  565  , Epsilon:  0.058889873488111255 , Reward -347.3038854496021 , mean_reward:  -224.62821634358295 , time_score:  90 , memory:  129279\n","Episode:  570  , Epsilon:  0.05743227569078546 , Reward -396.8196562049207 , mean_reward:  -238.01814514439837 , time_score:  52 , memory:  129620\n","Episode:  575  , Epsilon:  0.05601075525639029 , Reward -326.9390743012085 , mean_reward:  -251.18486559377754 , time_score:  54 , memory:  129944\n","Episode:  580  , Epsilon:  0.05462441922520914 , Reward -379.3508203396393 , mean_reward:  -260.0124644748073 , time_score:  51 , memory:  130372\n","Episode:  585  , Epsilon:  0.05327239673939179 , Reward -489.01437138072845 , mean_reward:  -281.09996266400486 , time_score:  116 , memory:  130796\n","Episode:  590  , Epsilon:  0.05195383849590569 , Reward -636.5190549375932 , mean_reward:  -297.0366719675393 , time_score:  94 , memory:  131214\n","Episode:  595  , Epsilon:  0.05066791621302729 , Reward -377.74085539362414 , mean_reward:  -295.1575665348592 , time_score:  90 , memory:  131543\n","Episode:  600  , Epsilon:  0.0494138221100385 , Reward -468.13850168799524 , mean_reward:  -304.21758496443965 , time_score:  61 , memory:  131845\n","Episode:  605  , Epsilon:  0.048190768399801194 , Reward -338.47451183314377 , mean_reward:  -313.1428325091187 , time_score:  77 , memory:  132331\n","Episode:  610  , Epsilon:  0.046997986793891174 , Reward -540.6966888143555 , mean_reward:  -329.8380808113135 , time_score:  90 , memory:  132643\n","Episode:  615  , Epsilon:  0.04583472801998072 , Reward -427.03708970341654 , mean_reward:  -351.4073551859281 , time_score:  79 , memory:  133018\n","Episode:  620  , Epsilon:  0.04470026135116646 , Reward -416.7058202029273 , mean_reward:  -368.6268361138313 , time_score:  79 , memory:  133399\n","Episode:  625  , Epsilon:  0.04359387414694703 , Reward -388.2121734339588 , mean_reward:  -383.2311024959634 , time_score:  50 , memory:  133725\n","Episode:  630  , Epsilon:  0.04251487140556204 , Reward -448.54066440374737 , mean_reward:  -402.8843919475014 , time_score:  60 , memory:  134037\n","Episode:  635  , Epsilon:  0.04146257532741124 , Reward -433.0525649324117 , mean_reward:  -419.7872708236303 , time_score:  80 , memory:  134401\n","Episode:  640  , Epsilon:  0.04043632488927963 , Reward -320.3904318267455 , mean_reward:  -427.7980495492668 , time_score:  59 , memory:  134716\n","Episode:  645  , Epsilon:  0.039435475429100995 , Reward -307.8749484460203 , mean_reward:  -438.6715329033508 , time_score:  52 , memory:  135072\n","Episode:  650  , Epsilon:  0.03845939824099909 , Reward -299.08530287751995 , mean_reward:  -446.4294356495839 , time_score:  52 , memory:  135408\n","Episode:  655  , Epsilon:  0.03750748018035199 , Reward -457.9513401662189 , mean_reward:  -452.68482464925285 , time_score:  59 , memory:  135765\n","Episode:  660  , Epsilon:  0.03657912327863173 , Reward -551.2050775429673 , mean_reward:  -450.2801640002872 , time_score:  76 , memory:  136083\n","Episode:  665  , Epsilon:  0.035673744367776934 , Reward -477.7223602166524 , mean_reward:  -452.7840533218613 , time_score:  90 , memory:  136417\n","Episode:  670  , Epsilon:  0.03479077471386296 , Reward -474.9972254110848 , mean_reward:  -457.44895131100645 , time_score:  67 , memory:  136816\n","Episode:  675  , Epsilon:  0.03392965965983891 , Reward -575.7382968724125 , mean_reward:  -460.47649296993745 , time_score:  79 , memory:  137164\n","Episode:  680  , Epsilon:  0.03308985827710748 , Reward -351.81604775136475 , mean_reward:  -463.23145272820585 , time_score:  61 , memory:  137482\n","Episode:  685  , Epsilon:  0.03227084302572862 , Reward -408.28179278416525 , mean_reward:  -463.6222258502105 , time_score:  58 , memory:  137864\n","Episode:  690  , Epsilon:  0.03147209942303359 , Reward -539.2012107195148 , mean_reward:  -464.8460036619339 , time_score:  76 , memory:  138195\n","Episode:  695  , Epsilon:  0.030693125720441184 , Reward -570.2664691330336 , mean_reward:  -467.0591407991841 , time_score:  62 , memory:  138524\n","Episode:  700  , Epsilon:  0.029933432588273214 , Reward -347.5895537680519 , mean_reward:  -468.6044895154924 , time_score:  80 , memory:  138886\n","Episode:  705  , Epsilon:  0.029192542808371146 , Reward -501.30555153423245 , mean_reward:  -462.40633099150756 , time_score:  80 , memory:  139230\n","Episode:  710  , Epsilon:  0.028469990974320916 , Reward -363.33195444816016 , mean_reward:  -460.30567407046476 , time_score:  62 , memory:  139583\n","Episode:  715  , Epsilon:  0.027765323199097504 , Reward -313.54126720945237 , mean_reward:  -452.77419219518794 , time_score:  74 , memory:  139957\n","Episode:  720  , Epsilon:  0.02707809682994571 , Reward -422.31390837100946 , mean_reward:  -452.9546172897496 , time_score:  83 , memory:  140395\n","Episode:  725  , Epsilon:  0.026407880170317945 , Reward -234.50067501610062 , mean_reward:  -450.61742915384224 , time_score:  108 , memory:  140884\n","Episode:  730  , Epsilon:  0.025754252208694463 , Reward -537.6079811586242 , mean_reward:  -447.4191930819788 , time_score:  86 , memory:  141314\n","Episode:  735  , Epsilon:  0.025116802354115567 , Reward -450.5742467193778 , mean_reward:  -447.06465021697136 , time_score:  125 , memory:  141831\n","Episode:  740  , Epsilon:  0.02449513017825978 , Reward -494.85974169320207 , mean_reward:  -450.8496590790373 , time_score:  141 , memory:  142384\n","Episode:  745  , Epsilon:  0.023888845163905856 , Reward -547.4272493012488 , mean_reward:  -454.5134657679992 , time_score:  112 , memory:  142895\n","Episode:  750  , Epsilon:  0.023297566459620722 , Reward -516.5234590099353 , mean_reward:  -458.043034020571 , time_score:  78 , memory:  143438\n","Episode:  755  , Epsilon:  0.022720922640519125 , Reward -517.6273984430019 , mean_reward:  -458.75552519794013 , time_score:  109 , memory:  144024\n","Episode:  760  , Epsilon:  0.022158551474944856 , Reward -557.1321012878257 , mean_reward:  -461.5756022078025 , time_score:  98 , memory:  144543\n","Episode:  765  , Epsilon:  0.021610099696926857 , Reward -551.9880540670016 , mean_reward:  -466.1052378706008 , time_score:  129 , memory:  145090\n","Episode:  770  , Epsilon:  0.021075222784267326 , Reward -661.5693842993062 , mean_reward:  -462.4338629276128 , time_score:  107 , memory:  145770\n","Episode:  775  , Epsilon:  0.020553584742122436 , Reward -407.9137290579191 , mean_reward:  -462.7463112544403 , time_score:  160 , memory:  146491\n","Episode:  780  , Epsilon:  0.020044857891939702 , Reward -490.4568935492878 , mean_reward:  -464.8907249471223 , time_score:  148 , memory:  147232\n","Episode:  785  , Epsilon:  0.01954872266561937 , Reward -532.9876572698981 , mean_reward:  -463.4230142060715 , time_score:  100 , memory:  147919\n","Episode:  790  , Epsilon:  0.019064867404770626 , Reward -521.2910211144294 , mean_reward:  -463.03907068410075 , time_score:  168 , memory:  148741\n","Episode:  795  , Epsilon:  0.018592988164936427 , Reward -430.9335314497724 , mean_reward:  -466.3708013621815 , time_score:  162 , memory:  149510\n","Episode:  800  , Epsilon:  0.018132788524664028 , Reward -843.2765837368185 , mean_reward:  -467.9014673126319 , time_score:  183 , memory:  150251\n","Episode:  805  , Epsilon:  0.017683979399301233 , Reward -745.2133723421643 , mean_reward:  -475.5908709940065 , time_score:  118 , memory:  151254\n","Episode:  810  , Epsilon:  0.01724627885940145 , Reward -347.74512957760123 , mean_reward:  -476.93372651564715 , time_score:  401 , memory:  152378\n","Episode:  815  , Epsilon:  0.01681941195362342 , Reward -830.3025980093689 , mean_reward:  -490.793758100821 , time_score:  248 , memory:  153558\n","Episode:  820  , Epsilon:  0.0164031105360144 , Reward -301.25465918218424 , mean_reward:  -498.35748909508266 , time_score:  203 , memory:  154419\n","Episode:  825  , Epsilon:  0.015997113097568336 , Reward -341.6564179272383 , mean_reward:  -493.3062209438566 , time_score:  78 , memory:  155090\n","Episode:  830  , Epsilon:  0.015601164601953134 , Reward -1660.8745695546008 , mean_reward:  -502.53216244404047 , time_score:  271 , memory:  155992\n","Episode:  835  , Epsilon:  0.015215016325303928 , Reward -492.2397508551928 , mean_reward:  -512.7035648277407 , time_score:  187 , memory:  156870\n","Episode:  840  , Epsilon:  0.014838425699981627 , Reward -334.5708285940358 , mean_reward:  -508.2918515095046 , time_score:  163 , memory:  157719\n","Episode:  845  , Epsilon:  0.014471156162198668 , Reward -380.8256983216348 , mean_reward:  -507.7482916254288 , time_score:  148 , memory:  158735\n","Episode:  850  , Epsilon:  0.014112977003416188 , Reward -410.135992974556 , mean_reward:  -514.8436393956961 , time_score:  121 , memory:  159558\n","Episode:  855  , Epsilon:  0.013763663225419333 , Reward -467.4418237083036 , mean_reward:  -525.3435480940558 , time_score:  145 , memory:  160451\n","Episode:  860  , Epsilon:  0.013422995398979608 , Reward -367.76002328211933 , mean_reward:  -533.3675001996967 , time_score:  118 , memory:  161506\n","Episode:  865  , Epsilon:  0.013090759526015528 , Reward -1083.9577992680474 , mean_reward:  -548.9459253012931 , time_score:  256 , memory:  162715\n","Episode:  870  , Epsilon:  0.012766746905164949 , Reward -747.1630327712713 , mean_reward:  -585.820704029521 , time_score:  252 , memory:  164225\n","Episode:  875  , Epsilon:  0.012450754000684672 , Reward -437.79275217841047 , mean_reward:  -578.3080612466224 , time_score:  296 , memory:  165637\n","Episode:  880  , Epsilon:  0.012142582314594924 , Reward -694.0818401752092 , mean_reward:  -587.990562538315 , time_score:  163 , memory:  166280\n","Episode:  885  , Epsilon:  0.01184203826198843 , Reward -657.1606692721717 , mean_reward:  -601.8568483613824 , time_score:  135 , memory:  167285\n","Episode:  890  , Epsilon:  0.01154893304942575 , Reward -425.8803337778117 , mean_reward:  -596.2061884531896 , time_score:  100 , memory:  168425\n","Episode:  895  , Epsilon:  0.011263082556340478 , Reward -531.8210524956678 , mean_reward:  -602.4611183801925 , time_score:  124 , memory:  169279\n","Episode:  900  , Epsilon:  0.01098430721937979 , Reward -283.84220562483233 , mean_reward:  -614.2623381556159 , time_score:  207 , memory:  170422\n","Episode:  905  , Epsilon:  0.01071243191960775 , Reward -251.09894937352567 , mean_reward:  -609.2616056872788 , time_score:  186 , memory:  171114\n","Episode:  910  , Epsilon:  0.010447285872500434 , Reward -366.8834087358653 , mean_reward:  -608.1058038921883 , time_score:  133 , memory:  171651\n","Episode:  915  , Epsilon:  0.010188702520663827 , Reward -534.5584190329853 , mean_reward:  -596.7973821880894 , time_score:  112 , memory:  172229\n","Episode:  920  , Epsilon:  0.01 , Reward -595.3455188840544 , mean_reward:  -603.0772772412699 , time_score:  113 , memory:  173035\n","Episode:  925  , Epsilon:  0.01 , Reward -1328.289944941716 , mean_reward:  -621.3456894858476 , time_score:  245 , memory:  173923\n","Episode:  930  , Epsilon:  0.01 , Reward -1708.0356783436057 , mean_reward:  -631.7418902822749 , time_score:  285 , memory:  175080\n","Episode:  935  , Epsilon:  0.01 , Reward -1106.4328419065419 , mean_reward:  -630.388762680299 , time_score:  495 , memory:  176095\n","Episode:  940  , Epsilon:  0.01 , Reward -578.9959741238408 , mean_reward:  -633.793555260479 , time_score:  78 , memory:  177054\n","Episode:  945  , Epsilon:  0.01 , Reward -310.4572311764 , mean_reward:  -639.270827241751 , time_score:  200 , memory:  178603\n","Episode:  950  , Epsilon:  0.01 , Reward -281.9459116603589 , mean_reward:  -635.1732088887579 , time_score:  95 , memory:  179944\n","Episode:  955  , Epsilon:  0.01 , Reward -160.24735547497266 , mean_reward:  -610.7592771413707 , time_score:  143 , memory:  180949\n","Episode:  960  , Epsilon:  0.01 , Reward -498.3074516605558 , mean_reward:  -599.6937518894698 , time_score:  154 , memory:  181971\n","Episode:  965  , Epsilon:  0.01 , Reward -107.30075081236616 , mean_reward:  -568.6072307613573 , time_score:  67 , memory:  182701\n","Episode:  970  , Epsilon:  0.01 , Reward -2701.3289761022206 , mean_reward:  -551.7298958955117 , time_score:  359 , memory:  183815\n","Episode:  975  , Epsilon:  0.01 , Reward -32.878375386275096 , mean_reward:  -556.2810037914218 , time_score:  144 , memory:  184970\n","Episode:  980  , Epsilon:  0.01 , Reward 288.6829775307268 , mean_reward:  -530.8798480882095 , time_score:  203 , memory:  186689\n","Episode:  985  , Epsilon:  0.01 , Reward -1895.4655126886682 , mean_reward:  -516.3440236131769 , time_score:  467 , memory:  187908\n","Episode:  990  , Epsilon:  0.01 , Reward -1215.369194407331 , mean_reward:  -519.681965356417 , time_score:  195 , memory:  189038\n","Episode:  995  , Epsilon:  0.01 , Reward -55.337515614801475 , mean_reward:  -510.1054480053944 , time_score:  137 , memory:  189892\n","Episode:  1000  , Epsilon:  0.01 , Reward -499.43136081186367 , mean_reward:  -487.64447238050343 , time_score:  246 , memory:  190868\n","Episode:  1005  , Epsilon:  0.01 , Reward -162.43703439184426 , mean_reward:  -472.2486439089306 , time_score:  295 , memory:  191497\n","Episode:  1010  , Epsilon:  0.01 , Reward -4.670083533069018 , mean_reward:  -457.75982748072505 , time_score:  78 , memory:  191926\n","Episode:  1015  , Epsilon:  0.01 , Reward -71.0429678797812 , mean_reward:  -437.02194907851407 , time_score:  106 , memory:  192553\n","Episode:  1020  , Epsilon:  0.01 , Reward -121.32325557562642 , mean_reward:  -405.59777031821596 , time_score:  221 , memory:  193301\n","Episode:  1025  , Epsilon:  0.01 , Reward -53.46163663460443 , mean_reward:  -377.4929471150662 , time_score:  241 , memory:  193989\n","Episode:  1030  , Epsilon:  0.01 , Reward -37.67908719184808 , mean_reward:  -341.26849718199594 , time_score:  110 , memory:  194630\n","Episode:  1035  , Epsilon:  0.01 , Reward -30.158877241746907 , mean_reward:  -310.25990649353173 , time_score:  136 , memory:  195868\n","Episode:  1040  , Epsilon:  0.01 , Reward -211.4682497664034 , mean_reward:  -292.48732688748123 , time_score:  86 , memory:  196421\n","Episode:  1045  , Epsilon:  0.01 , Reward -72.77064184436153 , mean_reward:  -265.381428296868 , time_score:  84 , memory:  197204\n","Episode:  1050  , Epsilon:  0.01 , Reward -125.40372284802757 , mean_reward:  -239.85846873996195 , time_score:  141 , memory:  197819\n","Episode:  1055  , Epsilon:  0.01 , Reward -843.5072622836174 , mean_reward:  -253.08447924862733 , time_score:  231 , memory:  199008\n","Episode:  1060  , Epsilon:  0.01 , Reward 32.785617368215355 , mean_reward:  -248.16713262182253 , time_score:  165 , memory:  200222\n","Episode:  1065  , Epsilon:  0.01 , Reward -101.14406567508057 , mean_reward:  -254.19810581578807 , time_score:  500 , memory:  201963\n","Episode:  1070  , Epsilon:  0.01 , Reward -512.8785615144852 , mean_reward:  -237.5040467701543 , time_score:  171 , memory:  203050\n","Episode:  1075  , Epsilon:  0.01 , Reward -1663.9421504063396 , mean_reward:  -309.5986257796969 , time_score:  395 , memory:  204717\n","Episode:  1080  , Epsilon:  0.01 , Reward -1478.6065566654843 , mean_reward:  -339.7083704121038 , time_score:  393 , memory:  206021\n","Episode:  1085  , Epsilon:  0.01 , Reward -687.3180236587816 , mean_reward:  -378.46677044518793 , time_score:  500 , memory:  207411\n","Episode:  1090  , Epsilon:  0.01 , Reward -714.3709365486775 , mean_reward:  -380.8087148373582 , time_score:  78 , memory:  208332\n","Episode:  1095  , Epsilon:  0.01 , Reward -705.5479247657521 , mean_reward:  -394.11458013402023 , time_score:  161 , memory:  209060\n","Episode:  1100  , Epsilon:  0.01 , Reward -190.2210813405855 , mean_reward:  -404.5921890617893 , time_score:  309 , memory:  210124\n","Episode:  1105  , Epsilon:  0.01 , Reward -643.5139634131574 , mean_reward:  -419.33612340229314 , time_score:  156 , memory:  211429\n","Episode:  1110  , Epsilon:  0.01 , Reward -129.38546230230227 , mean_reward:  -428.3018934394735 , time_score:  460 , memory:  212877\n","Episode:  1115  , Epsilon:  0.01 , Reward -425.3628877674821 , mean_reward:  -452.5726102306724 , time_score:  85 , memory:  213672\n","Episode:  1120  , Epsilon:  0.01 , Reward -228.82641563060972 , mean_reward:  -464.7883231026249 , time_score:  299 , memory:  214728\n","Episode:  1125  , Epsilon:  0.01 , Reward -666.1367537483111 , mean_reward:  -480.03854797198693 , time_score:  206 , memory:  215586\n","Episode:  1130  , Epsilon:  0.01 , Reward -295.01350318740236 , mean_reward:  -489.0622491462447 , time_score:  107 , memory:  216694\n","Episode:  1135  , Epsilon:  0.01 , Reward -2417.6754801533507 , mean_reward:  -522.93818446038 , time_score:  319 , memory:  217826\n","Episode:  1140  , Epsilon:  0.01 , Reward -128.65445667537927 , mean_reward:  -527.4031327110072 , time_score:  303 , memory:  218812\n","Episode:  1145  , Epsilon:  0.01 , Reward -254.87099322561403 , mean_reward:  -536.532642479291 , time_score:  100 , memory:  219487\n","Episode:  1150  , Epsilon:  0.01 , Reward -163.07164970019738 , mean_reward:  -546.9154848445169 , time_score:  382 , memory:  220470\n","Episode:  1155  , Epsilon:  0.01 , Reward -320.8266318025942 , mean_reward:  -541.4509310117768 , time_score:  171 , memory:  221110\n","Episode:  1160  , Epsilon:  0.01 , Reward -233.77445388701244 , mean_reward:  -540.1824003088462 , time_score:  101 , memory:  222085\n","Episode:  1165  , Epsilon:  0.01 , Reward -268.614426313479 , mean_reward:  -535.2358522554647 , time_score:  108 , memory:  222730\n","Episode:  1170  , Epsilon:  0.01 , Reward -332.11610021969045 , mean_reward:  -528.8957477699892 , time_score:  74 , memory:  223275\n","Episode:  1175  , Epsilon:  0.01 , Reward -224.36553708131646 , mean_reward:  -450.57260893938263 , time_score:  178 , memory:  223852\n","Episode:  1180  , Epsilon:  0.01 , Reward -249.7835169718479 , mean_reward:  -430.8305834216501 , time_score:  116 , memory:  224695\n","Episode:  1185  , Epsilon:  0.01 , Reward -125.61994813909382 , mean_reward:  -378.16161430971096 , time_score:  144 , memory:  225655\n","Episode:  1190  , Epsilon:  0.01 , Reward -345.156098037504 , mean_reward:  -378.79160111706756 , time_score:  150 , memory:  226611\n","Episode:  1195  , Epsilon:  0.01 , Reward -895.5936101665449 , mean_reward:  -374.60480520468207 , time_score:  229 , memory:  227816\n","Episode:  1200  , Epsilon:  0.01 , Reward -207.41794212877318 , mean_reward:  -363.3491094047412 , time_score:  227 , memory:  229296\n","Episode:  1205  , Epsilon:  0.01 , Reward -140.01347130774775 , mean_reward:  -352.8385614812053 , time_score:  222 , memory:  230540\n","Episode:  1210  , Epsilon:  0.01 , Reward -156.87682069332902 , mean_reward:  -353.2692079006506 , time_score:  179 , memory:  231889\n","Episode:  1215  , Epsilon:  0.01 , Reward -168.79485023156604 , mean_reward:  -337.539856668271 , time_score:  396 , memory:  233331\n","Episode:  1220  , Epsilon:  0.01 , Reward -438.44768374916026 , mean_reward:  -330.90730124089794 , time_score:  318 , memory:  234856\n","Episode:  1225  , Epsilon:  0.01 , Reward -329.3017561148396 , mean_reward:  -323.3684213108182 , time_score:  455 , memory:  236796\n","Episode:  1230  , Epsilon:  0.01 , Reward -46.28639915100062 , mean_reward:  -316.2399519405972 , time_score:  500 , memory:  238789\n","Episode:  1235  , Epsilon:  0.01 , Reward -82.8001913365149 , mean_reward:  -288.8786396692751 , time_score:  500 , memory:  240841\n","Episode:  1240  , Epsilon:  0.01 , Reward -21.94492815163909 , mean_reward:  -296.74186118848417 , time_score:  347 , memory:  242639\n","Episode:  1245  , Epsilon:  0.01 , Reward -385.65665882519255 , mean_reward:  -294.8539293873609 , time_score:  234 , memory:  244475\n","Episode:  1250  , Epsilon:  0.01 , Reward -319.80103486074415 , mean_reward:  -299.65786284292494 , time_score:  212 , memory:  245382\n","Episode:  1255  , Epsilon:  0.01 , Reward -433.70567637684934 , mean_reward:  -296.99851147029335 , time_score:  274 , memory:  246942\n","Episode:  1260  , Epsilon:  0.01 , Reward -289.2158014370157 , mean_reward:  -298.29406442621075 , time_score:  115 , memory:  247831\n","Episode:  1265  , Epsilon:  0.01 , Reward -965.6692217090173 , mean_reward:  -311.6995022180876 , time_score:  345 , memory:  249076\n","Episode:  1270  , Epsilon:  0.01 , Reward -341.14551566642535 , mean_reward:  -314.5520542343198 , time_score:  67 , memory:  249564\n","Episode:  1275  , Epsilon:  0.01 , Reward -432.59057662358373 , mean_reward:  -321.4612799865626 , time_score:  180 , memory:  250077\n","Episode:  1280  , Epsilon:  0.01 , Reward -454.7564932012699 , mean_reward:  -323.8361939072973 , time_score:  225 , memory:  250645\n","Episode:  1285  , Epsilon:  0.01 , Reward -366.1644097604685 , mean_reward:  -337.5250546469361 , time_score:  109 , memory:  251195\n","Episode:  1290  , Epsilon:  0.01 , Reward -329.75555069331165 , mean_reward:  -336.05545617065565 , time_score:  72 , memory:  251683\n","Episode:  1295  , Epsilon:  0.01 , Reward -663.0088423376922 , mean_reward:  -329.64427270565835 , time_score:  124 , memory:  252349\n","Episode:  1300  , Epsilon:  0.01 , Reward -206.12599010433104 , mean_reward:  -331.6268529220943 , time_score:  146 , memory:  252866\n","Episode:  1305  , Epsilon:  0.01 , Reward -233.38504228416537 , mean_reward:  -334.6115921205297 , time_score:  73 , memory:  253290\n","Episode:  1310  , Epsilon:  0.01 , Reward -459.2352122300201 , mean_reward:  -336.0304960074859 , time_score:  103 , memory:  254387\n","Episode:  1315  , Epsilon:  0.01 , Reward -567.8695045264807 , mean_reward:  -344.4304355231698 , time_score:  65 , memory:  255022\n","Episode:  1320  , Epsilon:  0.01 , Reward -420.80769128348857 , mean_reward:  -348.544971742609 , time_score:  236 , memory:  255625\n","Episode:  1325  , Epsilon:  0.01 , Reward -226.41209865146337 , mean_reward:  -353.66172107740283 , time_score:  77 , memory:  256012\n","Episode:  1330  , Epsilon:  0.01 , Reward -225.711739902663 , mean_reward:  -358.3810572717528 , time_score:  98 , memory:  256658\n","Episode:  1335  , Epsilon:  0.01 , Reward -229.4345268073707 , mean_reward:  -364.86557964222555 , time_score:  181 , memory:  257317\n","Episode:  1340  , Epsilon:  0.01 , Reward -456.06576596260714 , mean_reward:  -360.71036669048044 , time_score:  79 , memory:  257850\n","Episode:  1345  , Epsilon:  0.01 , Reward -128.51985201789296 , mean_reward:  -358.0475232487173 , time_score:  78 , memory:  258292\n","Episode:  1350  , Epsilon:  0.01 , Reward -216.0495447801783 , mean_reward:  -347.7776335278528 , time_score:  65 , memory:  258654\n","Episode:  1355  , Epsilon:  0.01 , Reward -312.3766911492878 , mean_reward:  -346.6338594455223 , time_score:  86 , memory:  259064\n","Episode:  1360  , Epsilon:  0.01 , Reward -492.519073042685 , mean_reward:  -343.8040698571787 , time_score:  68 , memory:  259560\n","Episode:  1365  , Epsilon:  0.01 , Reward -206.03664996953387 , mean_reward:  -329.0055970077998 , time_score:  79 , memory:  260000\n","Episode:  1370  , Epsilon:  0.01 , Reward -204.79598197707207 , mean_reward:  -320.4764745362621 , time_score:  85 , memory:  260435\n","Episode:  1375  , Epsilon:  0.01 , Reward -262.44202408786106 , mean_reward:  -312.73690450307004 , time_score:  105 , memory:  261044\n","Episode:  1380  , Epsilon:  0.01 , Reward -168.06953194925848 , mean_reward:  -303.72921705014693 , time_score:  96 , memory:  261517\n","Episode:  1385  , Epsilon:  0.01 , Reward -228.4940505474455 , mean_reward:  -296.4037435410661 , time_score:  103 , memory:  262047\n","Episode:  1390  , Epsilon:  0.01 , Reward -208.04347578091466 , mean_reward:  -282.49503694355496 , time_score:  102 , memory:  262466\n","Episode:  1395  , Epsilon:  0.01 , Reward -531.9454552411133 , mean_reward:  -268.663978826739 , time_score:  197 , memory:  263118\n","Episode:  1400  , Epsilon:  0.01 , Reward -144.6388880222121 , mean_reward:  -269.33698143719454 , time_score:  81 , memory:  263989\n","Episode:  1405  , Epsilon:  0.01 , Reward -147.4699577926952 , mean_reward:  -265.0126163421668 , time_score:  66 , memory:  264459\n","Episode:  1410  , Epsilon:  0.01 , Reward -160.89157091388245 , mean_reward:  -253.41780893777337 , time_score:  109 , memory:  264951\n","Episode:  1415  , Epsilon:  0.01 , Reward -178.8370727949053 , mean_reward:  -239.9900614557426 , time_score:  76 , memory:  265821\n","Episode:  1420  , Epsilon:  0.01 , Reward -61.62606019343924 , mean_reward:  -231.16833714832262 , time_score:  195 , memory:  266760\n","Episode:  1425  , Epsilon:  0.01 , Reward -85.58332937659426 , mean_reward:  -222.42101464353766 , time_score:  182 , memory:  267440\n","Episode:  1430  , Epsilon:  0.01 , Reward -133.1183048019447 , mean_reward:  -216.03837820657046 , time_score:  72 , memory:  268034\n","Episode:  1435  , Epsilon:  0.01 , Reward -69.06384315037491 , mean_reward:  -210.22421995194702 , time_score:  109 , memory:  268594\n","Episode:  1440  , Epsilon:  0.01 , Reward -105.52503664066334 , mean_reward:  -205.2806788882677 , time_score:  137 , memory:  269294\n","Episode:  1445  , Epsilon:  0.01 , Reward -243.85735538811628 , mean_reward:  -204.9735298899732 , time_score:  142 , memory:  269972\n","Episode:  1450  , Epsilon:  0.01 , Reward -326.6183951457552 , mean_reward:  -208.54548138704374 , time_score:  295 , memory:  270749\n","Episode:  1455  , Epsilon:  0.01 , Reward -465.381758170447 , mean_reward:  -210.75969681734335 , time_score:  178 , memory:  271422\n","Episode:  1460  , Epsilon:  0.01 , Reward -111.25646821558126 , mean_reward:  -211.25567272795618 , time_score:  66 , memory:  272226\n","Episode:  1465  , Epsilon:  0.01 , Reward -792.6027748909229 , mean_reward:  -212.42716496497047 , time_score:  423 , memory:  273018\n","Episode:  1470  , Epsilon:  0.01 , Reward -133.94276128801332 , mean_reward:  -208.54872060626946 , time_score:  71 , memory:  273549\n","Episode:  1475  , Epsilon:  0.01 , Reward -150.99655037859975 , mean_reward:  -201.37562753263362 , time_score:  73 , memory:  274196\n","Episode:  1480  , Epsilon:  0.01 , Reward -119.79019042746827 , mean_reward:  -199.14030656957038 , time_score:  155 , memory:  274916\n","Episode:  1485  , Epsilon:  0.01 , Reward -138.328536036822 , mean_reward:  -193.1696207145394 , time_score:  75 , memory:  275444\n","Episode:  1490  , Epsilon:  0.01 , Reward -105.0499338881009 , mean_reward:  -197.1891470797681 , time_score:  165 , memory:  276022\n","Episode:  1495  , Epsilon:  0.01 , Reward -179.68549023709141 , mean_reward:  -197.23859277282475 , time_score:  89 , memory:  276706\n","Episode:  1500  , Epsilon:  0.01 , Reward -211.92040678504978 , mean_reward:  -191.07379537594545 , time_score:  203 , memory:  277449\n","Episode:  1505  , Epsilon:  0.01 , Reward -103.1402802332107 , mean_reward:  -190.93347690959231 , time_score:  162 , memory:  278305\n","Episode:  1510  , Epsilon:  0.01 , Reward -476.38598129347065 , mean_reward:  -197.61535254211452 , time_score:  294 , memory:  279168\n","Episode:  1515  , Epsilon:  0.01 , Reward -207.84347188898641 , mean_reward:  -201.47757254651154 , time_score:  242 , memory:  279996\n","Episode:  1520  , Epsilon:  0.01 , Reward -261.79246414382305 , mean_reward:  -207.8245606250639 , time_score:  96 , memory:  280749\n","Episode:  1525  , Epsilon:  0.01 , Reward -862.7339606872445 , mean_reward:  -220.2149955839903 , time_score:  176 , memory:  281707\n","Episode:  1530  , Epsilon:  0.01 , Reward -27.143175025865617 , mean_reward:  -221.88109554082826 , time_score:  149 , memory:  282376\n","Episode:  1535  , Epsilon:  0.01 , Reward -583.0910426219566 , mean_reward:  -234.437940055335 , time_score:  154 , memory:  283354\n","Episode:  1540  , Epsilon:  0.01 , Reward -391.8457184152995 , mean_reward:  -240.9227655687391 , time_score:  124 , memory:  284067\n","Episode:  1545  , Epsilon:  0.01 , Reward -286.94648771204544 , mean_reward:  -250.61030271079915 , time_score:  160 , memory:  284877\n","Episode:  1550  , Epsilon:  0.01 , Reward -180.86120494250636 , mean_reward:  -257.0034798728825 , time_score:  192 , memory:  285740\n","Episode:  1555  , Epsilon:  0.01 , Reward -18.77744283524524 , mean_reward:  -281.4316262701044 , time_score:  143 , memory:  286921\n","Episode:  1560  , Epsilon:  0.01 , Reward -391.24698097149684 , mean_reward:  -290.8051753059292 , time_score:  85 , memory:  287735\n","Episode:  1565  , Epsilon:  0.01 , Reward -354.75250128349523 , mean_reward:  -289.87202323903824 , time_score:  155 , memory:  288824\n","Episode:  1570  , Epsilon:  0.01 , Reward -707.497432910547 , mean_reward:  -312.30063952626415 , time_score:  234 , memory:  289670\n","Episode:  1575  , Epsilon:  0.01 , Reward -901.3855494575258 , mean_reward:  -336.1452750152356 , time_score:  187 , memory:  290337\n","Episode:  1580  , Epsilon:  0.01 , Reward -603.6343605131357 , mean_reward:  -355.93452666142184 , time_score:  142 , memory:  291042\n","Episode:  1585  , Epsilon:  0.01 , Reward -501.8143357313993 , mean_reward:  -382.64155386800127 , time_score:  133 , memory:  291792\n","Episode:  1590  , Epsilon:  0.01 , Reward -444.9654530627159 , mean_reward:  -400.2914596952818 , time_score:  130 , memory:  292780\n","Episode:  1595  , Epsilon:  0.01 , Reward -559.2836277590941 , mean_reward:  -417.2906958441969 , time_score:  79 , memory:  293525\n","Episode:  1600  , Epsilon:  0.01 , Reward -879.9394227482811 , mean_reward:  -452.0680893204834 , time_score:  149 , memory:  294469\n","Episode:  1605  , Epsilon:  0.01 , Reward -612.6652418942882 , mean_reward:  -475.82929707854 , time_score:  106 , memory:  295181\n","Episode:  1610  , Epsilon:  0.01 , Reward -735.0573547098962 , mean_reward:  -492.20990689575075 , time_score:  190 , memory:  296013\n","Episode:  1615  , Epsilon:  0.01 , Reward -571.1261951109908 , mean_reward:  -509.65263870834286 , time_score:  109 , memory:  296713\n","Episode:  1620  , Epsilon:  0.01 , Reward -310.5615560498341 , mean_reward:  -527.8439225674126 , time_score:  90 , memory:  297291\n","Episode:  1625  , Epsilon:  0.01 , Reward -602.6696242416413 , mean_reward:  -537.9392495882657 , time_score:  167 , memory:  298032\n","Episode:  1630  , Epsilon:  0.01 , Reward -460.87289919396744 , mean_reward:  -561.3357253827496 , time_score:  142 , memory:  298725\n","Episode:  1635  , Epsilon:  0.01 , Reward -745.9934504350116 , mean_reward:  -570.7767962147752 , time_score:  121 , memory:  299360\n","Episode:  1640  , Epsilon:  0.01 , Reward -459.2953556713797 , mean_reward:  -576.3071628635421 , time_score:  142 , memory:  300048\n","Episode:  1645  , Epsilon:  0.01 , Reward -602.1805017122826 , mean_reward:  -583.0924465683565 , time_score:  115 , memory:  300624\n","Episode:  1650  , Epsilon:  0.01 , Reward -413.02323218994843 , mean_reward:  -581.6257584286612 , time_score:  134 , memory:  301358\n","Episode:  1655  , Epsilon:  0.01 , Reward -533.4115921628027 , mean_reward:  -564.2184695402551 , time_score:  127 , memory:  302063\n","Episode:  1660  , Epsilon:  0.01 , Reward -273.844869334275 , mean_reward:  -552.0833608328326 , time_score:  97 , memory:  302713\n","Episode:  1665  , Epsilon:  0.01 , Reward -705.2812654520925 , mean_reward:  -559.0810748535464 , time_score:  262 , memory:  303559\n","Episode:  1670  , Epsilon:  0.01 , Reward -20.826613574081577 , mean_reward:  -538.0394033137575 , time_score:  75 , memory:  304022\n","Episode:  1675  , Epsilon:  0.01 , Reward -848.349549065674 , mean_reward:  -531.5946971674706 , time_score:  87 , memory:  304668\n","Episode:  1680  , Epsilon:  0.01 , Reward -310.4146933086229 , mean_reward:  -526.2351390504286 , time_score:  230 , memory:  305300\n","Episode:  1685  , Epsilon:  0.01 , Reward -334.17112586752023 , mean_reward:  -505.8808770454747 , time_score:  193 , memory:  306034\n","Episode:  1690  , Epsilon:  0.01 , Reward -294.4776893386196 , mean_reward:  -489.8495153128722 , time_score:  68 , memory:  306671\n","Episode:  1695  , Epsilon:  0.01 , Reward -114.22633050748387 , mean_reward:  -481.17020440525926 , time_score:  63 , memory:  307140\n","Episode:  1700  , Epsilon:  0.01 , Reward -169.92638136612544 , mean_reward:  -455.66864963563836 , time_score:  66 , memory:  307574\n","Episode:  1705  , Epsilon:  0.01 , Reward -161.672687689729 , mean_reward:  -438.85062975046344 , time_score:  76 , memory:  308024\n","Episode:  1710  , Epsilon:  0.01 , Reward -151.4710224410155 , mean_reward:  -416.194412994507 , time_score:  62 , memory:  308412\n","Episode:  1715  , Epsilon:  0.01 , Reward -225.69231230056005 , mean_reward:  -407.8522881511207 , time_score:  123 , memory:  309309\n","Episode:  1720  , Epsilon:  0.01 , Reward -400.8393166859225 , mean_reward:  -391.94507087948443 , time_score:  131 , memory:  309939\n","Episode:  1725  , Epsilon:  0.01 , Reward -380.95459201361007 , mean_reward:  -377.35839615653 , time_score:  100 , memory:  310849\n","Episode:  1730  , Epsilon:  0.01 , Reward -363.58288732427604 , mean_reward:  -360.22028288582953 , time_score:  128 , memory:  311457\n","Episode:  1735  , Epsilon:  0.01 , Reward -581.1905222020082 , mean_reward:  -344.1353034966245 , time_score:  80 , memory:  312046\n","Episode:  1740  , Epsilon:  0.01 , Reward 0.2834155833627676 , mean_reward:  -332.3916595898975 , time_score:  90 , memory:  312517\n","Episode:  1745  , Epsilon:  0.01 , Reward -92.0089329310056 , mean_reward:  -319.6786700097771 , time_score:  54 , memory:  312957\n","Episode:  1750  , Epsilon:  0.01 , Reward -106.12690331772856 , mean_reward:  -313.3818566269342 , time_score:  171 , memory:  313707\n","Episode:  1755  , Epsilon:  0.01 , Reward -600.3895970986423 , mean_reward:  -298.71019313187355 , time_score:  389 , memory:  314544\n","Episode:  1760  , Epsilon:  0.01 , Reward -150.06721524728084 , mean_reward:  -300.823873630549 , time_score:  80 , memory:  315053\n","Episode:  1765  , Epsilon:  0.01 , Reward -107.86566141732924 , mean_reward:  -301.58988772893804 , time_score:  61 , memory:  315712\n","Episode:  1770  , Epsilon:  0.01 , Reward -138.7175488371675 , mean_reward:  -301.43403207717904 , time_score:  109 , memory:  316235\n","Episode:  1775  , Epsilon:  0.01 , Reward -124.285471607707 , mean_reward:  -284.63896697081486 , time_score:  58 , memory:  316976\n","Episode:  1780  , Epsilon:  0.01 , Reward -153.01381274262542 , mean_reward:  -272.0683492174155 , time_score:  138 , memory:  317587\n","Episode:  1785  , Epsilon:  0.01 , Reward -212.76836793478435 , mean_reward:  -267.80783171659454 , time_score:  80 , memory:  318162\n","Episode:  1790  , Epsilon:  0.01 , Reward -215.0755358039845 , mean_reward:  -265.3070143534495 , time_score:  149 , memory:  318716\n","Episode:  1795  , Epsilon:  0.01 , Reward -613.7830488027712 , mean_reward:  -269.7579486325347 , time_score:  141 , memory:  319184\n","Episode:  1800  , Epsilon:  0.01 , Reward -111.34649845930979 , mean_reward:  -258.2713099843792 , time_score:  120 , memory:  319667\n","Episode:  1805  , Epsilon:  0.01 , Reward -118.96529430100362 , mean_reward:  -249.88359628808016 , time_score:  148 , memory:  320098\n","Episode:  1810  , Epsilon:  0.01 , Reward -146.16600063534244 , mean_reward:  -257.2250374353204 , time_score:  75 , memory:  320516\n","Episode:  1815  , Epsilon:  0.01 , Reward -334.6346793841186 , mean_reward:  -252.81086505572677 , time_score:  69 , memory:  320943\n","Episode:  1820  , Epsilon:  0.01 , Reward -107.55125982549436 , mean_reward:  -248.30079097477733 , time_score:  84 , memory:  321408\n","Episode:  1825  , Epsilon:  0.01 , Reward -146.37850534760761 , mean_reward:  -242.34028644367152 , time_score:  87 , memory:  321861\n","Episode:  1830  , Epsilon:  0.01 , Reward -313.91029705411654 , mean_reward:  -238.99762356749244 , time_score:  87 , memory:  322241\n","Episode:  1835  , Epsilon:  0.01 , Reward -171.64673562035276 , mean_reward:  -231.34771752947174 , time_score:  58 , memory:  322652\n","Episode:  1840  , Epsilon:  0.01 , Reward -147.91059872916225 , mean_reward:  -231.04167640745584 , time_score:  90 , memory:  323157\n","Episode:  1845  , Epsilon:  0.01 , Reward -196.47595283792776 , mean_reward:  -226.65858785261588 , time_score:  67 , memory:  323526\n","Episode:  1850  , Epsilon:  0.01 , Reward -121.23874156022794 , mean_reward:  -227.59960069515415 , time_score:  73 , memory:  323898\n","Episode:  1855  , Epsilon:  0.01 , Reward -97.8884824873522 , mean_reward:  -226.44061395469387 , time_score:  120 , memory:  324334\n","Episode:  1860  , Epsilon:  0.01 , Reward -145.37992855208017 , mean_reward:  -221.17667867635745 , time_score:  67 , memory:  324711\n","Episode:  1865  , Epsilon:  0.01 , Reward -132.4879379177812 , mean_reward:  -209.60911346568213 , time_score:  73 , memory:  325106\n","Episode:  1870  , Epsilon:  0.01 , Reward -121.30710076677592 , mean_reward:  -206.46981949578236 , time_score:  63 , memory:  325490\n","Episode:  1875  , Epsilon:  0.01 , Reward -125.58121371009398 , mean_reward:  -209.79848057725175 , time_score:  75 , memory:  326028\n","Episode:  1880  , Epsilon:  0.01 , Reward -189.11724254765306 , mean_reward:  -207.3264572865381 , time_score:  74 , memory:  326400\n","Episode:  1885  , Epsilon:  0.01 , Reward -160.08574292235295 , mean_reward:  -210.18856031309772 , time_score:  78 , memory:  327005\n","Episode:  1890  , Epsilon:  0.01 , Reward -179.1913724057133 , mean_reward:  -206.31123819002315 , time_score:  76 , memory:  327369\n","Episode:  1895  , Epsilon:  0.01 , Reward -121.041682461683 , mean_reward:  -188.5886030286323 , time_score:  66 , memory:  327706\n","Episode:  1900  , Epsilon:  0.01 , Reward -114.3725947898396 , mean_reward:  -188.38206315928008 , time_score:  75 , memory:  328210\n","Episode:  1905  , Epsilon:  0.01 , Reward -164.66307136001763 , mean_reward:  -188.79834825024838 , time_score:  59 , memory:  328533\n","Episode:  1910  , Epsilon:  0.01 , Reward -117.14537566646966 , mean_reward:  -181.36417289767738 , time_score:  79 , memory:  329011\n","Episode:  1915  , Epsilon:  0.01 , Reward -272.6898849267527 , mean_reward:  -175.3408045867874 , time_score:  65 , memory:  329353\n","Episode:  1920  , Epsilon:  0.01 , Reward -94.33706599984762 , mean_reward:  -173.5767269477278 , time_score:  52 , memory:  329904\n","Episode:  1925  , Epsilon:  0.01 , Reward -132.49759026759338 , mean_reward:  -170.65207262560278 , time_score:  129 , memory:  330334\n","Episode:  1930  , Epsilon:  0.01 , Reward -470.4433074268017 , mean_reward:  -168.63237507716346 , time_score:  83 , memory:  330741\n","Episode:  1935  , Epsilon:  0.01 , Reward -156.5141664204021 , mean_reward:  -169.10769422053716 , time_score:  69 , memory:  331069\n","Episode:  1940  , Epsilon:  0.01 , Reward -65.16352221577726 , mean_reward:  -167.2930527485922 , time_score:  103 , memory:  331612\n","Episode:  1945  , Epsilon:  0.01 , Reward -137.37040276445572 , mean_reward:  -163.7636346034281 , time_score:  53 , memory:  332023\n","Episode:  1950  , Epsilon:  0.01 , Reward -101.5899009118674 , mean_reward:  -160.36566163644383 , time_score:  119 , memory:  332540\n","Episode:  1955  , Epsilon:  0.01 , Reward -509.11955657003523 , mean_reward:  -162.24299624446104 , time_score:  161 , memory:  333083\n","Episode:  1960  , Epsilon:  0.01 , Reward -25.172805645817334 , mean_reward:  -167.05609215534383 , time_score:  57 , memory:  333467\n","Episode:  1965  , Epsilon:  0.01 , Reward -89.48800595688087 , mean_reward:  -169.3536828311712 , time_score:  133 , memory:  333870\n","Episode:  1970  , Epsilon:  0.01 , Reward -161.48166299368816 , mean_reward:  -169.92628042618892 , time_score:  71 , memory:  334394\n","Episode:  1975  , Epsilon:  0.01 , Reward -422.6097040622822 , mean_reward:  -172.8195731523201 , time_score:  101 , memory:  334984\n","Episode:  1980  , Epsilon:  0.01 , Reward -140.40732397794972 , mean_reward:  -172.89616539178172 , time_score:  86 , memory:  335428\n","Episode:  1985  , Epsilon:  0.01 , Reward -155.1076730200821 , mean_reward:  -166.92115268117516 , time_score:  80 , memory:  335873\n","Episode:  1990  , Epsilon:  0.01 , Reward -437.95050624844043 , mean_reward:  -174.17332268208412 , time_score:  82 , memory:  336333\n","Episode:  1995  , Epsilon:  0.01 , Reward -456.7704808290718 , mean_reward:  -182.03796530190033 , time_score:  59 , memory:  336797\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8zEK6_8NkZvY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzXeEPyZkZx5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmu7jobCkZ0S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}