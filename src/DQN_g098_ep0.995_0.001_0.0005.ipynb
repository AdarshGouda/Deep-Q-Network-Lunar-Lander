{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"g98_ep0p995_0p001_0p0005.csv\"\n",
    "        self.model_filename = \"g98_ep0p995_0p001_0p0005.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "            if e % 100 == 0:\n",
    "\n",
    "              self.Q_model.save(self.model_filename)\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "              self.ep *= self.ep_decay\n",
    "            else:\n",
    "              self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "\n",
    "        with open(self.csv_filename, 'a') as f:\n",
    "          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return self.df_ddqn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -215.4149050055692 , mean_reward:  -215.4149050055692 , time_score:  77 , memory:  77\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -120.07211287495079 , mean_reward:  -244.434728350103 , time_score:  67 , memory:  534\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -27.008309959950708 , mean_reward:  -215.92650349181795 , time_score:  96 , memory:  1047\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -218.8820057304002 , mean_reward:  -202.1791387892071 , time_score:  118 , memory:  1573\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -169.38435737453935 , mean_reward:  -193.6728586972175 , time_score:  78 , memory:  2047\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -196.00897894648017 , mean_reward:  -185.59538554285064 , time_score:  89 , memory:  2543\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -96.7336721513453 , mean_reward:  -181.88272001545718 , time_score:  111 , memory:  3082\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -96.32790896265132 , mean_reward:  -173.79806427957033 , time_score:  81 , memory:  3546\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -189.3921580288794 , mean_reward:  -174.9112964873126 , time_score:  75 , memory:  4055\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -190.0751176975942 , mean_reward:  -173.35046931207486 , time_score:  95 , memory:  4580\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward 14.687267024436863 , mean_reward:  -172.19889446152402 , time_score:  106 , memory:  5171\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -107.62333860720264 , mean_reward:  -174.64826184120093 , time_score:  127 , memory:  5747\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -116.36450891747504 , mean_reward:  -172.5611553133619 , time_score:  104 , memory:  6745\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -174.33270844604596 , mean_reward:  -166.87962802228645 , time_score:  172 , memory:  7404\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -194.10831737679518 , mean_reward:  -167.8848156522329 , time_score:  127 , memory:  8001\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -21.81652573853762 , mean_reward:  -161.38941607586483 , time_score:  94 , memory:  8603\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward -191.80342324909878 , mean_reward:  -157.33232224471917 , time_score:  87 , memory:  9297\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -339.39722334204066 , mean_reward:  -159.98051919461628 , time_score:  140 , memory:  10026\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -66.81087546792374 , mean_reward:  -158.4851185366793 , time_score:  164 , memory:  10804\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -195.00719465368292 , mean_reward:  -158.3503979404187 , time_score:  120 , memory:  11477\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -135.34600148765514 , mean_reward:  -156.8355832435967 , time_score:  169 , memory:  12276\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -150.21243425726368 , mean_reward:  -151.59772544200086 , time_score:  156 , memory:  12973\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -255.2517941086792 , mean_reward:  -148.98386453880497 , time_score:  117 , memory:  13760\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -66.98484695826828 , mean_reward:  -146.97761010853338 , time_score:  96 , memory:  14530\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -99.61348334260096 , mean_reward:  -143.48059850586685 , time_score:  118 , memory:  15413\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -81.13263850504225 , mean_reward:  -142.1109703199368 , time_score:  126 , memory:  16297\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -87.79351926521954 , mean_reward:  -142.54129533492122 , time_score:  265 , memory:  17216\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -145.45607719102208 , mean_reward:  -142.02515405901963 , time_score:  143 , memory:  17840\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward 58.60722267485218 , mean_reward:  -134.72380385390932 , time_score:  168 , memory:  18850\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward -139.83637572771062 , mean_reward:  -132.72018279408599 , time_score:  239 , memory:  19734\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -108.13900462361494 , mean_reward:  -130.6290828595203 , time_score:  195 , memory:  20662\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -67.3360519271813 , mean_reward:  -124.91181653038616 , time_score:  200 , memory:  21478\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -180.60247864341798 , mean_reward:  -121.07463844974568 , time_score:  205 , memory:  22897\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward 106.27560893567352 , mean_reward:  -119.78010757062954 , time_score:  500 , memory:  24425\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -370.169465500846 , mean_reward:  -117.43353604359903 , time_score:  322 , memory:  25666\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward 32.288240133487676 , mean_reward:  -117.18278096545568 , time_score:  177 , memory:  26970\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward 83.82212954719165 , mean_reward:  -116.2608535033782 , time_score:  500 , memory:  29108\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -47.913029125658696 , mean_reward:  -107.45001081539885 , time_score:  138 , memory:  30675\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward -88.08694025466974 , mean_reward:  -101.4219962601272 , time_score:  450 , memory:  32669\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -78.41012423399161 , mean_reward:  -96.37057216459527 , time_score:  311 , memory:  34272\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -176.0007686145338 , mean_reward:  -91.89655821556582 , time_score:  321 , memory:  35690\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward -270.31758300265585 , mean_reward:  -87.09604334227325 , time_score:  278 , memory:  37722\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -188.43070334081392 , mean_reward:  -82.57122800295751 , time_score:  311 , memory:  39392\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward 6.776334662043098 , mean_reward:  -80.38651191867285 , time_score:  204 , memory:  40957\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward 51.82320234656302 , mean_reward:  -72.75213447701225 , time_score:  500 , memory:  43158\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward -176.1405109258212 , mean_reward:  -65.93628083302637 , time_score:  314 , memory:  44855\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward -87.00410351388977 , mean_reward:  -59.09033576698792 , time_score:  500 , memory:  46812\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward 40.42399769047509 , mean_reward:  -53.084531995748605 , time_score:  500 , memory:  48894\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward -14.113352710977836 , mean_reward:  -52.71216606071451 , time_score:  181 , memory:  50624\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward -70.79732772940274 , mean_reward:  -49.50940259486707 , time_score:  500 , memory:  53105\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward 110.95694680106841 , mean_reward:  -43.34066704139148 , time_score:  500 , memory:  55325\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward 134.69423246144777 , mean_reward:  -36.920349707142634 , time_score:  500 , memory:  57825\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward -16.66787806972061 , mean_reward:  -34.042955423535844 , time_score:  500 , memory:  59906\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward -15.783936486468157 , mean_reward:  -31.159364469430148 , time_score:  500 , memory:  62406\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward -14.73119202129671 , mean_reward:  -25.654590559833185 , time_score:  500 , memory:  64624\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward -51.826918805655815 , mean_reward:  -23.75361119034475 , time_score:  500 , memory:  67124\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward -47.28339732029781 , mean_reward:  -19.043295917983883 , time_score:  500 , memory:  69338\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward -24.74480543728459 , mean_reward:  -15.719812358531163 , time_score:  500 , memory:  71838\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward 1.7015890354815877 , mean_reward:  -14.085933408084609 , time_score:  500 , memory:  74338\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward -10.689998010697026 , mean_reward:  -12.27258392971448 , time_score:  500 , memory:  76838\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward 37.14258270130479 , mean_reward:  -8.805965910868684 , time_score:  500 , memory:  79338\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward -149.9817497501496 , mean_reward:  -7.473507290682713 , time_score:  159 , memory:  81497\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward -22.526285241498744 , mean_reward:  -6.438534469118988 , time_score:  500 , memory:  83997\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward -47.23549601895393 , mean_reward:  -2.8274787623561015 , time_score:  500 , memory:  86274\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward 8.194068584018265 , mean_reward:  -4.164178515389912 , time_score:  500 , memory:  88774\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward 22.08281442824478 , mean_reward:  -4.002795053981715 , time_score:  500 , memory:  91274\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward 4.082533301032087 , mean_reward:  -3.431497978074056 , time_score:  500 , memory:  93774\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward -21.263922449477555 , mean_reward:  -4.775622482998296 , time_score:  500 , memory:  96274\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward 24.154322805690295 , mean_reward:  -4.855919349531887 , time_score:  500 , memory:  98509\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward -11.1876709162792 , mean_reward:  -2.13362201729571 , time_score:  500 , memory:  101009\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward -3.036240750524036 , mean_reward:  -4.197154476434842 , time_score:  500 , memory:  103509\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward 24.941148030488673 , mean_reward:  -7.189942733247936 , time_score:  500 , memory:  106009\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward 9.053130141695812 , mean_reward:  -6.992778303688234 , time_score:  500 , memory:  108509\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward 31.890152458314702 , mean_reward:  -5.1154895113377075 , time_score:  500 , memory:  111009\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward -46.91409251060472 , mean_reward:  -4.280190415265365 , time_score:  500 , memory:  113509\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward 5.542893663529257 , mean_reward:  -1.7954717455180276 , time_score:  500 , memory:  116009\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward 37.74890030641866 , mean_reward:  -2.313193125319576 , time_score:  500 , memory:  118419\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward 9.122952344014164 , mean_reward:  -3.77105610268324 , time_score:  500 , memory:  120919\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward 5.810535296692753 , mean_reward:  -4.878918818495926 , time_score:  500 , memory:  123419\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward 48.505404832826414 , mean_reward:  -2.3005799962814653 , time_score:  500 , memory:  125919\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward 89.03261269897686 , mean_reward:  -0.6208277779311459 , time_score:  500 , memory:  128419\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward 10.755110167465183 , mean_reward:  -0.08269048019325961 , time_score:  500 , memory:  130919\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward 46.545281467084806 , mean_reward:  1.694404919064749 , time_score:  500 , memory:  133419\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward 143.9711315414019 , mean_reward:  4.520508878988437 , time_score:  500 , memory:  135919\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward 12.693515757926633 , mean_reward:  3.87828556500701 , time_score:  500 , memory:  138419\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward 8.675911047933749 , mean_reward:  4.697741297478667 , time_score:  500 , memory:  140919\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward 120.4878563946651 , mean_reward:  8.315689299229208 , time_score:  500 , memory:  143419\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward -24.530021931675016 , mean_reward:  11.738911372831428 , time_score:  500 , memory:  145912\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward 10.24720594789654 , mean_reward:  12.286120629617109 , time_score:  500 , memory:  148043\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward 51.027820917853504 , mean_reward:  12.79022375191247 , time_score:  500 , memory:  150543\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward 5.21858426551978 , mean_reward:  14.835061074445644 , time_score:  500 , memory:  152725\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward 14.037932954445317 , mean_reward:  14.98476994752063 , time_score:  500 , memory:  155225\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward 16.33454679389954 , mean_reward:  16.037472768315478 , time_score:  500 , memory:  157725\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward 18.681127686259746 , mean_reward:  15.248258408007722 , time_score:  500 , memory:  159969\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward 68.52360078343385 , mean_reward:  14.308377292525646 , time_score:  500 , memory:  161846\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward 53.472311955401196 , mean_reward:  12.489150984645702 , time_score:  500 , memory:  163990\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward 34.09367681074878 , mean_reward:  12.397629958882517 , time_score:  500 , memory:  166490\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward -113.53778862975871 , mean_reward:  11.633126886347029 , time_score:  380 , memory:  168870\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward 62.021923966124916 , mean_reward:  12.739229662297474 , time_score:  500 , memory:  171370\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward -3.0339413740604293 , mean_reward:  12.031918821305558 , time_score:  500 , memory:  173870\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward -66.9333781338878 , mean_reward:  8.682595789281734 , time_score:  176 , memory:  176046\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward -25.127039706185638 , mean_reward:  5.860825459005522 , time_score:  500 , memory:  178465\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward -13.961016579339834 , mean_reward:  3.9231552054073204 , time_score:  500 , memory:  180587\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward -10.048688186039085 , mean_reward:  1.7231613761234583 , time_score:  500 , memory:  183087\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward 269.5689370910067 , mean_reward:  4.946775799343965 , time_score:  458 , memory:  185545\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward 98.37079172380786 , mean_reward:  5.279902498962421 , time_score:  500 , memory:  188045\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward 14.131782702614403 , mean_reward:  5.721302087847244 , time_score:  500 , memory:  190545\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward 44.621063012164726 , mean_reward:  4.750135690614383 , time_score:  500 , memory:  193045\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward 33.81416561138087 , mean_reward:  6.42944769946669 , time_score:  500 , memory:  195545\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward -1.1926891275685423 , mean_reward:  6.341507639498124 , time_score:  500 , memory:  198045\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward 25.70719331494095 , mean_reward:  8.172256630595802 , time_score:  500 , memory:  200545\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward 11.015804541761309 , mean_reward:  12.294008916786204 , time_score:  500 , memory:  203045\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward 79.03258148826646 , mean_reward:  14.285128718761364 , time_score:  500 , memory:  205545\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward -7.416447091827375 , mean_reward:  14.532008746025562 , time_score:  500 , memory:  208045\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward 116.97579370356686 , mean_reward:  17.648446158547006 , time_score:  500 , memory:  210545\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward 55.26693424186706 , mean_reward:  19.08287211860874 , time_score:  500 , memory:  213045\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward -20.74273231439736 , mean_reward:  17.948997180706893 , time_score:  500 , memory:  215212\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward 36.2099721895318 , mean_reward:  19.408491192477115 , time_score:  500 , memory:  217712\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward 92.59451562283304 , mean_reward:  23.001316167103365 , time_score:  500 , memory:  220206\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward 56.40919926623053 , mean_reward:  24.046675987244527 , time_score:  500 , memory:  222706\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward -0.14597587265926948 , mean_reward:  26.715798783739455 , time_score:  500 , memory:  225206\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward -10.066673972148639 , mean_reward:  32.209864963271656 , time_score:  500 , memory:  227706\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward -11.044815440141026 , mean_reward:  34.31899772657004 , time_score:  500 , memory:  230206\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward 73.7674283198114 , mean_reward:  36.607263944225714 , time_score:  500 , memory:  232706\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward 51.47981297293615 , mean_reward:  34.51310493551897 , time_score:  500 , memory:  235206\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward 27.97272607240692 , mean_reward:  33.857539750739406 , time_score:  500 , memory:  237706\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward 65.64987730748139 , mean_reward:  32.03940321351957 , time_score:  500 , memory:  240206\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward 31.396633180252934 , mean_reward:  32.08655835841017 , time_score:  500 , memory:  242706\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward 62.89141715960108 , mean_reward:  33.70279849057555 , time_score:  500 , memory:  245206\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward 52.48228591010604 , mean_reward:  36.598419788103726 , time_score:  500 , memory:  247706\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward 43.70616610780384 , mean_reward:  36.420516732163875 , time_score:  500 , memory:  250206\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward 16.87616822981428 , mean_reward:  34.702156596645736 , time_score:  500 , memory:  252706\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 62.52236980777566 , mean_reward:  33.79145493276937 , time_score:  500 , memory:  255206\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward 27.944916625188004 , mean_reward:  33.46365693313625 , time_score:  500 , memory:  257706\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward 39.60396125779202 , mean_reward:  33.96471282333991 , time_score:  500 , memory:  260206\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward -11.266018050884245 , mean_reward:  34.338573752888934 , time_score:  500 , memory:  262706\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward 0.058659642009015966 , mean_reward:  36.55115777895256 , time_score:  500 , memory:  265206\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward 92.37496375212966 , mean_reward:  38.652107898016894 , time_score:  500 , memory:  267706\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward 55.13449839545055 , mean_reward:  37.456234696461344 , time_score:  500 , memory:  270206\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward 55.155106895481595 , mean_reward:  37.43836444783088 , time_score:  500 , memory:  272706\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward 18.539731243622754 , mean_reward:  37.51050496194347 , time_score:  500 , memory:  275206\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward 34.86599977975527 , mean_reward:  36.26225389864637 , time_score:  500 , memory:  277706\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward 13.424895300559811 , mean_reward:  38.026783382136315 , time_score:  500 , memory:  280206\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 72.07615402703193 , mean_reward:  37.70116493282887 , time_score:  500 , memory:  282706\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward 23.101251041281426 , mean_reward:  37.298055936352405 , time_score:  500 , memory:  285206\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 60.26491585222506 , mean_reward:  38.569998078656965 , time_score:  500 , memory:  287706\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward 38.841557532275985 , mean_reward:  39.912448757657366 , time_score:  500 , memory:  290206\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward 28.033937337101538 , mean_reward:  40.9462506527389 , time_score:  500 , memory:  292706\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 11.522125439597259 , mean_reward:  40.24069091064528 , time_score:  500 , memory:  295206\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward 229.84403769005283 , mean_reward:  40.842237674126096 , time_score:  409 , memory:  297615\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward 63.781303597461246 , mean_reward:  41.70034304183971 , time_score:  500 , memory:  300115\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward 58.23138322065164 , mean_reward:  43.0724847333997 , time_score:  500 , memory:  302615\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward 77.45550436260274 , mean_reward:  44.99115703146412 , time_score:  500 , memory:  305115\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward 52.629310457159555 , mean_reward:  46.73802468481442 , time_score:  500 , memory:  307615\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward 25.873068737227527 , mean_reward:  45.80293850802619 , time_score:  500 , memory:  310115\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward 40.35226733007362 , mean_reward:  47.72900411394758 , time_score:  500 , memory:  312615\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward 36.10041209420648 , mean_reward:  49.28174238179603 , time_score:  500 , memory:  315115\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward 7.776228350652232 , mean_reward:  47.11141197217925 , time_score:  500 , memory:  317615\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward 72.70437154666105 , mean_reward:  46.087294499869905 , time_score:  500 , memory:  320115\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 72.41538380281698 , mean_reward:  46.5817957709739 , time_score:  500 , memory:  322615\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward 6.566436103732675 , mean_reward:  47.218586380949525 , time_score:  500 , memory:  325115\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward 28.61232834025233 , mean_reward:  49.01835257222888 , time_score:  500 , memory:  327615\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward 70.47344216043727 , mean_reward:  49.34944717207237 , time_score:  500 , memory:  330115\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward 99.15708471914833 , mean_reward:  49.67574055208704 , time_score:  500 , memory:  332615\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward 105.66988548564312 , mean_reward:  50.048559307821414 , time_score:  500 , memory:  335115\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward 84.36321275314678 , mean_reward:  51.46315814003834 , time_score:  500 , memory:  337615\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward 39.72711374813337 , mean_reward:  51.451322773630494 , time_score:  500 , memory:  340115\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward 28.444293402606167 , mean_reward:  50.49761546198281 , time_score:  500 , memory:  342615\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward 29.99431088504418 , mean_reward:  50.507415523008476 , time_score:  500 , memory:  345115\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward 88.29996422653578 , mean_reward:  48.368710235599636 , time_score:  500 , memory:  347615\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward 26.405394935750554 , mean_reward:  47.598695869478036 , time_score:  500 , memory:  350115\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward 21.884440588853337 , mean_reward:  46.98418303865551 , time_score:  500 , memory:  352615\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward 22.52613055584177 , mean_reward:  44.82640046071024 , time_score:  500 , memory:  355115\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward 0.6378118313530401 , mean_reward:  44.64162397308698 , time_score:  500 , memory:  357615\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward 9.230267003095483 , mean_reward:  45.40790523956443 , time_score:  500 , memory:  360115\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward -1.5522131696172898 , mean_reward:  45.556011526047094 , time_score:  500 , memory:  362611\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward 41.074603150995046 , mean_reward:  44.627517561550086 , time_score:  500 , memory:  365111\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward 25.009321723143557 , mean_reward:  45.151931258924385 , time_score:  500 , memory:  367611\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward 48.259216675944664 , mean_reward:  44.74004274359196 , time_score:  500 , memory:  370111\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward 25.70607532204497 , mean_reward:  44.34885634436924 , time_score:  500 , memory:  372611\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward 65.30806219856383 , mean_reward:  44.730035214467215 , time_score:  500 , memory:  375111\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward 19.482334886005795 , mean_reward:  43.5968704980043 , time_score:  500 , memory:  377611\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward 39.658971041268316 , mean_reward:  43.0194589733452 , time_score:  500 , memory:  380111\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward 11.270726165902154 , mean_reward:  41.787009321035114 , time_score:  500 , memory:  382611\n",
      "Episode:  920  , Epsilon:  0.01 , Reward 69.7228827945894 , mean_reward:  44.587493431560176 , time_score:  500 , memory:  385091\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 60.281593473304625 , mean_reward:  42.242541648877 , time_score:  500 , memory:  387591\n",
      "Episode:  930  , Epsilon:  0.01 , Reward 89.33569699933038 , mean_reward:  42.417138658269074 , time_score:  500 , memory:  390091\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 43.23411918982746 , mean_reward:  47.230169804960845 , time_score:  500 , memory:  392569\n",
      "Episode:  940  , Epsilon:  0.01 , Reward 52.65734949675522 , mean_reward:  48.201880669179246 , time_score:  500 , memory:  395069\n",
      "Episode:  945  , Epsilon:  0.01 , Reward 101.39975008469649 , mean_reward:  49.46904722713563 , time_score:  500 , memory:  397569\n",
      "Episode:  950  , Epsilon:  0.01 , Reward 34.03007589735581 , mean_reward:  50.326803718196636 , time_score:  500 , memory:  400069\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 34.52843072007659 , mean_reward:  50.08822720200462 , time_score:  500 , memory:  402569\n",
      "Episode:  960  , Epsilon:  0.01 , Reward 97.31076944300719 , mean_reward:  51.536358695167735 , time_score:  500 , memory:  405069\n",
      "Episode:  965  , Epsilon:  0.01 , Reward 0.49093782965656496 , mean_reward:  51.949564762002886 , time_score:  500 , memory:  407569\n",
      "Episode:  970  , Epsilon:  0.01 , Reward 5.966986985981518 , mean_reward:  51.60961449709655 , time_score:  500 , memory:  410069\n",
      "Episode:  975  , Epsilon:  0.01 , Reward 25.34540777988182 , mean_reward:  49.758392882083015 , time_score:  500 , memory:  412569\n",
      "Episode:  980  , Epsilon:  0.01 , Reward 55.756717663606466 , mean_reward:  49.58766041994487 , time_score:  500 , memory:  415069\n",
      "Episode:  985  , Epsilon:  0.01 , Reward 72.67719084907515 , mean_reward:  50.51715819422536 , time_score:  500 , memory:  417569\n",
      "Episode:  990  , Epsilon:  0.01 , Reward 48.63054786683389 , mean_reward:  50.89318109163403 , time_score:  500 , memory:  420069\n",
      "Episode:  995  , Epsilon:  0.01 , Reward 55.82608963424751 , mean_reward:  52.06488725924695 , time_score:  500 , memory:  422569\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward -14.263704468356096 , mean_reward:  51.35395350214023 , time_score:  500 , memory:  425069\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward 100.38405990751544 , mean_reward:  54.80587138970284 , time_score:  500 , memory:  427547\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward 49.1319500389561 , mean_reward:  55.49208164334634 , time_score:  500 , memory:  430047\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward 46.005172469031706 , mean_reward:  56.79650067666475 , time_score:  500 , memory:  432547\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward 84.24872300991784 , mean_reward:  55.246461037249894 , time_score:  500 , memory:  435047\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward 14.822837635140893 , mean_reward:  55.66131037593637 , time_score:  500 , memory:  437547\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward -1.7414468541980415 , mean_reward:  53.79647525338309 , time_score:  500 , memory:  440047\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward 60.47391513310383 , mean_reward:  50.815361292659716 , time_score:  500 , memory:  442538\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward 32.8177744345742 , mean_reward:  50.305500320825686 , time_score:  500 , memory:  445038\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward 7.012102274708098 , mean_reward:  48.513775300001626 , time_score:  500 , memory:  447538\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward -5.080668992125436 , mean_reward:  49.11623872904088 , time_score:  500 , memory:  450035\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward 51.784305325996485 , mean_reward:  49.94983001591014 , time_score:  500 , memory:  452535\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward 31.851186195353108 , mean_reward:  49.90032302882203 , time_score:  500 , memory:  455035\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward 68.30023794163768 , mean_reward:  49.95658596861755 , time_score:  500 , memory:  457535\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward 81.90945923618553 , mean_reward:  51.07747007063762 , time_score:  500 , memory:  460035\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward 49.68629063303155 , mean_reward:  53.41956193562685 , time_score:  500 , memory:  462535\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward 77.93073298194703 , mean_reward:  57.03593366547571 , time_score:  500 , memory:  465023\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward 34.380610446955515 , mean_reward:  57.919686799531284 , time_score:  500 , memory:  467523\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward 76.75090620767958 , mean_reward:  61.63873037726034 , time_score:  500 , memory:  470017\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward 102.70923526590516 , mean_reward:  62.06099103576335 , time_score:  500 , memory:  472517\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward 107.33137669700174 , mean_reward:  65.19176012871377 , time_score:  500 , memory:  474956\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward 141.35406777324704 , mean_reward:  64.87123644858131 , time_score:  500 , memory:  477456\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward 108.24149030049134 , mean_reward:  65.4092465105271 , time_score:  500 , memory:  479956\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 39.681682940461435 , mean_reward:  66.32273215056408 , time_score:  500 , memory:  482456\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward 238.2574446509648 , mean_reward:  68.55850407526405 , time_score:  486 , memory:  484942\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward 32.51051656308451 , mean_reward:  70.14467298374169 , time_score:  500 , memory:  487442\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward 36.07301082453084 , mean_reward:  74.3006072631763 , time_score:  500 , memory:  489931\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward 60.90906975469303 , mean_reward:  74.97296353562302 , time_score:  500 , memory:  492431\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward 68.48099727475329 , mean_reward:  76.03670457182338 , time_score:  500 , memory:  494931\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward 58.27239933850497 , mean_reward:  78.33895266465481 , time_score:  500 , memory:  497431\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward 45.01538019398257 , mean_reward:  78.4307958898462 , time_score:  500 , memory:  499931\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward 75.88251781694832 , mean_reward:  80.67653047392149 , time_score:  500 , memory:  502416\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward 240.59448947744022 , mean_reward:  86.5393379508202 , time_score:  414 , memory:  504745\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 69.13635781573265 , mean_reward:  88.03608819580701 , time_score:  500 , memory:  507245\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward 117.21721689487475 , mean_reward:  88.93186656210104 , time_score:  500 , memory:  509745\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward 54.37799338932699 , mean_reward:  90.13433386330445 , time_score:  500 , memory:  512245\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward 113.90812645621554 , mean_reward:  93.55085111480817 , time_score:  500 , memory:  514543\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward 261.7347674128416 , mean_reward:  96.72272785039011 , time_score:  433 , memory:  516976\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward 90.22433550495704 , mean_reward:  94.65235817036987 , time_score:  500 , memory:  519476\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward 48.450698292865546 , mean_reward:  93.7175938438433 , time_score:  500 , memory:  521976\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward 66.35499055167993 , mean_reward:  93.94470122039627 , time_score:  500 , memory:  524416\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward 40.09183829785486 , mean_reward:  96.36425483354222 , time_score:  500 , memory:  526739\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 64.06600420075296 , mean_reward:  98.02649596586868 , time_score:  500 , memory:  529225\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward 51.55047220304363 , mean_reward:  100.51629395751215 , time_score:  500 , memory:  531665\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward 23.919747836227025 , mean_reward:  100.31707065028502 , time_score:  500 , memory:  534040\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward 271.47051042956616 , mean_reward:  106.80638585098711 , time_score:  430 , memory:  536308\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward 60.062243555416146 , mean_reward:  108.67882897282335 , time_score:  500 , memory:  538634\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward 29.591743666614427 , mean_reward:  108.18217043148337 , time_score:  500 , memory:  541134\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 34.620864211949446 , mean_reward:  110.14631312537243 , time_score:  500 , memory:  543566\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 103.26681273840052 , mean_reward:  114.11801579106424 , time_score:  500 , memory:  545927\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 233.63088329080406 , mean_reward:  117.66992343840835 , time_score:  489 , memory:  548363\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward 25.962597138832905 , mean_reward:  118.74115430557542 , time_score:  500 , memory:  550673\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward 257.5857421338991 , mean_reward:  117.22039699567337 , time_score:  444 , memory:  553044\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 106.36259558044063 , mean_reward:  117.20162390668591 , time_score:  500 , memory:  555544\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward 39.74938701404879 , mean_reward:  117.97304111824953 , time_score:  500 , memory:  558044\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward 101.78401345076517 , mean_reward:  117.98894192445212 , time_score:  500 , memory:  560544\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward 84.04149114587159 , mean_reward:  115.38647349901811 , time_score:  500 , memory:  562951\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward 47.740689013910504 , mean_reward:  115.48403399109893 , time_score:  500 , memory:  565292\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward 97.8195028036489 , mean_reward:  114.47488218814728 , time_score:  500 , memory:  567792\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward 254.75573981504215 , mean_reward:  120.28896995287829 , time_score:  451 , memory:  570135\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward 128.00776639749282 , mean_reward:  125.40484609387332 , time_score:  500 , memory:  572223\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward 245.62384593488616 , mean_reward:  126.85646783060693 , time_score:  384 , memory:  574302\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward 94.98183940142633 , mean_reward:  129.25756392116926 , time_score:  500 , memory:  576572\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward 92.55340640523372 , mean_reward:  133.66543794455146 , time_score:  500 , memory:  578891\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 48.7393339269622 , mean_reward:  136.72862842802024 , time_score:  500 , memory:  581191\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward 235.19360936041505 , mean_reward:  134.53898135792247 , time_score:  480 , memory:  583557\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward 81.68421294406909 , mean_reward:  136.43319218633894 , time_score:  500 , memory:  585836\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward 220.58002931676194 , mean_reward:  144.36564673054394 , time_score:  373 , memory:  587903\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward 110.10331652871153 , mean_reward:  146.92538784061344 , time_score:  500 , memory:  590284\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward 189.5996601823519 , mean_reward:  147.2282897816005 , time_score:  440 , memory:  592695\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward 80.51346739427385 , mean_reward:  149.9333738346335 , time_score:  500 , memory:  594999\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward 88.60927113015208 , mean_reward:  153.35568778721185 , time_score:  500 , memory:  597116\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward 54.15985652747416 , mean_reward:  154.36886913385078 , time_score:  500 , memory:  599392\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward 218.0571731945169 , mean_reward:  159.0645858632166 , time_score:  462 , memory:  601695\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward 116.15215677934131 , mean_reward:  163.6659054796318 , time_score:  500 , memory:  603758\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward 202.3812251036166 , mean_reward:  169.99706735569598 , time_score:  425 , memory:  605904\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward 221.0542851594742 , mean_reward:  173.38269444781832 , time_score:  435 , memory:  608017\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward 256.69908456962094 , mean_reward:  178.62570008388775 , time_score:  375 , memory:  610002\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward 273.1704229601897 , mean_reward:  185.44644894714938 , time_score:  358 , memory:  612190\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward 248.23132675709186 , mean_reward:  189.56057106014916 , time_score:  415 , memory:  614046\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward 244.54801876561072 , mean_reward:  189.18506009888122 , time_score:  360 , memory:  616166\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward 92.52235448765364 , mean_reward:  191.52425521516224 , time_score:  500 , memory:  618323\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward 273.4519563307452 , mean_reward:  196.0649528777816 , time_score:  371 , memory:  620213\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward 79.1802486752613 , mean_reward:  194.93425730118702 , time_score:  500 , memory:  622497\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward 220.29873637328387 , mean_reward:  197.28432670404575 , time_score:  383 , memory:  624552\n",
      "BRAVO, GOAL ACHIEVED!!!\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.98, episodes=2000, alpha = 0.001, lr=0.0005)\n",
    "df = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zEK6_8NkZvY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzXeEPyZkZx5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
