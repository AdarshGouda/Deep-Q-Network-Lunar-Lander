{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"g98_ep0p995_0p001_0p0001.csv\"\n",
    "        self.model_filename = \"g98_ep0p995_0p001_0p0001.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "            if e % 100 == 0:\n",
    "\n",
    "              self.Q_model.save(self.model_filename)\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "              self.ep *= self.ep_decay\n",
    "            else:\n",
    "              self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "\n",
    "        with open(self.csv_filename, 'a') as f:\n",
    "          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return self.df_ddqn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -206.10994494231966 , mean_reward:  -206.10994494231966 , time_score:  78 , memory:  78\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -211.3931962952821 , mean_reward:  -162.09306601195144 , time_score:  84 , memory:  535\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -129.02060649673146 , mean_reward:  -131.83374964521434 , time_score:  101 , memory:  983\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -111.49546085085944 , mean_reward:  -156.28886501085285 , time_score:  88 , memory:  1516\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -518.0662678245667 , mean_reward:  -186.4679300535651 , time_score:  96 , memory:  1972\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -106.93256385833469 , mean_reward:  -183.2824341042022 , time_score:  109 , memory:  2542\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -447.38032606508534 , mean_reward:  -188.73054847459107 , time_score:  80 , memory:  2985\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -432.03459277218474 , mean_reward:  -192.67327910440105 , time_score:  105 , memory:  3560\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -207.55879819248668 , mean_reward:  -187.1008689527506 , time_score:  119 , memory:  4103\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -399.2859866068779 , mean_reward:  -192.1149976583763 , time_score:  129 , memory:  4732\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward -114.70056145757901 , mean_reward:  -194.21921138039883 , time_score:  127 , memory:  5298\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -176.99510664942198 , mean_reward:  -196.04464558703012 , time_score:  135 , memory:  5923\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -174.41834175918802 , mean_reward:  -199.12969085935563 , time_score:  121 , memory:  6503\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -267.36602936579254 , mean_reward:  -206.17453063404966 , time_score:  137 , memory:  7142\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -253.60357485342774 , mean_reward:  -203.28283104815208 , time_score:  121 , memory:  7831\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -311.74127391583045 , mean_reward:  -204.15896109961852 , time_score:  132 , memory:  8460\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward -189.67586556016096 , mean_reward:  -205.39502970875432 , time_score:  159 , memory:  9159\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -203.33034405485228 , mean_reward:  -206.8638967385761 , time_score:  102 , memory:  9940\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -187.5787835107849 , mean_reward:  -205.332518150936 , time_score:  113 , memory:  10500\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -238.54002148931602 , mean_reward:  -206.43361273198434 , time_score:  104 , memory:  11222\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -131.86511132450792 , mean_reward:  -204.61681395394575 , time_score:  107 , memory:  11984\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -127.74830567471963 , mean_reward:  -205.12879088816533 , time_score:  196 , memory:  12700\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -227.5281101769898 , mean_reward:  -208.7830739634232 , time_score:  170 , memory:  13376\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -109.41389254107239 , mean_reward:  -205.72431882798787 , time_score:  225 , memory:  14194\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -100.54590260981497 , mean_reward:  -196.7976009428269 , time_score:  199 , memory:  14978\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -136.55908271105292 , mean_reward:  -194.3681817377177 , time_score:  81 , memory:  16007\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -137.26980111391822 , mean_reward:  -190.62293990981453 , time_score:  206 , memory:  16880\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -98.77959519235134 , mean_reward:  -186.7070244198369 , time_score:  117 , memory:  17589\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward -227.87811432018498 , mean_reward:  -188.11752225510944 , time_score:  207 , memory:  18596\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward -140.96926772874804 , mean_reward:  -183.01754800094875 , time_score:  139 , memory:  19468\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -216.70583969899673 , mean_reward:  -180.52227735063087 , time_score:  196 , memory:  20382\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -166.57246815555752 , mean_reward:  -176.9138743334042 , time_score:  267 , memory:  21395\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -298.232678167072 , mean_reward:  -171.0453813978541 , time_score:  228 , memory:  22754\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward -173.07853329969527 , mean_reward:  -161.30600525149694 , time_score:  402 , memory:  24461\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -161.01694381880685 , mean_reward:  -159.84446563675394 , time_score:  116 , memory:  25486\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward -110.68524399645183 , mean_reward:  -155.3695504537194 , time_score:  172 , memory:  26409\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward -81.368595462374 , mean_reward:  -148.04310220087703 , time_score:  80 , memory:  27404\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -115.70661707476813 , mean_reward:  -143.43801900850366 , time_score:  117 , memory:  28325\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward 12.617541711534557 , mean_reward:  -139.1167102714447 , time_score:  255 , memory:  29462\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -38.173729711067416 , mean_reward:  -129.41904863561976 , time_score:  500 , memory:  31196\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -219.53032514771422 , mean_reward:  -127.4862317656345 , time_score:  341 , memory:  32390\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward -291.7894706577351 , mean_reward:  -125.47453710090811 , time_score:  326 , memory:  34299\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -89.05556243767442 , mean_reward:  -122.859935820856 , time_score:  150 , memory:  35302\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward -86.69512351322385 , mean_reward:  -117.67368051956444 , time_score:  227 , memory:  37322\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward -210.67138017259697 , mean_reward:  -118.38543435191065 , time_score:  419 , memory:  39004\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward -112.18474597150927 , mean_reward:  -117.39463862865001 , time_score:  225 , memory:  40324\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward -135.2688965131643 , mean_reward:  -115.04754050672936 , time_score:  391 , memory:  42095\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward -274.42369442586494 , mean_reward:  -117.07160902186672 , time_score:  444 , memory:  43720\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward -64.36212317156652 , mean_reward:  -113.22021245368828 , time_score:  500 , memory:  45716\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward -71.34115027973162 , mean_reward:  -110.05415556264677 , time_score:  278 , memory:  47815\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward -43.85001296806045 , mean_reward:  -106.675181051861 , time_score:  500 , memory:  49832\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward -24.130497437741184 , mean_reward:  -101.81079715484579 , time_score:  249 , memory:  51780\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward 48.28076677650274 , mean_reward:  -96.97974313880161 , time_score:  500 , memory:  53191\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward 2.681190788104047 , mean_reward:  -95.66564419475904 , time_score:  223 , memory:  54864\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward -61.87078599601523 , mean_reward:  -90.33779351705317 , time_score:  251 , memory:  56958\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward 97.91884393006022 , mean_reward:  -84.91802653830854 , time_score:  500 , memory:  58912\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward -14.811711930298841 , mean_reward:  -85.04161121880581 , time_score:  391 , memory:  60296\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward -223.13255472381343 , mean_reward:  -81.00947578999636 , time_score:  341 , memory:  61944\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward -83.9887964884201 , mean_reward:  -74.82570452069184 , time_score:  371 , memory:  64077\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward 27.460192549146264 , mean_reward:  -73.14481260024962 , time_score:  500 , memory:  66577\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward 105.70755692603652 , mean_reward:  -65.70521439132973 , time_score:  500 , memory:  68507\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward 63.51678979013413 , mean_reward:  -59.58435255332603 , time_score:  500 , memory:  70594\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward -27.417325762423218 , mean_reward:  -54.76177751723546 , time_score:  272 , memory:  72305\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward -39.59894947718109 , mean_reward:  -53.44482960456516 , time_score:  500 , memory:  74805\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward 29.751990106395123 , mean_reward:  -45.58537179226598 , time_score:  500 , memory:  77305\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward 9.059887436530166 , mean_reward:  -40.99451418361323 , time_score:  500 , memory:  79588\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward -68.45257051860195 , mean_reward:  -38.05517799075601 , time_score:  210 , memory:  81723\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward -54.88838619886812 , mean_reward:  -29.305024153631184 , time_score:  317 , memory:  83890\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward -60.595627648212826 , mean_reward:  -24.535892373905437 , time_score:  496 , memory:  86386\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward 11.312324069733455 , mean_reward:  -20.83612890431529 , time_score:  500 , memory:  88318\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward 8.145662538302714 , mean_reward:  -14.505248786112578 , time_score:  500 , memory:  90818\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward -7.757278611839017 , mean_reward:  -12.183818315810218 , time_score:  500 , memory:  93163\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward 107.171936040527 , mean_reward:  -8.517863283868008 , time_score:  500 , memory:  95541\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward 20.457124659491566 , mean_reward:  -4.848416025157035 , time_score:  500 , memory:  98041\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward 14.364432048293517 , mean_reward:  -2.1777250275444184 , time_score:  500 , memory:  100541\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward 118.19070922098587 , mean_reward:  0.3403045249607473 , time_score:  500 , memory:  103041\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward 14.11618004980663 , mean_reward:  5.665942752759536 , time_score:  500 , memory:  105541\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward 19.019601089976867 , mean_reward:  9.472227483088039 , time_score:  500 , memory:  108041\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward 61.753197959600044 , mean_reward:  9.075293910860646 , time_score:  500 , memory:  110352\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward 14.754059360955548 , mean_reward:  10.105379011573703 , time_score:  500 , memory:  112852\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward 6.552909480616917 , mean_reward:  9.389675926894608 , time_score:  500 , memory:  115352\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward 18.48316919340564 , mean_reward:  9.972194525765952 , time_score:  500 , memory:  117852\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward 46.710183969355086 , mean_reward:  12.12039722229508 , time_score:  500 , memory:  120352\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward 9.732683589657082 , mean_reward:  14.044372247367207 , time_score:  500 , memory:  122852\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward 60.416661578540285 , mean_reward:  13.940106120346925 , time_score:  500 , memory:  125352\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward 43.40942129670776 , mean_reward:  15.174137203077658 , time_score:  500 , memory:  127852\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward 45.45422221935864 , mean_reward:  17.804322659109506 , time_score:  500 , memory:  130352\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward -7.669445372511554 , mean_reward:  17.99051042416764 , time_score:  500 , memory:  132852\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward 31.68227257538694 , mean_reward:  19.218005092955366 , time_score:  500 , memory:  135352\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward 6.177520952457746 , mean_reward:  20.1694997504607 , time_score:  500 , memory:  137852\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward 21.694474462104367 , mean_reward:  19.13024475466678 , time_score:  500 , memory:  140352\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward 19.90425170084008 , mean_reward:  20.023801382008244 , time_score:  500 , memory:  142852\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward -3.326574405818337 , mean_reward:  17.831933888966464 , time_score:  500 , memory:  145352\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward 75.84689616412703 , mean_reward:  18.959416217618895 , time_score:  500 , memory:  147852\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward -18.905655412817502 , mean_reward:  18.704132832215404 , time_score:  500 , memory:  150352\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward -8.124323977877204 , mean_reward:  18.083890194983017 , time_score:  500 , memory:  152852\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward 29.982870245734006 , mean_reward:  17.365802815333023 , time_score:  500 , memory:  155352\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward 15.303011697632792 , mean_reward:  17.845028899090863 , time_score:  500 , memory:  157852\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward 32.75159799010709 , mean_reward:  16.702966582591753 , time_score:  500 , memory:  160236\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward 85.75208671414507 , mean_reward:  17.356942683352017 , time_score:  500 , memory:  162736\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward -18.259054322508778 , mean_reward:  16.794932479875364 , time_score:  500 , memory:  165236\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward -19.63067854139049 , mean_reward:  17.040418927935093 , time_score:  500 , memory:  167736\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward -29.45210121510262 , mean_reward:  14.677028004684974 , time_score:  500 , memory:  170235\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward 2.1992170138443887 , mean_reward:  12.957099264845418 , time_score:  500 , memory:  172726\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward 46.456816381383994 , mean_reward:  13.233381411804448 , time_score:  500 , memory:  175226\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward 40.780628541647154 , mean_reward:  13.946707620560096 , time_score:  500 , memory:  177726\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward -94.07497372778383 , mean_reward:  13.408277750206702 , time_score:  420 , memory:  180146\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward -184.79149179045277 , mean_reward:  11.544901240463593 , time_score:  448 , memory:  182572\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward 29.205733850522083 , mean_reward:  8.831694092797987 , time_score:  500 , memory:  185028\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward 50.91291262662959 , mean_reward:  8.388441222680889 , time_score:  500 , memory:  187528\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward -98.15878231763745 , mean_reward:  7.720622062160893 , time_score:  378 , memory:  189906\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward 5.332749490402926 , mean_reward:  7.534264882320461 , time_score:  500 , memory:  192406\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward -123.39966588490167 , mean_reward:  5.250445217705586 , time_score:  419 , memory:  194628\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward 1.5890111139558747 , mean_reward:  5.3341475091503385 , time_score:  500 , memory:  197128\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward 15.046736425577969 , mean_reward:  4.459659060037025 , time_score:  500 , memory:  199628\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward 28.95156371785188 , mean_reward:  5.106693681331819 , time_score:  500 , memory:  202128\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward 29.886457982804465 , mean_reward:  5.221557970486661 , time_score:  500 , memory:  204628\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward -8.85036651547181 , mean_reward:  4.596479003195852 , time_score:  500 , memory:  207128\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward -7.221239783598489 , mean_reward:  3.600922306982922 , time_score:  500 , memory:  209602\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward 1.6571573048198804 , mean_reward:  3.041343905444849 , time_score:  500 , memory:  211886\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward 81.49447696629237 , mean_reward:  6.515504324951101 , time_score:  500 , memory:  214386\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward 56.01908148659149 , mean_reward:  8.166696961708293 , time_score:  500 , memory:  216886\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward 22.810144760577153 , mean_reward:  10.187710727595194 , time_score:  500 , memory:  219386\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward 39.73204749623925 , mean_reward:  10.79768344192102 , time_score:  500 , memory:  221869\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward 54.29562983185426 , mean_reward:  10.724580894655682 , time_score:  500 , memory:  224369\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward 63.12369069602521 , mean_reward:  10.603610128140911 , time_score:  500 , memory:  226869\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward 89.72567481127973 , mean_reward:  13.129207689494562 , time_score:  500 , memory:  229369\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward 77.15757553319322 , mean_reward:  18.036700875762037 , time_score:  500 , memory:  231869\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward 105.281737870635 , mean_reward:  22.502536481295547 , time_score:  500 , memory:  234369\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward 31.019813174985597 , mean_reward:  24.94975101848262 , time_score:  500 , memory:  236869\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward 3.4160750348870317 , mean_reward:  27.407578328276177 , time_score:  500 , memory:  239369\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward 69.4050418956178 , mean_reward:  29.150620511617394 , time_score:  500 , memory:  241869\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 119.21339088713736 , mean_reward:  34.520537958655126 , time_score:  500 , memory:  244369\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward 67.23869682095581 , mean_reward:  37.50992251007219 , time_score:  500 , memory:  246869\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward 24.829858885557204 , mean_reward:  40.10797918440946 , time_score:  500 , memory:  249369\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward 63.14715843919751 , mean_reward:  41.59116570111883 , time_score:  500 , memory:  251869\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward 93.38278822971893 , mean_reward:  42.44734800140909 , time_score:  500 , memory:  254369\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward 16.840091728895047 , mean_reward:  45.20791761121598 , time_score:  500 , memory:  256869\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward 7.008962803903173 , mean_reward:  48.710822299869335 , time_score:  500 , memory:  259369\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward 65.25466390236508 , mean_reward:  49.76977970808229 , time_score:  500 , memory:  261869\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward 70.81149519432303 , mean_reward:  49.27690678426301 , time_score:  500 , memory:  264369\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward 75.48055002896008 , mean_reward:  49.621740728212416 , time_score:  500 , memory:  266869\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward 6.942505687779544 , mean_reward:  51.94811701858157 , time_score:  500 , memory:  269369\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 74.15166432801522 , mean_reward:  54.49850498142484 , time_score:  500 , memory:  271869\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward 58.10200639756617 , mean_reward:  56.53074719935852 , time_score:  500 , memory:  274369\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 118.94321156862567 , mean_reward:  59.340972629800625 , time_score:  500 , memory:  276869\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward -15.78807037738871 , mean_reward:  59.47573280503604 , time_score:  500 , memory:  279369\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward 83.49437914706628 , mean_reward:  60.69951206273068 , time_score:  500 , memory:  281869\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 25.30108755328474 , mean_reward:  60.94383361156412 , time_score:  500 , memory:  284369\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward 112.87947923454082 , mean_reward:  62.186887885121834 , time_score:  500 , memory:  286869\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward 63.616463372797284 , mean_reward:  63.879835554685286 , time_score:  500 , memory:  289369\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward 13.790528159726648 , mean_reward:  63.7371720796869 , time_score:  500 , memory:  291735\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward 103.9706762049669 , mean_reward:  64.06097872387014 , time_score:  500 , memory:  294235\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward 102.31791249858989 , mean_reward:  62.634260788932444 , time_score:  500 , memory:  296735\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward 37.770429130790276 , mean_reward:  61.726732276136424 , time_score:  500 , memory:  299235\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward 114.59753736176405 , mean_reward:  62.426842505106414 , time_score:  500 , memory:  301735\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward -6.0794273325909165 , mean_reward:  63.755745172637816 , time_score:  500 , memory:  304235\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward 120.49631750472666 , mean_reward:  65.14016859834241 , time_score:  500 , memory:  306616\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward 95.61232798399647 , mean_reward:  66.84613611176297 , time_score:  500 , memory:  309116\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 87.01887969842863 , mean_reward:  67.3635303027873 , time_score:  500 , memory:  311616\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward -39.46685056560913 , mean_reward:  66.5824762613526 , time_score:  500 , memory:  314116\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward -17.221021643420105 , mean_reward:  65.46593441437766 , time_score:  500 , memory:  316616\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward 86.0073191355695 , mean_reward:  66.55261440207568 , time_score:  500 , memory:  319116\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward 90.70203240004415 , mean_reward:  67.56632404299422 , time_score:  500 , memory:  321616\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward 0.0411784346875641 , mean_reward:  67.01573026563715 , time_score:  500 , memory:  324116\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward 103.31848623971649 , mean_reward:  65.72930738566846 , time_score:  500 , memory:  326616\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward 41.71760930121104 , mean_reward:  65.61146674009031 , time_score:  500 , memory:  329116\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward 78.36610841080575 , mean_reward:  64.69753630844556 , time_score:  500 , memory:  331616\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward 114.19852851791731 , mean_reward:  64.25925820878975 , time_score:  500 , memory:  334116\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward 83.33701782212084 , mean_reward:  61.49829970122183 , time_score:  500 , memory:  336616\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward 80.14703689896857 , mean_reward:  61.3987462957889 , time_score:  500 , memory:  339116\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward -12.492939280772958 , mean_reward:  59.57839114387284 , time_score:  500 , memory:  341616\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward 36.856801396428125 , mean_reward:  58.4407826164661 , time_score:  500 , memory:  344116\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward -10.593349745401309 , mean_reward:  57.01491898497537 , time_score:  500 , memory:  346616\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward -20.348974739533414 , mean_reward:  54.903395821987964 , time_score:  500 , memory:  349116\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward 114.37227350030781 , mean_reward:  51.86407612002885 , time_score:  500 , memory:  351616\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward 108.63218331433576 , mean_reward:  50.658576463855894 , time_score:  500 , memory:  354116\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward 49.80416227970328 , mean_reward:  47.55791720021377 , time_score:  500 , memory:  356616\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward 121.07879478936466 , mean_reward:  46.338523103396504 , time_score:  500 , memory:  359116\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward 111.5035913067988 , mean_reward:  48.6170621011376 , time_score:  500 , memory:  361616\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward -11.615412859430354 , mean_reward:  47.00281528845874 , time_score:  500 , memory:  364116\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward 0.7845498996901674 , mean_reward:  46.841763107381254 , time_score:  500 , memory:  366616\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward -40.30000672044412 , mean_reward:  45.318280309664104 , time_score:  500 , memory:  369116\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward 0.22526056107234177 , mean_reward:  43.63034701068946 , time_score:  500 , memory:  371616\n",
      "Episode:  920  , Epsilon:  0.01 , Reward 99.83908404371917 , mean_reward:  42.502784443554255 , time_score:  500 , memory:  374116\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 103.26618621584491 , mean_reward:  42.022264316919255 , time_score:  500 , memory:  376616\n",
      "Episode:  930  , Epsilon:  0.01 , Reward -19.951735386008206 , mean_reward:  39.95281824477663 , time_score:  500 , memory:  379116\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 77.14397670061588 , mean_reward:  37.214249745958575 , time_score:  500 , memory:  381616\n",
      "Episode:  940  , Epsilon:  0.01 , Reward 127.9857399626957 , mean_reward:  37.47787381102355 , time_score:  500 , memory:  384116\n",
      "Episode:  945  , Epsilon:  0.01 , Reward 99.98959747255235 , mean_reward:  38.21643202724478 , time_score:  500 , memory:  386616\n",
      "Episode:  950  , Epsilon:  0.01 , Reward -8.357547195829284 , mean_reward:  34.21431417273611 , time_score:  500 , memory:  389116\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 89.83410230477882 , mean_reward:  35.51032360635828 , time_score:  500 , memory:  391616\n",
      "Episode:  960  , Epsilon:  0.01 , Reward 41.139954678190875 , mean_reward:  33.527436092586434 , time_score:  500 , memory:  394116\n",
      "Episode:  965  , Epsilon:  0.01 , Reward 71.73890710734074 , mean_reward:  35.4633290841338 , time_score:  500 , memory:  396616\n",
      "Episode:  970  , Epsilon:  0.01 , Reward 69.59883715386324 , mean_reward:  37.68868889812051 , time_score:  500 , memory:  399116\n",
      "Episode:  975  , Epsilon:  0.01 , Reward 54.97701395537 , mean_reward:  38.120612074757574 , time_score:  500 , memory:  401616\n",
      "Episode:  980  , Epsilon:  0.01 , Reward 87.14591419296978 , mean_reward:  37.176030195990336 , time_score:  500 , memory:  404116\n",
      "Episode:  985  , Epsilon:  0.01 , Reward 96.78556090427247 , mean_reward:  38.91573790724952 , time_score:  500 , memory:  406616\n",
      "Episode:  990  , Epsilon:  0.01 , Reward 57.18329757295567 , mean_reward:  38.121115993408985 , time_score:  500 , memory:  409116\n",
      "Episode:  995  , Epsilon:  0.01 , Reward 102.77011245725858 , mean_reward:  34.75452867503282 , time_score:  500 , memory:  411616\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward 100.04616710001638 , mean_reward:  37.92006745777317 , time_score:  500 , memory:  414116\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward 6.116189314307368 , mean_reward:  38.49070877466816 , time_score:  500 , memory:  416616\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward 34.48883060914557 , mean_reward:  37.25694219301175 , time_score:  500 , memory:  419116\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward 60.296759950659535 , mean_reward:  37.030322835254175 , time_score:  500 , memory:  421616\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward -24.74842967450702 , mean_reward:  36.24100232305742 , time_score:  500 , memory:  424116\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward 91.31911495351584 , mean_reward:  35.01704662093095 , time_score:  500 , memory:  426616\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward -35.943697701965064 , mean_reward:  37.31457882983598 , time_score:  500 , memory:  429116\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward 78.27047831754977 , mean_reward:  39.587542803349955 , time_score:  500 , memory:  431616\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward -15.208851344725115 , mean_reward:  37.99676030001278 , time_score:  500 , memory:  434116\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward -33.522052714063086 , mean_reward:  34.719700789589645 , time_score:  500 , memory:  436616\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward 87.15062688106154 , mean_reward:  36.30411548001433 , time_score:  500 , memory:  439116\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward -22.011411499721614 , mean_reward:  35.420061865625165 , time_score:  500 , memory:  441616\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward 97.29734629408745 , mean_reward:  35.23397022058539 , time_score:  500 , memory:  444116\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward 1.5862410423028235 , mean_reward:  32.06328501677703 , time_score:  500 , memory:  446616\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward 94.71052851951852 , mean_reward:  31.252588112266917 , time_score:  500 , memory:  449116\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward 117.85731416959523 , mean_reward:  32.796077173065314 , time_score:  500 , memory:  451616\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward 13.953344969706082 , mean_reward:  33.29616470938217 , time_score:  500 , memory:  454116\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward -35.01833706575385 , mean_reward:  31.263984264740934 , time_score:  500 , memory:  456616\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward -47.63628005660155 , mean_reward:  28.392792455966585 , time_score:  500 , memory:  459116\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward 39.81449805180017 , mean_reward:  28.08223546380084 , time_score:  500 , memory:  461616\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward 60.57234571520637 , mean_reward:  25.83651455831538 , time_score:  500 , memory:  464116\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward -68.30397837347438 , mean_reward:  23.92180844081289 , time_score:  500 , memory:  466616\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward 65.0472283597854 , mean_reward:  24.33050596323369 , time_score:  500 , memory:  469116\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward 72.03955439015682 , mean_reward:  24.648995410934955 , time_score:  500 , memory:  471616\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward 69.45965049361038 , mean_reward:  23.767986465554436 , time_score:  500 , memory:  474116\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward -25.27761507856067 , mean_reward:  23.94065989504242 , time_score:  500 , memory:  476616\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward 93.57467714244807 , mean_reward:  23.042791581910052 , time_score:  500 , memory:  479116\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward 77.12742985224668 , mean_reward:  21.39713449204347 , time_score:  500 , memory:  481616\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward 64.86821267329509 , mean_reward:  21.969425254577736 , time_score:  500 , memory:  484116\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward 51.74344141735151 , mean_reward:  24.759622931574015 , time_score:  500 , memory:  486616\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward -43.17381854886149 , mean_reward:  25.385859471696232 , time_score:  500 , memory:  489116\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward 104.47533278312886 , mean_reward:  25.807009209503946 , time_score:  500 , memory:  491616\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward 86.72288739999554 , mean_reward:  28.78212470347239 , time_score:  500 , memory:  494116\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 20.96467053417644 , mean_reward:  30.41859318385029 , time_score:  500 , memory:  496616\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward -1.8243170362717598 , mean_reward:  30.821391746704702 , time_score:  500 , memory:  499116\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward 25.24785165473794 , mean_reward:  30.416563825614794 , time_score:  500 , memory:  501616\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward 56.229856065134015 , mean_reward:  30.977575901533584 , time_score:  500 , memory:  504116\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward 111.87201426641876 , mean_reward:  33.34794395689295 , time_score:  500 , memory:  506616\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward 36.852174438526305 , mean_reward:  35.845086676793045 , time_score:  500 , memory:  509116\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward 12.069134171402904 , mean_reward:  36.72449097672834 , time_score:  500 , memory:  511616\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward 112.2542862866525 , mean_reward:  37.575276610598 , time_score:  500 , memory:  514116\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward 68.57041774771199 , mean_reward:  39.0323610115684 , time_score:  500 , memory:  516616\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 106.4055836453365 , mean_reward:  40.149215462131565 , time_score:  500 , memory:  519116\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward 29.017098976614918 , mean_reward:  40.62142147706467 , time_score:  500 , memory:  521616\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward 57.139594157470874 , mean_reward:  42.046830776499256 , time_score:  500 , memory:  524116\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward 86.44270009978177 , mean_reward:  44.7548752039772 , time_score:  500 , memory:  526616\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward -13.387615362375998 , mean_reward:  44.3621926063015 , time_score:  500 , memory:  529116\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward 25.51368840786316 , mean_reward:  44.43819983411302 , time_score:  500 , memory:  531616\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 85.67412413557842 , mean_reward:  44.96570004826227 , time_score:  500 , memory:  534116\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward 59.754370578538335 , mean_reward:  44.841381628729714 , time_score:  500 , memory:  536616\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 19.751354179330992 , mean_reward:  44.72820971986175 , time_score:  500 , memory:  539116\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward 61.8999686302921 , mean_reward:  46.68244243510644 , time_score:  500 , memory:  541616\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward 44.213323788962335 , mean_reward:  45.31810227536941 , time_score:  500 , memory:  544116\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 98.39006377733293 , mean_reward:  45.640092314916465 , time_score:  500 , memory:  546616\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward 16.13916053495446 , mean_reward:  45.40632103397405 , time_score:  500 , memory:  549116\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward 57.25906344071634 , mean_reward:  46.39459272465834 , time_score:  500 , memory:  551616\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward 11.77403213690663 , mean_reward:  47.92650991860523 , time_score:  500 , memory:  554116\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward 1.7719176363657745 , mean_reward:  46.04389407801565 , time_score:  500 , memory:  556616\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward 122.00263558455578 , mean_reward:  47.63017420608156 , time_score:  500 , memory:  559116\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward 64.30481881855725 , mean_reward:  48.308413686739634 , time_score:  500 , memory:  561616\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward 80.55663436696773 , mean_reward:  48.799886345100404 , time_score:  500 , memory:  564116\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward 74.17550833186563 , mean_reward:  47.53738522091989 , time_score:  500 , memory:  566616\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward -13.822612697760944 , mean_reward:  44.92451539148114 , time_score:  500 , memory:  569116\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward -16.867658433884543 , mean_reward:  45.59222652349363 , time_score:  500 , memory:  571616\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward 106.34445263298102 , mean_reward:  45.752885312631115 , time_score:  500 , memory:  574116\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward 65.75843532167244 , mean_reward:  42.52491545892947 , time_score:  500 , memory:  576616\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward 8.356303852446164 , mean_reward:  40.44269964380455 , time_score:  500 , memory:  579116\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward 101.12523364958273 , mean_reward:  42.43826702610388 , time_score:  500 , memory:  581616\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward 53.905513038815926 , mean_reward:  43.34651682810948 , time_score:  500 , memory:  584116\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward 38.568638117037096 , mean_reward:  44.6483541460851 , time_score:  500 , memory:  586616\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward 81.49669791917168 , mean_reward:  45.84705046184721 , time_score:  500 , memory:  589116\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward 264.26029988724474 , mean_reward:  48.04117885604607 , time_score:  455 , memory:  591571\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward 100.03243814329801 , mean_reward:  49.321070522081925 , time_score:  500 , memory:  594071\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward 75.8351292164515 , mean_reward:  49.44861891692749 , time_score:  500 , memory:  596571\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward 89.14835288395655 , mean_reward:  51.663855168939236 , time_score:  500 , memory:  599071\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward 71.46217674033204 , mean_reward:  51.965443601434515 , time_score:  500 , memory:  601571\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward 18.088174760551336 , mean_reward:  50.1834354746319 , time_score:  500 , memory:  604071\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward 72.97090882641658 , mean_reward:  49.97291527990277 , time_score:  500 , memory:  606571\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward 49.43591030751104 , mean_reward:  49.083178362846596 , time_score:  500 , memory:  609071\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward 101.66481895602234 , mean_reward:  49.9605620615566 , time_score:  500 , memory:  611571\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward 6.582973656182965 , mean_reward:  48.290871263171255 , time_score:  500 , memory:  614071\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward 76.74410442351804 , mean_reward:  50.851226253058165 , time_score:  500 , memory:  616571\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward 83.30581096089438 , mean_reward:  54.038506307666104 , time_score:  500 , memory:  619071\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward 91.08717354772489 , mean_reward:  53.85187823315379 , time_score:  500 , memory:  621571\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward 101.63264210745358 , mean_reward:  55.35534990150089 , time_score:  500 , memory:  624071\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward 50.10360839721594 , mean_reward:  58.06381299113932 , time_score:  500 , memory:  626571\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward 25.863842691201413 , mean_reward:  59.906317857865325 , time_score:  500 , memory:  629071\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward 10.011107935115064 , mean_reward:  59.79639380536028 , time_score:  500 , memory:  631571\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward 39.940736205394266 , mean_reward:  59.17132288594016 , time_score:  500 , memory:  634071\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward 39.1446973061924 , mean_reward:  58.924425762518005 , time_score:  500 , memory:  636571\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward 36.06915113232914 , mean_reward:  58.214474909944656 , time_score:  500 , memory:  639071\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward 89.56002840397342 , mean_reward:  54.72206617665581 , time_score:  500 , memory:  641524\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward 90.61215987909496 , mean_reward:  54.60303991941165 , time_score:  500 , memory:  644024\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward 95.13606682457663 , mean_reward:  55.59932063711155 , time_score:  500 , memory:  646524\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward 9.70170946338591 , mean_reward:  54.62168938598023 , time_score:  500 , memory:  649024\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward 71.05553310484235 , mean_reward:  55.20483039891488 , time_score:  500 , memory:  651524\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward 44.6128570935122 , mean_reward:  56.30762029684469 , time_score:  500 , memory:  654024\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward 26.156790630886192 , mean_reward:  56.691139343290416 , time_score:  500 , memory:  656524\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward 95.1491807379004 , mean_reward:  56.1064413519496 , time_score:  500 , memory:  659024\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward 123.24234189938834 , mean_reward:  55.62412892159305 , time_score:  500 , memory:  661524\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward 21.202009706155767 , mean_reward:  57.43675692492866 , time_score:  500 , memory:  664024\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward 80.71948947322427 , mean_reward:  57.54405275310848 , time_score:  500 , memory:  666524\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward 76.80954356217018 , mean_reward:  56.97472201368451 , time_score:  500 , memory:  669024\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward 21.4495963148202 , mean_reward:  56.86241188150035 , time_score:  500 , memory:  671524\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward 69.14201054743211 , mean_reward:  56.02013800618094 , time_score:  500 , memory:  674024\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward 6.526954876700287 , mean_reward:  54.99176697338362 , time_score:  500 , memory:  676524\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward 79.2367913782686 , mean_reward:  56.54964299499972 , time_score:  500 , memory:  679024\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward 33.738421716229496 , mean_reward:  57.050335853018595 , time_score:  500 , memory:  681524\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward 48.19418533899532 , mean_reward:  57.62805033052347 , time_score:  500 , memory:  684024\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward 78.15903374234773 , mean_reward:  57.346519616589106 , time_score:  500 , memory:  686524\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward 111.28069155140997 , mean_reward:  58.15647342294912 , time_score:  500 , memory:  689024\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward 61.124325833514746 , mean_reward:  57.641866321632094 , time_score:  500 , memory:  691524\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward 95.79271830166351 , mean_reward:  58.41088164002715 , time_score:  500 , memory:  694024\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward 42.12468156880468 , mean_reward:  57.125128510973866 , time_score:  500 , memory:  696524\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward 62.035668511095565 , mean_reward:  58.06001906362408 , time_score:  500 , memory:  699024\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward 52.21712644400395 , mean_reward:  56.39340540972443 , time_score:  500 , memory:  701524\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward 37.872725405258315 , mean_reward:  56.44690675497873 , time_score:  500 , memory:  704024\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward 99.35025842614844 , mean_reward:  58.5521412738262 , time_score:  500 , memory:  706524\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward 55.19283915072034 , mean_reward:  58.80106927998622 , time_score:  500 , memory:  709024\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward 92.68509604086272 , mean_reward:  58.22025075755262 , time_score:  500 , memory:  711524\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward 11.53079883684173 , mean_reward:  57.92327358419845 , time_score:  500 , memory:  714024\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward 10.818115151850966 , mean_reward:  57.96191300580384 , time_score:  500 , memory:  716524\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward 62.32679707759959 , mean_reward:  58.462676360889866 , time_score:  500 , memory:  719024\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward 50.20727975478952 , mean_reward:  59.727358717006375 , time_score:  500 , memory:  721524\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward 45.40867079582533 , mean_reward:  58.951366597975436 , time_score:  500 , memory:  724024\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward 48.99322184875739 , mean_reward:  58.790561604322 , time_score:  500 , memory:  726524\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward 53.682483195155264 , mean_reward:  58.859665718464264 , time_score:  500 , memory:  729024\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward 79.54707161706705 , mean_reward:  57.90968265310412 , time_score:  500 , memory:  731524\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward 68.39295505512511 , mean_reward:  57.28052420514996 , time_score:  500 , memory:  734024\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward 34.21634569990573 , mean_reward:  56.826612071939024 , time_score:  500 , memory:  736524\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 58.11320668267486 , mean_reward:  56.502064029369336 , time_score:  500 , memory:  739024\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward -11.673460438006385 , mean_reward:  56.70281882264787 , time_score:  500 , memory:  741524\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward 43.84158546613818 , mean_reward:  56.02203177432122 , time_score:  500 , memory:  744024\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward 30.9255599723276 , mean_reward:  56.74213378907123 , time_score:  500 , memory:  746524\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward 94.31365009032478 , mean_reward:  56.154258006830624 , time_score:  500 , memory:  749024\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward 66.30779193072301 , mean_reward:  55.62238440284776 , time_score:  500 , memory:  751524\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward 16.106440867853372 , mean_reward:  55.45841252165457 , time_score:  500 , memory:  754024\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward 46.30993013718559 , mean_reward:  53.7704697746389 , time_score:  500 , memory:  756524\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward 79.42710737048675 , mean_reward:  54.61599413275017 , time_score:  500 , memory:  759024\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward 67.99778415752398 , mean_reward:  54.45689587819736 , time_score:  500 , memory:  761524\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward 25.907563086689866 , mean_reward:  53.6606790844648 , time_score:  500 , memory:  764024\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward -14.336763478791969 , mean_reward:  53.66715538675668 , time_score:  500 , memory:  766524\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward 100.58733933433194 , mean_reward:  53.8823145621192 , time_score:  500 , memory:  769024\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward 72.73030068654033 , mean_reward:  53.34432346212671 , time_score:  500 , memory:  771524\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward 26.254861618930207 , mean_reward:  54.09423164718558 , time_score:  500 , memory:  774024\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward 85.52427600147954 , mean_reward:  54.58885983575865 , time_score:  500 , memory:  776524\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward 63.6752113326543 , mean_reward:  54.085782763196455 , time_score:  500 , memory:  779024\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward 46.6536684758465 , mean_reward:  53.996363353555736 , time_score:  500 , memory:  781524\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward 98.60307249161572 , mean_reward:  55.01115532254903 , time_score:  500 , memory:  784024\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward 98.0793409005508 , mean_reward:  56.74457861620638 , time_score:  500 , memory:  786524\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward 50.592074450614 , mean_reward:  56.23493112458114 , time_score:  500 , memory:  789024\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward -4.808974378973062 , mean_reward:  56.445392687005835 , time_score:  500 , memory:  791524\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward 117.81988055810656 , mean_reward:  56.60283424231927 , time_score:  500 , memory:  794024\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward 32.020286803404204 , mean_reward:  56.88205544377324 , time_score:  500 , memory:  796524\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward 74.21198503682413 , mean_reward:  56.75353396251644 , time_score:  500 , memory:  799024\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward 32.84556971556255 , mean_reward:  57.10318815869762 , time_score:  500 , memory:  801524\n",
      "Episode:  1780  , Epsilon:  0.01 , Reward 29.69613007344349 , mean_reward:  57.44902656529799 , time_score:  500 , memory:  804024\n",
      "Episode:  1785  , Epsilon:  0.01 , Reward 64.50383742373268 , mean_reward:  57.011908065966985 , time_score:  500 , memory:  806524\n",
      "Episode:  1790  , Epsilon:  0.01 , Reward 42.52782494978011 , mean_reward:  57.04236972858276 , time_score:  500 , memory:  809024\n",
      "Episode:  1795  , Epsilon:  0.01 , Reward 53.22470311856452 , mean_reward:  56.91796937117802 , time_score:  500 , memory:  811524\n",
      "Episode:  1800  , Epsilon:  0.01 , Reward 49.12523352169278 , mean_reward:  58.54575559228504 , time_score:  500 , memory:  814024\n",
      "Episode:  1805  , Epsilon:  0.01 , Reward 51.238686874296235 , mean_reward:  57.11437144635011 , time_score:  500 , memory:  816524\n",
      "Episode:  1810  , Epsilon:  0.01 , Reward 68.04979053417009 , mean_reward:  56.311833113391465 , time_score:  500 , memory:  819024\n",
      "Episode:  1815  , Epsilon:  0.01 , Reward -5.402848185481625 , mean_reward:  54.812420707729096 , time_score:  500 , memory:  821524\n",
      "Episode:  1820  , Epsilon:  0.01 , Reward 31.08136228525618 , mean_reward:  54.482224858475426 , time_score:  500 , memory:  824024\n",
      "Episode:  1825  , Epsilon:  0.01 , Reward 80.38659750659974 , mean_reward:  55.29852902554054 , time_score:  500 , memory:  826524\n",
      "Episode:  1830  , Epsilon:  0.01 , Reward 36.360830274287935 , mean_reward:  54.97438985220058 , time_score:  500 , memory:  829024\n",
      "Episode:  1835  , Epsilon:  0.01 , Reward 52.70239804923516 , mean_reward:  55.15331814234407 , time_score:  500 , memory:  831524\n",
      "Episode:  1840  , Epsilon:  0.01 , Reward 102.82781625931028 , mean_reward:  53.66846058094961 , time_score:  500 , memory:  834024\n",
      "Episode:  1845  , Epsilon:  0.01 , Reward 52.71451841418593 , mean_reward:  52.543464367519135 , time_score:  500 , memory:  836524\n",
      "Episode:  1850  , Epsilon:  0.01 , Reward 60.414857433779176 , mean_reward:  51.964847478636095 , time_score:  500 , memory:  839024\n",
      "Episode:  1855  , Epsilon:  0.01 , Reward 13.512214516067543 , mean_reward:  53.1804373088603 , time_score:  500 , memory:  841524\n",
      "Episode:  1860  , Epsilon:  0.01 , Reward 74.4313606414247 , mean_reward:  53.3250180651165 , time_score:  500 , memory:  844024\n",
      "Episode:  1865  , Epsilon:  0.01 , Reward 93.19991768184977 , mean_reward:  54.745652617986146 , time_score:  500 , memory:  846524\n",
      "Episode:  1870  , Epsilon:  0.01 , Reward 62.216381888625236 , mean_reward:  54.52046467199827 , time_score:  500 , memory:  849024\n",
      "Episode:  1875  , Epsilon:  0.01 , Reward 61.17772943113324 , mean_reward:  54.96623795722214 , time_score:  500 , memory:  851524\n",
      "Episode:  1880  , Epsilon:  0.01 , Reward 117.87314870932221 , mean_reward:  54.580456800308966 , time_score:  500 , memory:  854024\n",
      "Episode:  1885  , Epsilon:  0.01 , Reward 31.595163929748765 , mean_reward:  54.31211064794648 , time_score:  500 , memory:  856524\n",
      "Episode:  1890  , Epsilon:  0.01 , Reward 102.64970884916342 , mean_reward:  55.18597374001243 , time_score:  500 , memory:  859024\n",
      "Episode:  1895  , Epsilon:  0.01 , Reward 32.16290930184433 , mean_reward:  55.154945351982406 , time_score:  500 , memory:  861524\n",
      "Episode:  1900  , Epsilon:  0.01 , Reward 86.11762558850363 , mean_reward:  55.66480056800778 , time_score:  500 , memory:  864024\n",
      "Episode:  1905  , Epsilon:  0.01 , Reward 103.06032277459401 , mean_reward:  57.2216048137793 , time_score:  500 , memory:  866524\n",
      "Episode:  1910  , Epsilon:  0.01 , Reward 103.58233468277818 , mean_reward:  58.45795326239279 , time_score:  500 , memory:  869024\n",
      "Episode:  1915  , Epsilon:  0.01 , Reward 85.15574953640065 , mean_reward:  59.09614921983097 , time_score:  500 , memory:  871524\n",
      "Episode:  1920  , Epsilon:  0.01 , Reward 113.32555147671248 , mean_reward:  60.87322638284337 , time_score:  500 , memory:  874024\n",
      "Episode:  1925  , Epsilon:  0.01 , Reward 86.3056505986535 , mean_reward:  59.55461699719535 , time_score:  500 , memory:  876524\n",
      "Episode:  1930  , Epsilon:  0.01 , Reward 95.66307108527127 , mean_reward:  60.05695374234159 , time_score:  500 , memory:  879024\n",
      "Episode:  1935  , Epsilon:  0.01 , Reward 101.22079206870185 , mean_reward:  61.32913696801691 , time_score:  500 , memory:  881524\n",
      "Episode:  1940  , Epsilon:  0.01 , Reward 85.59508319380649 , mean_reward:  62.40212320404467 , time_score:  500 , memory:  884024\n",
      "Episode:  1945  , Epsilon:  0.01 , Reward 42.755922982772816 , mean_reward:  62.953962087589524 , time_score:  500 , memory:  886524\n",
      "Episode:  1950  , Epsilon:  0.01 , Reward 32.333868608012146 , mean_reward:  63.663588130479 , time_score:  500 , memory:  889024\n",
      "Episode:  1955  , Epsilon:  0.01 , Reward 83.45324761811973 , mean_reward:  63.15810855989791 , time_score:  500 , memory:  891524\n",
      "Episode:  1960  , Epsilon:  0.01 , Reward 73.73159810401532 , mean_reward:  62.70002728035641 , time_score:  500 , memory:  894024\n",
      "Episode:  1965  , Epsilon:  0.01 , Reward 44.595889432795815 , mean_reward:  61.49011696960691 , time_score:  500 , memory:  896524\n",
      "Episode:  1970  , Epsilon:  0.01 , Reward 107.396349883025 , mean_reward:  63.26641941076416 , time_score:  500 , memory:  899024\n",
      "Episode:  1975  , Epsilon:  0.01 , Reward 44.922078870662624 , mean_reward:  64.97215798026235 , time_score:  500 , memory:  901524\n",
      "Episode:  1980  , Epsilon:  0.01 , Reward 34.01137577253666 , mean_reward:  65.571316606146 , time_score:  500 , memory:  904024\n",
      "Episode:  1985  , Epsilon:  0.01 , Reward 88.2981639732936 , mean_reward:  67.62812368626592 , time_score:  500 , memory:  906524\n",
      "Episode:  1990  , Epsilon:  0.01 , Reward 94.58995039240845 , mean_reward:  67.47770309996493 , time_score:  500 , memory:  909024\n",
      "Episode:  1995  , Epsilon:  0.01 , Reward 83.88159902264044 , mean_reward:  68.7769583623471 , time_score:  500 , memory:  911524\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.98, episodes=2000, alpha = 0.001, lr=0.0001)\n",
    "df = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zEK6_8NkZvY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzXeEPyZkZx5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
