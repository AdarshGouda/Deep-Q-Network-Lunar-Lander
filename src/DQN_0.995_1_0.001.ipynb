{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_0.995_1_0.001.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"mount_file_id":"1Td8hC0tNt76Y_lsdQd-GP0O1LPQLEFrU","authorship_tag":"ABX9TyMDOh+zu8YRy7fohAUYFk7/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624646988051,"user_tz":360,"elapsed":16491,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"1cefdee5-80c7-49ca-8d70-b243c129bfaf"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5tkqTWOGQnGa"},"source":["#COMPLETED"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624647648745,"user_tz":360,"elapsed":5217,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"21f789f2-635e-4d71-e321-20605dc7799b"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cN9nvke9tZtk"},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8"},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/1.0_0.001/1p0_0p001.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/1.0_0.001/1p0_0p001.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624654655363,"user_tz":360,"elapsed":6993109,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"e1c49036-f5f7-4159-e314-c1130b9f6b94"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 1.0, lr=0.001)\n","df = dqn.play()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -183.38526714717452 , mean_reward:  -183.38526714717452 , time_score:  82 , memory:  82\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -87.06033457224623 , mean_reward:  -156.07084070114408 , time_score:  57 , memory:  537\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -273.3636857219287 , mean_reward:  -181.58242720451497 , time_score:  101 , memory:  1003\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -73.47309939885253 , mean_reward:  -175.0449103879584 , time_score:  64 , memory:  1454\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward -178.59851754757437 , mean_reward:  -161.51874477397436 , time_score:  99 , memory:  1912\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -84.03795767951658 , mean_reward:  -151.2411104059266 , time_score:  88 , memory:  2401\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -137.54684523216798 , mean_reward:  -154.02157837513167 , time_score:  79 , memory:  2868\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -112.16023112794353 , mean_reward:  -150.20247057783396 , time_score:  77 , memory:  3338\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -128.1301725788556 , mean_reward:  -142.4595142313809 , time_score:  128 , memory:  3857\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -60.301941369697516 , mean_reward:  -141.57876872424998 , time_score:  130 , memory:  4424\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -74.97376932916652 , mean_reward:  -140.40050951755043 , time_score:  72 , memory:  4927\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -72.72705576509117 , mean_reward:  -136.69446346266713 , time_score:  82 , memory:  5415\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -54.04626115235632 , mean_reward:  -131.7354206571316 , time_score:  81 , memory:  5949\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -55.50490426905844 , mean_reward:  -127.62269429483727 , time_score:  93 , memory:  6372\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -107.59193339178225 , mean_reward:  -124.19997120070627 , time_score:  125 , memory:  6955\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward -78.91745680113027 , mean_reward:  -121.67521699845685 , time_score:  151 , memory:  7522\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -40.99884768791905 , mean_reward:  -116.87030336621012 , time_score:  124 , memory:  8125\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -83.78618380772514 , mean_reward:  -114.26972759938656 , time_score:  135 , memory:  8811\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -173.41328420527924 , mean_reward:  -114.00992377923461 , time_score:  132 , memory:  9468\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -51.19780196885125 , mean_reward:  -110.53037873277788 , time_score:  88 , memory:  10221\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward -36.82064224025919 , mean_reward:  -107.74277185277283 , time_score:  184 , memory:  11008\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -51.89482591789912 , mean_reward:  -106.28130056608585 , time_score:  128 , memory:  11665\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward -45.88249593428289 , mean_reward:  -96.46711973612173 , time_score:  500 , memory:  12797\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward 25.45689776312105 , mean_reward:  -88.52469572269716 , time_score:  122 , memory:  13943\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -37.697190217767044 , mean_reward:  -85.60177041506554 , time_score:  260 , memory:  14949\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward -55.97749138382731 , mean_reward:  -84.53244158041052 , time_score:  160 , memory:  15811\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -17.36233819860341 , mean_reward:  -81.94726018188507 , time_score:  96 , memory:  16740\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -110.93616611389999 , mean_reward:  -81.13640128844987 , time_score:  156 , memory:  17881\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -83.95137969850711 , mean_reward:  -80.55045467934906 , time_score:  218 , memory:  18932\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -54.10886658058797 , mean_reward:  -76.76784367861843 , time_score:  141 , memory:  20254\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward -267.043146175855 , mean_reward:  -74.0631742693314 , time_score:  379 , memory:  22182\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward -16.986076114315644 , mean_reward:  -69.67079568535445 , time_score:  500 , memory:  23907\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward 9.968761686846722 , mean_reward:  -66.85131844225747 , time_score:  500 , memory:  25642\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward -38.09749288260248 , mean_reward:  -62.86627636493196 , time_score:  193 , memory:  27478\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward 4.853038365660794 , mean_reward:  -59.37902099453927 , time_score:  162 , memory:  28438\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward 62.80441722305371 , mean_reward:  -55.190330361704525 , time_score:  500 , memory:  30368\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward 27.519683837503333 , mean_reward:  -53.43982761321301 , time_score:  500 , memory:  32245\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward 14.33515109646655 , mean_reward:  -49.55509944100296 , time_score:  500 , memory:  34331\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward 80.58025532500344 , mean_reward:  -41.76312502929357 , time_score:  500 , memory:  36508\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward 6.1922026565952555 , mean_reward:  -36.97752037779567 , time_score:  500 , memory:  38671\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward 46.30188332748824 , mean_reward:  -30.113622254585813 , time_score:  171 , memory:  40663\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward 72.29154246180175 , mean_reward:  -19.97763507732601 , time_score:  500 , memory:  43163\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward 19.76872355933952 , mean_reward:  -19.160141703169003 , time_score:  119 , memory:  45101\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward -71.90576037038069 , mean_reward:  -16.569738163163656 , time_score:  500 , memory:  47601\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward 73.73205485912641 , mean_reward:  -9.411349229535658 , time_score:  500 , memory:  50101\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward 102.28548148576235 , mean_reward:  -6.635709651907055 , time_score:  500 , memory:  52115\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward 2.669306202832913 , mean_reward:  -2.519385430966621 , time_score:  500 , memory:  54053\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward 57.27180769385204 , mean_reward:  5.254637574888541 , time_score:  500 , memory:  56553\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward -0.7613387893200212 , mean_reward:  8.162039163610013 , time_score:  163 , memory:  58715\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward 44.67900715176389 , mean_reward:  10.64657489313138 , time_score:  500 , memory:  61138\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward -16.51326516756706 , mean_reward:  16.444664993541053 , time_score:  500 , memory:  63638\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward 41.88886812502807 , mean_reward:  19.220206025757076 , time_score:  500 , memory:  66138\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward 76.45418787385185 , mean_reward:  24.92373238598379 , time_score:  500 , memory:  68638\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward 89.56607859712604 , mean_reward:  24.835776212659326 , time_score:  500 , memory:  70593\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward -21.74218438885798 , mean_reward:  26.098268076987466 , time_score:  117 , memory:  72710\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward 46.42303193584553 , mean_reward:  23.943353911740996 , time_score:  500 , memory:  74480\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward 76.05477617415693 , mean_reward:  24.87065072259238 , time_score:  500 , memory:  76816\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward 85.24455479269483 , mean_reward:  29.0371495295061 , time_score:  500 , memory:  79251\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward 100.97595330493637 , mean_reward:  29.186941854332677 , time_score:  500 , memory:  81359\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward 96.93314289695728 , mean_reward:  31.164945389433015 , time_score:  500 , memory:  83859\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward 45.33680975848232 , mean_reward:  29.721329688861434 , time_score:  500 , memory:  86359\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward 54.84135757058529 , mean_reward:  28.899578333682143 , time_score:  500 , memory:  88859\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward 54.17862797868137 , mean_reward:  32.24264431361516 , time_score:  500 , memory:  91359\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward 101.89040882886763 , mean_reward:  30.981462404769463 , time_score:  500 , memory:  93576\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward 63.05813064531285 , mean_reward:  30.645719654283198 , time_score:  500 , memory:  96076\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward 99.80657594887809 , mean_reward:  35.656304698215614 , time_score:  500 , memory:  98576\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward 80.83859636996 , mean_reward:  38.520077719670454 , time_score:  500 , memory:  100944\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward 255.06676102551165 , mean_reward:  41.74029284860957 , time_score:  267 , memory:  103211\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward -19.007899859271376 , mean_reward:  46.413573940183475 , time_score:  500 , memory:  105711\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward 77.2125656473704 , mean_reward:  50.33513286751748 , time_score:  500 , memory:  108211\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward 81.67095663529344 , mean_reward:  52.500936271700766 , time_score:  500 , memory:  110711\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward 121.29233910049905 , mean_reward:  54.23205019008532 , time_score:  500 , memory:  112847\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward -33.629628315891395 , mean_reward:  50.26504332426035 , time_score:  500 , memory:  115000\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward 60.317325161749366 , mean_reward:  53.44528283539276 , time_score:  500 , memory:  117500\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward 19.053772750900063 , mean_reward:  55.29356544710855 , time_score:  500 , memory:  120000\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward 47.142999893927495 , mean_reward:  62.1865364724632 , time_score:  500 , memory:  122438\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward 89.32608957877343 , mean_reward:  65.45087099899223 , time_score:  500 , memory:  124938\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward 37.1043363859973 , mean_reward:  65.8805383108074 , time_score:  500 , memory:  127344\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward 33.14172256138137 , mean_reward:  66.80116605556208 , time_score:  500 , memory:  129844\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward 93.25746037595644 , mean_reward:  66.41388798872093 , time_score:  500 , memory:  132344\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward -18.648380152381772 , mean_reward:  68.11400049440846 , time_score:  500 , memory:  134844\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward 91.62513107287985 , mean_reward:  69.57016062933741 , time_score:  500 , memory:  137300\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward 137.04885126623182 , mean_reward:  72.90245380817647 , time_score:  500 , memory:  139710\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward 64.84542245640598 , mean_reward:  74.43188345957373 , time_score:  500 , memory:  142210\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward 84.33156786343459 , mean_reward:  74.47525970898627 , time_score:  500 , memory:  144710\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward 48.18319752723852 , mean_reward:  77.40745440294373 , time_score:  500 , memory:  147120\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward 85.56414535397022 , mean_reward:  82.01818031419057 , time_score:  500 , memory:  149620\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward 83.47052186900069 , mean_reward:  80.79376832099756 , time_score:  500 , memory:  152003\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward 67.0721042784477 , mean_reward:  83.49350388551176 , time_score:  500 , memory:  154356\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward 102.30703358752545 , mean_reward:  86.60998071152187 , time_score:  500 , memory:  156729\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward 42.9844179447006 , mean_reward:  86.72541390837861 , time_score:  500 , memory:  159229\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward 103.3928860723567 , mean_reward:  88.45364843773893 , time_score:  500 , memory:  161546\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward 281.69288682812794 , mean_reward:  97.86639491624639 , time_score:  496 , memory:  163779\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward 54.205879603973194 , mean_reward:  101.9757043344364 , time_score:  500 , memory:  166055\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward 147.746801910951 , mean_reward:  105.32934952243485 , time_score:  500 , memory:  168555\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward 254.6769469326062 , mean_reward:  111.5803964396698 , time_score:  414 , memory:  170616\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward 295.70379621309206 , mean_reward:  119.03597272476767 , time_score:  291 , memory:  172452\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward 148.08179555465009 , mean_reward:  124.27932876584931 , time_score:  500 , memory:  174688\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward 110.05900852531317 , mean_reward:  131.25873534927538 , time_score:  500 , memory:  176791\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward 216.04476266631536 , mean_reward:  134.08152348483273 , time_score:  441 , memory:  178694\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward 227.56104014809176 , mean_reward:  140.40896951145092 , time_score:  332 , memory:  180847\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward 263.597914621886 , mean_reward:  148.33485242397808 , time_score:  383 , memory:  182570\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward 108.58995208396824 , mean_reward:  152.62561014433663 , time_score:  500 , memory:  184712\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward 180.0200028818862 , mean_reward:  158.67434290413917 , time_score:  366 , memory:  186715\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward 273.4708062851251 , mean_reward:  164.59381181054755 , time_score:  312 , memory:  188399\n","Episode:  525  , Epsilon:  0.07196434741762824 , Reward 125.38615273129982 , mean_reward:  170.16804225827326 , time_score:  500 , memory:  190339\n","Episode:  530  , Epsilon:  0.07018314008827135 , Reward 293.7209690452015 , mean_reward:  173.44011802801234 , time_score:  351 , memory:  192472\n","Episode:  535  , Epsilon:  0.06844601986126451 , Reward 225.7810871922074 , mean_reward:  176.95056415754482 , time_score:  406 , memory:  194813\n","Episode:  540  , Epsilon:  0.0667518955258533 , Reward 113.20324123658875 , mean_reward:  182.2967347596532 , time_score:  500 , memory:  197057\n","Episode:  545  , Epsilon:  0.06509970288011008 , Reward 77.08557603021923 , mean_reward:  185.20490248702788 , time_score:  500 , memory:  199301\n","Episode:  550  , Epsilon:  0.06348840406243188 , Reward 91.26973630974909 , mean_reward:  190.79379466980924 , time_score:  500 , memory:  201391\n","Episode:  555  , Epsilon:  0.06191698689958447 , Reward 257.27830575761084 , mean_reward:  195.24752415937152 , time_score:  363 , memory:  203605\n","Episode:  560  , Epsilon:  0.06038446427088321 , Reward 255.79967377234357 , mean_reward:  192.4577953647462 , time_score:  280 , memory:  205798\n","Episode:  565  , Epsilon:  0.058889873488111255 , Reward 271.70652416083817 , mean_reward:  194.44573167930216 , time_score:  332 , memory:  207914\n","Episode:  570  , Epsilon:  0.05743227569078546 , Reward 252.50422047580048 , mean_reward:  200.99616710636758 , time_score:  432 , memory:  209584\n","BRAVO, GOAL ACHIEVED!!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QzXeEPyZkZx5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmu7jobCkZ0S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}