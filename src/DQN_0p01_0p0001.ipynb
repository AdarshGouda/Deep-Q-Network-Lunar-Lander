{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0p01_0p0001.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"mount_file_id":"1xDASlP8bz3XYtqU6cj-l0qDAT6nnhfTT","authorship_tag":"ABX9TyMlHFJHjBdfbvjAXfLXkWxx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624385880165,"user_tz":360,"elapsed":34110,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4903c42e-8fec-433d-bb83-13eeba9fcc0a"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624824529031,"user_tz":360,"elapsed":7831,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"69e6351d-3acc-48a4-a04f-e57c58484796"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting box2d-py\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 3.9MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6uU96UvCnyK8","executionInfo":{"status":"ok","timestamp":1624824529035,"user_tz":360,"elapsed":7,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8","executionInfo":{"status":"ok","timestamp":1624824602856,"user_tz":360,"elapsed":469,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.01_0.0001/0p01_0p0001.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.01_0.0001/0p01_0p0001.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","      "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624858764856,"user_tz":360,"elapsed":34160988,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"cb8b14f3-c234-4f01-93a9-eac8b71b7003"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.01, lr=0.0001)\n","df = dqn.play()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -315.92693734342527 , mean_reward:  -315.92693734342527 , time_score:  82 , memory:  82\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -276.5249574581966 , mean_reward:  -247.51334253104218 , time_score:  75 , memory:  512\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -101.36781489059553 , mean_reward:  -201.64045629630527 , time_score:  64 , memory:  944\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -112.16515036784203 , mean_reward:  -175.2300262844971 , time_score:  107 , memory:  1380\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward -295.0198748047735 , mean_reward:  -188.60243719201645 , time_score:  105 , memory:  2003\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -238.63366109064435 , mean_reward:  -181.70711359161533 , time_score:  93 , memory:  2474\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -169.98187323493312 , mean_reward:  -177.57878907981848 , time_score:  77 , memory:  2935\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -70.80539496047643 , mean_reward:  -174.65915117709474 , time_score:  68 , memory:  3424\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -219.1653483820549 , mean_reward:  -175.05459868571228 , time_score:  95 , memory:  3904\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -240.58054323145788 , mean_reward:  -171.15791780503102 , time_score:  98 , memory:  4459\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -270.820215717299 , mean_reward:  -172.63123102800313 , time_score:  129 , memory:  4964\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -196.4820044145883 , mean_reward:  -169.78627622345374 , time_score:  120 , memory:  5543\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -63.20007899807141 , mean_reward:  -165.39330215908922 , time_score:  74 , memory:  6080\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -43.87267388972984 , mean_reward:  -158.3779434035264 , time_score:  140 , memory:  7073\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -212.5203144266916 , mean_reward:  -156.7568601869114 , time_score:  176 , memory:  7753\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward -66.03468459963578 , mean_reward:  -156.0190175773023 , time_score:  96 , memory:  8294\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -144.52000866103862 , mean_reward:  -155.1875170036352 , time_score:  162 , memory:  9012\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -224.27235757342223 , mean_reward:  -155.3596039522495 , time_score:  128 , memory:  9879\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -220.33946715501216 , mean_reward:  -158.99279668952173 , time_score:  150 , memory:  10597\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -359.4917008879929 , mean_reward:  -160.1335608656054 , time_score:  223 , memory:  11407\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward -224.33404618242548 , mean_reward:  -162.45381025749222 , time_score:  144 , memory:  12205\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -312.989166384685 , mean_reward:  -157.74196216255785 , time_score:  196 , memory:  12878\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward -75.50359349943488 , mean_reward:  -157.85913246398633 , time_score:  146 , memory:  13535\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward -93.33142825727647 , mean_reward:  -158.09188559305744 , time_score:  93 , memory:  14341\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -108.58607861451743 , mean_reward:  -154.59740390285378 , time_score:  126 , memory:  15173\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward -198.29054690395523 , mean_reward:  -152.42078208985515 , time_score:  237 , memory:  16049\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -51.463153755832565 , mean_reward:  -150.6002960851518 , time_score:  214 , memory:  16898\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -61.2865181761466 , mean_reward:  -148.22085919951837 , time_score:  206 , memory:  17828\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -199.37167680818703 , mean_reward:  -146.29206027702284 , time_score:  203 , memory:  18635\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -1.4005432050145146 , mean_reward:  -142.78978070150896 , time_score:  148 , memory:  19601\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward -96.56748196367732 , mean_reward:  -138.4120601684671 , time_score:  228 , memory:  20587\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward -46.500921987788345 , mean_reward:  -137.11149943935013 , time_score:  148 , memory:  21476\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward -110.51789481847062 , mean_reward:  -136.7236907298238 , time_score:  207 , memory:  22547\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward -230.7112953729272 , mean_reward:  -139.9658108340385 , time_score:  393 , memory:  23552\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward -223.53876969031347 , mean_reward:  -139.14723810166086 , time_score:  323 , memory:  24706\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward -92.92562737220777 , mean_reward:  -135.28244281497848 , time_score:  130 , memory:  25476\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward -182.99407111012619 , mean_reward:  -130.59060987084504 , time_score:  240 , memory:  26932\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward -56.26466893706015 , mean_reward:  -123.34190919542384 , time_score:  166 , memory:  28534\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward -61.92263798763743 , mean_reward:  -115.43568305447121 , time_score:  156 , memory:  29900\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward -85.50043342932287 , mean_reward:  -113.0022236525253 , time_score:  310 , memory:  31358\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward 88.96585784450622 , mean_reward:  -100.60041606303969 , time_score:  500 , memory:  33348\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward 91.80501305696656 , mean_reward:  -93.71796018586603 , time_score:  500 , memory:  35515\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward 41.27554279337689 , mean_reward:  -88.74661397859526 , time_score:  500 , memory:  36758\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward 46.63802527916781 , mean_reward:  -84.2691409649944 , time_score:  500 , memory:  38409\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward 54.17234195435837 , mean_reward:  -79.27210475385122 , time_score:  500 , memory:  40533\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward 8.687671728204009 , mean_reward:  -74.32348356182028 , time_score:  500 , memory:  42919\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward 71.2405698006216 , mean_reward:  -73.65369999905205 , time_score:  500 , memory:  44457\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward -343.2069268155731 , mean_reward:  -73.88685616324035 , time_score:  312 , memory:  45714\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward -201.12418976145477 , mean_reward:  -73.36061681425552 , time_score:  100 , memory:  46737\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward -275.6532230440813 , mean_reward:  -74.3728298337026 , time_score:  192 , memory:  48275\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward -305.524770802949 , mean_reward:  -77.30690248012634 , time_score:  391 , memory:  49670\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward -54.1187182352692 , mean_reward:  -75.70182547086955 , time_score:  117 , memory:  50892\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward -73.60509787651941 , mean_reward:  -77.2602491633484 , time_score:  254 , memory:  51973\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward 44.93207630464439 , mean_reward:  -67.6527055319378 , time_score:  500 , memory:  54155\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward 15.927817774523952 , mean_reward:  -62.01963250358427 , time_score:  500 , memory:  56186\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward 60.40744073853264 , mean_reward:  -55.97070042966572 , time_score:  500 , memory:  58686\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward -146.33480306423428 , mean_reward:  -53.70212901174982 , time_score:  383 , memory:  61069\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward 46.42723468654466 , mean_reward:  -51.64613193003683 , time_score:  500 , memory:  63287\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward 30.309368170649268 , mean_reward:  -46.54595298414847 , time_score:  500 , memory:  65787\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward 0.09049468595438831 , mean_reward:  -38.69405255016209 , time_score:  500 , memory:  67994\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward 3.173566048264755 , mean_reward:  -38.01483841023104 , time_score:  500 , memory:  70494\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward 6.234930305206477 , mean_reward:  -36.53494593045636 , time_score:  500 , memory:  72994\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward 27.138386818204257 , mean_reward:  -32.9089835602885 , time_score:  500 , memory:  75494\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward 49.06022443818772 , mean_reward:  -30.006082358746795 , time_score:  500 , memory:  77994\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward 27.902617221171926 , mean_reward:  -25.072739825726003 , time_score:  500 , memory:  80334\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward 9.433870665112094 , mean_reward:  -28.881865863050134 , time_score:  500 , memory:  82681\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward 45.60169196632904 , mean_reward:  -22.398724713330903 , time_score:  500 , memory:  85181\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward 1.0398443421179582 , mean_reward:  -15.66215811623357 , time_score:  500 , memory:  87440\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward -133.50558622942697 , mean_reward:  -12.07587230540752 , time_score:  422 , memory:  89778\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward 17.118965332995916 , mean_reward:  -9.609544012896958 , time_score:  500 , memory:  92145\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward -31.23843600699967 , mean_reward:  -0.697704069475164 , time_score:  500 , memory:  94645\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward -206.83058339939703 , mean_reward:  -4.921357496671298 , time_score:  415 , memory:  96832\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward -204.70403142487396 , mean_reward:  -3.6032726951304097 , time_score:  387 , memory:  98977\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward -201.13832262521055 , mean_reward:  -9.290953120668576 , time_score:  485 , memory:  101319\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward -153.3719090904491 , mean_reward:  -12.947100246672472 , time_score:  347 , memory:  102898\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward -153.16565642940986 , mean_reward:  -20.370311135418817 , time_score:  335 , memory:  104961\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward -39.04674201841901 , mean_reward:  -24.618407929529198 , time_score:  500 , memory:  106871\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward -145.68230275321255 , mean_reward:  -32.14870822041402 , time_score:  338 , memory:  108778\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward -149.94347259107116 , mean_reward:  -37.90236117527608 , time_score:  322 , memory:  110678\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward -15.266770053777446 , mean_reward:  -45.70315495207981 , time_score:  500 , memory:  112343\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward -6.996001186450964 , mean_reward:  -50.639388086722946 , time_score:  500 , memory:  114614\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward 51.539145085064504 , mean_reward:  -54.45502691046656 , time_score:  500 , memory:  116828\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward 59.40012044515382 , mean_reward:  -54.915607464796366 , time_score:  500 , memory:  119328\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward 41.69398440310421 , mean_reward:  -55.660084928241965 , time_score:  500 , memory:  121667\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward 2.0105530984793334 , mean_reward:  -58.87516047155679 , time_score:  500 , memory:  124152\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward 20.54253829064667 , mean_reward:  -53.53426252909684 , time_score:  500 , memory:  126652\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward 29.298235430172667 , mean_reward:  -53.14705393173365 , time_score:  500 , memory:  129152\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward -5.1135838630876425 , mean_reward:  -53.749436053751516 , time_score:  500 , memory:  131652\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward 13.853779660644788 , mean_reward:  -49.54347438240668 , time_score:  500 , memory:  134152\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward -23.61385753980918 , mean_reward:  -47.23547762159245 , time_score:  500 , memory:  136652\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward 39.04051974131327 , mean_reward:  -47.41464759039829 , time_score:  500 , memory:  139152\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward -14.217512496745197 , mean_reward:  -37.4712294117436 , time_score:  500 , memory:  141652\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward 38.58440869213239 , mean_reward:  -31.36920542040564 , time_score:  500 , memory:  144152\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward 30.032889182612784 , mean_reward:  -26.744528455008645 , time_score:  500 , memory:  146652\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward 64.03910036890677 , mean_reward:  -21.756721702132154 , time_score:  500 , memory:  149152\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward 50.53356410359945 , mean_reward:  -15.205894775386248 , time_score:  500 , memory:  151652\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward 69.3148482843325 , mean_reward:  -8.99955118556269 , time_score:  500 , memory:  154152\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward 50.27908158318056 , mean_reward:  -1.6531820614157515 , time_score:  500 , memory:  156652\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward 69.32277767608433 , mean_reward:  3.307463668088404 , time_score:  500 , memory:  159152\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward 75.2081074277516 , mean_reward:  11.286189875692594 , time_score:  500 , memory:  161652\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward 7.1007350027776095 , mean_reward:  16.290785508923303 , time_score:  500 , memory:  164152\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward 15.768660402354778 , mean_reward:  19.345930934061762 , time_score:  500 , memory:  166652\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward -24.351389436033568 , mean_reward:  19.63100521179888 , time_score:  500 , memory:  169152\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward -32.501521177529014 , mean_reward:  19.29780577847658 , time_score:  500 , memory:  171652\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward -1.6455789547806348 , mean_reward:  21.631826948056165 , time_score:  500 , memory:  174152\n","Episode:  525  , Epsilon:  0.07196434741762824 , Reward 49.47912734973153 , mean_reward:  22.376315458583555 , time_score:  500 , memory:  176652\n","Episode:  530  , Epsilon:  0.07018314008827135 , Reward 12.376785478407008 , mean_reward:  21.9575295499928 , time_score:  500 , memory:  179152\n","Episode:  535  , Epsilon:  0.06844601986126451 , Reward 52.96445827330878 , mean_reward:  23.3404692106414 , time_score:  500 , memory:  181652\n","Episode:  540  , Epsilon:  0.0667518955258533 , Reward 21.784898567200838 , mean_reward:  23.534791362720984 , time_score:  500 , memory:  184152\n","Episode:  545  , Epsilon:  0.06509970288011008 , Reward 63.335508880081356 , mean_reward:  25.036542285015134 , time_score:  500 , memory:  186652\n","Episode:  550  , Epsilon:  0.06348840406243188 , Reward -24.94077737774194 , mean_reward:  24.146973002784748 , time_score:  500 , memory:  189152\n","Episode:  555  , Epsilon:  0.06191698689958447 , Reward -8.227906382020556 , mean_reward:  22.97336187218048 , time_score:  500 , memory:  191652\n","Episode:  560  , Epsilon:  0.06038446427088321 , Reward 47.763257777404036 , mean_reward:  23.67601924548035 , time_score:  500 , memory:  194152\n","Episode:  565  , Epsilon:  0.058889873488111255 , Reward 9.940022646030929 , mean_reward:  23.498254080607484 , time_score:  500 , memory:  196652\n","Episode:  570  , Epsilon:  0.05743227569078546 , Reward 62.30501397227761 , mean_reward:  24.071868430212902 , time_score:  500 , memory:  199152\n","Episode:  575  , Epsilon:  0.05601075525639029 , Reward 65.6987765302212 , mean_reward:  23.851028529005635 , time_score:  500 , memory:  201652\n","Episode:  580  , Epsilon:  0.05462441922520914 , Reward 30.674128152941012 , mean_reward:  23.63276457751735 , time_score:  500 , memory:  204152\n","Episode:  585  , Epsilon:  0.05327239673939179 , Reward -8.528743046876146 , mean_reward:  23.365427494363875 , time_score:  500 , memory:  206652\n","Episode:  590  , Epsilon:  0.05195383849590569 , Reward 10.33690239629262 , mean_reward:  23.709833128613024 , time_score:  500 , memory:  209152\n","Episode:  595  , Epsilon:  0.05066791621302729 , Reward 16.953484006219373 , mean_reward:  24.317962142685364 , time_score:  500 , memory:  211652\n","Episode:  600  , Epsilon:  0.0494138221100385 , Reward -1.91389347147856 , mean_reward:  23.666654668070315 , time_score:  500 , memory:  214152\n","Episode:  605  , Epsilon:  0.048190768399801194 , Reward -21.204768273736263 , mean_reward:  25.002648696448297 , time_score:  500 , memory:  216652\n","Episode:  610  , Epsilon:  0.046997986793891174 , Reward 1.0168010774202467 , mean_reward:  24.42107716425177 , time_score:  500 , memory:  219152\n","Episode:  615  , Epsilon:  0.04583472801998072 , Reward -7.491994502214301 , mean_reward:  25.52301186818808 , time_score:  500 , memory:  221652\n","Episode:  620  , Epsilon:  0.04470026135116646 , Reward -20.0073058453298 , mean_reward:  25.413611653492616 , time_score:  500 , memory:  224152\n","Episode:  625  , Epsilon:  0.04359387414694703 , Reward 54.93039630744785 , mean_reward:  24.838562457009722 , time_score:  500 , memory:  226652\n","Episode:  630  , Epsilon:  0.04251487140556204 , Reward 63.44733471930371 , mean_reward:  25.985517451792827 , time_score:  500 , memory:  229152\n","Episode:  635  , Epsilon:  0.04146257532741124 , Reward 32.09283698720518 , mean_reward:  24.73636535834274 , time_score:  500 , memory:  231652\n","Episode:  640  , Epsilon:  0.04043632488927963 , Reward 41.11265856186492 , mean_reward:  23.437353579369983 , time_score:  500 , memory:  234152\n","Episode:  645  , Epsilon:  0.039435475429100995 , Reward -2.6575721171734727 , mean_reward:  22.355360274414792 , time_score:  500 , memory:  236652\n","Episode:  650  , Epsilon:  0.03845939824099909 , Reward 69.21541118592839 , mean_reward:  24.77636480009078 , time_score:  500 , memory:  239152\n","Episode:  655  , Epsilon:  0.03750748018035199 , Reward 26.336243297611144 , mean_reward:  26.239734361364476 , time_score:  500 , memory:  241652\n","Episode:  660  , Epsilon:  0.03657912327863173 , Reward -14.4222874211621 , mean_reward:  24.806722565652407 , time_score:  500 , memory:  244152\n","Episode:  665  , Epsilon:  0.035673744367776934 , Reward -20.526993565530976 , mean_reward:  23.16195528647959 , time_score:  500 , memory:  246652\n","Episode:  670  , Epsilon:  0.03479077471386296 , Reward 55.198962438717786 , mean_reward:  23.305919205617112 , time_score:  500 , memory:  249152\n","Episode:  675  , Epsilon:  0.03392965965983891 , Reward -4.696602343351659 , mean_reward:  21.850079901142355 , time_score:  500 , memory:  251652\n","Episode:  680  , Epsilon:  0.03308985827710748 , Reward 17.249971480742918 , mean_reward:  21.581360429309015 , time_score:  500 , memory:  254152\n","Episode:  685  , Epsilon:  0.03227084302572862 , Reward 65.61517565265525 , mean_reward:  23.155475430105998 , time_score:  500 , memory:  256652\n","Episode:  690  , Epsilon:  0.03147209942303359 , Reward 5.94511860270307 , mean_reward:  21.577289042542326 , time_score:  500 , memory:  259152\n","Episode:  695  , Epsilon:  0.030693125720441184 , Reward -14.74563755748474 , mean_reward:  20.008460819168956 , time_score:  500 , memory:  261652\n","Episode:  700  , Epsilon:  0.029933432588273214 , Reward 33.23860928538481 , mean_reward:  19.394719892993788 , time_score:  500 , memory:  264152\n","Episode:  705  , Epsilon:  0.029192542808371146 , Reward -21.35357005628141 , mean_reward:  16.73374828209176 , time_score:  500 , memory:  266652\n","Episode:  710  , Epsilon:  0.028469990974320916 , Reward -25.191915323698186 , mean_reward:  16.862107911422545 , time_score:  500 , memory:  269152\n","Episode:  715  , Epsilon:  0.027765323199097504 , Reward -42.95801413292219 , mean_reward:  15.724353855982605 , time_score:  500 , memory:  271652\n","Episode:  720  , Epsilon:  0.02707809682994571 , Reward -39.949312130923786 , mean_reward:  15.825137404093622 , time_score:  500 , memory:  274152\n","Episode:  725  , Epsilon:  0.026407880170317945 , Reward -0.9602638103255039 , mean_reward:  14.112921411169745 , time_score:  500 , memory:  276652\n","Episode:  730  , Epsilon:  0.025754252208694463 , Reward -25.921485372074866 , mean_reward:  11.448976479853748 , time_score:  500 , memory:  279152\n","Episode:  735  , Epsilon:  0.025116802354115567 , Reward 20.622121467119154 , mean_reward:  11.045490417542501 , time_score:  500 , memory:  281652\n","Episode:  740  , Epsilon:  0.02449513017825978 , Reward 28.385368602816126 , mean_reward:  11.132066707892447 , time_score:  500 , memory:  284152\n","Episode:  745  , Epsilon:  0.023888845163905856 , Reward 25.297088755298148 , mean_reward:  11.929076718243207 , time_score:  500 , memory:  286652\n","Episode:  750  , Epsilon:  0.023297566459620722 , Reward -13.110709569783738 , mean_reward:  9.96770256567345 , time_score:  500 , memory:  289152\n","Episode:  755  , Epsilon:  0.022720922640519125 , Reward 1.7547432286457365 , mean_reward:  9.693729948717705 , time_score:  500 , memory:  291652\n","Episode:  760  , Epsilon:  0.022158551474944856 , Reward -17.086178444712637 , mean_reward:  9.887259733644202 , time_score:  500 , memory:  294152\n","Episode:  765  , Epsilon:  0.021610099696926857 , Reward -20.42132446793758 , mean_reward:  11.861073119137986 , time_score:  500 , memory:  296652\n","Episode:  770  , Epsilon:  0.021075222784267326 , Reward -28.27360689345 , mean_reward:  9.436550013999476 , time_score:  500 , memory:  299152\n","Episode:  775  , Epsilon:  0.020553584742122436 , Reward -8.706889152081288 , mean_reward:  9.685089082200975 , time_score:  500 , memory:  301652\n","Episode:  780  , Epsilon:  0.020044857891939702 , Reward -4.988844238867156 , mean_reward:  8.997443741972242 , time_score:  500 , memory:  304152\n","Episode:  785  , Epsilon:  0.01954872266561937 , Reward -26.651152056328865 , mean_reward:  6.517924974007952 , time_score:  500 , memory:  306652\n","Episode:  790  , Epsilon:  0.019064867404770626 , Reward -25.68221357098987 , mean_reward:  8.25806823519772 , time_score:  500 , memory:  309152\n","Episode:  795  , Epsilon:  0.018592988164936427 , Reward -16.311153108346943 , mean_reward:  7.638207837856894 , time_score:  500 , memory:  311652\n","Episode:  800  , Epsilon:  0.018132788524664028 , Reward 39.14664934814026 , mean_reward:  7.955161243767955 , time_score:  500 , memory:  314152\n","Episode:  805  , Epsilon:  0.017683979399301233 , Reward -26.749311698982417 , mean_reward:  8.466341372099857 , time_score:  500 , memory:  316652\n","Episode:  810  , Epsilon:  0.01724627885940145 , Reward 3.696462424863359 , mean_reward:  7.5634939658770755 , time_score:  500 , memory:  319152\n","Episode:  815  , Epsilon:  0.01681941195362342 , Reward 31.53828193384459 , mean_reward:  8.419187137306036 , time_score:  500 , memory:  321652\n","Episode:  820  , Epsilon:  0.0164031105360144 , Reward 30.53906201349177 , mean_reward:  9.427755574427035 , time_score:  500 , memory:  324152\n","Episode:  825  , Epsilon:  0.015997113097568336 , Reward -29.849338379302957 , mean_reward:  10.234531811597822 , time_score:  500 , memory:  326652\n","Episode:  830  , Epsilon:  0.015601164601953134 , Reward -2.032488746463638 , mean_reward:  11.296095123082523 , time_score:  500 , memory:  329152\n","Episode:  835  , Epsilon:  0.015215016325303928 , Reward -6.819017449536693 , mean_reward:  10.707156807447504 , time_score:  500 , memory:  331652\n","Episode:  840  , Epsilon:  0.014838425699981627 , Reward 0.0948925310306703 , mean_reward:  10.136006809319328 , time_score:  500 , memory:  334152\n","Episode:  845  , Epsilon:  0.014471156162198668 , Reward 50.470638260828665 , mean_reward:  10.101384979966564 , time_score:  500 , memory:  336652\n","Episode:  850  , Epsilon:  0.014112977003416188 , Reward -50.03713150402507 , mean_reward:  9.038979003492148 , time_score:  500 , memory:  339152\n","Episode:  855  , Epsilon:  0.013763663225419333 , Reward -7.401080562257768 , mean_reward:  7.360411101964337 , time_score:  500 , memory:  341652\n","Episode:  860  , Epsilon:  0.013422995398979608 , Reward -10.275483545401556 , mean_reward:  7.550062517591381 , time_score:  500 , memory:  344152\n","Episode:  865  , Epsilon:  0.013090759526015528 , Reward -5.851841763942434 , mean_reward:  5.968180540602617 , time_score:  500 , memory:  346652\n","Episode:  870  , Epsilon:  0.012766746905164949 , Reward -21.19573638391872 , mean_reward:  6.5649450076116045 , time_score:  500 , memory:  349152\n","Episode:  875  , Epsilon:  0.012450754000684672 , Reward 33.655012537069496 , mean_reward:  6.540958068628891 , time_score:  500 , memory:  351652\n","Episode:  880  , Epsilon:  0.012142582314594924 , Reward -28.46683924943841 , mean_reward:  5.075157984460953 , time_score:  500 , memory:  354152\n","Episode:  885  , Epsilon:  0.01184203826198843 , Reward -23.476072830347174 , mean_reward:  4.872868569310139 , time_score:  500 , memory:  356652\n","Episode:  890  , Epsilon:  0.01154893304942575 , Reward -10.187764930937236 , mean_reward:  4.241802012600154 , time_score:  500 , memory:  359152\n","Episode:  895  , Epsilon:  0.011263082556340478 , Reward -14.427951890322017 , mean_reward:  3.6038049391760745 , time_score:  500 , memory:  361652\n","Episode:  900  , Epsilon:  0.01098430721937979 , Reward 1.5414136380286976 , mean_reward:  4.672880976807265 , time_score:  500 , memory:  364152\n","Episode:  905  , Epsilon:  0.01071243191960775 , Reward -2.9217918248898442 , mean_reward:  4.737070127882623 , time_score:  500 , memory:  366652\n","Episode:  910  , Epsilon:  0.010447285872500434 , Reward -34.479044668881315 , mean_reward:  6.213356519575257 , time_score:  500 , memory:  369152\n","Episode:  915  , Epsilon:  0.010188702520663827 , Reward 0.6719522479121403 , mean_reward:  6.19845246418323 , time_score:  500 , memory:  371652\n","Episode:  920  , Epsilon:  0.01 , Reward 34.720420652726965 , mean_reward:  4.5652749392417205 , time_score:  500 , memory:  374152\n","Episode:  925  , Epsilon:  0.01 , Reward -13.711319384563069 , mean_reward:  4.063806701010844 , time_score:  500 , memory:  376652\n","Episode:  930  , Epsilon:  0.01 , Reward -31.771735018795216 , mean_reward:  3.1124204707775833 , time_score:  500 , memory:  379152\n","Episode:  935  , Epsilon:  0.01 , Reward -18.8728789314371 , mean_reward:  4.3584114550330755 , time_score:  500 , memory:  381652\n","Episode:  940  , Epsilon:  0.01 , Reward -15.192932241108347 , mean_reward:  5.887373639051625 , time_score:  500 , memory:  384152\n","Episode:  945  , Epsilon:  0.01 , Reward 30.324978581818133 , mean_reward:  5.005950786936766 , time_score:  500 , memory:  386652\n","Episode:  950  , Epsilon:  0.01 , Reward -27.715249199839995 , mean_reward:  5.730236286775346 , time_score:  500 , memory:  389152\n","Episode:  955  , Epsilon:  0.01 , Reward -22.506097339823473 , mean_reward:  6.683953517639306 , time_score:  500 , memory:  391652\n","Episode:  960  , Epsilon:  0.01 , Reward 23.16927710010564 , mean_reward:  7.618987412852594 , time_score:  500 , memory:  394152\n","Episode:  965  , Epsilon:  0.01 , Reward -11.626363867281013 , mean_reward:  6.344941182204994 , time_score:  500 , memory:  396652\n","Episode:  970  , Epsilon:  0.01 , Reward -8.159012041893746 , mean_reward:  5.445619559733935 , time_score:  500 , memory:  399152\n","Episode:  975  , Epsilon:  0.01 , Reward 53.62473224302604 , mean_reward:  6.552170207673871 , time_score:  500 , memory:  401652\n","Episode:  980  , Epsilon:  0.01 , Reward 30.23651183506102 , mean_reward:  8.205252761140159 , time_score:  500 , memory:  404152\n","Episode:  985  , Epsilon:  0.01 , Reward 21.426027066666748 , mean_reward:  8.993280978234854 , time_score:  500 , memory:  406652\n","Episode:  990  , Epsilon:  0.01 , Reward -43.664627940723 , mean_reward:  7.58313072504282 , time_score:  500 , memory:  409152\n","Episode:  995  , Epsilon:  0.01 , Reward 26.21157650407436 , mean_reward:  9.08668746087758 , time_score:  500 , memory:  411652\n","Episode:  1000  , Epsilon:  0.01 , Reward 87.68680319857866 , mean_reward:  8.830673111662799 , time_score:  500 , memory:  414152\n","Episode:  1005  , Epsilon:  0.01 , Reward 88.40186552721175 , mean_reward:  11.096085335634816 , time_score:  500 , memory:  416652\n","Episode:  1010  , Epsilon:  0.01 , Reward -0.24657267228010799 , mean_reward:  11.468032217468608 , time_score:  500 , memory:  419152\n","Episode:  1015  , Epsilon:  0.01 , Reward -10.48013683981824 , mean_reward:  11.535211774414167 , time_score:  500 , memory:  421652\n","Episode:  1020  , Epsilon:  0.01 , Reward 53.537066114466754 , mean_reward:  12.488017307698586 , time_score:  500 , memory:  424152\n","Episode:  1025  , Epsilon:  0.01 , Reward -32.71311848688456 , mean_reward:  13.462953054410512 , time_score:  500 , memory:  426652\n","Episode:  1030  , Epsilon:  0.01 , Reward 97.80227403978508 , mean_reward:  15.147686156452183 , time_score:  500 , memory:  429152\n","Episode:  1035  , Epsilon:  0.01 , Reward 35.55069823850913 , mean_reward:  15.411033528124186 , time_score:  500 , memory:  431652\n","Episode:  1040  , Epsilon:  0.01 , Reward -6.300024586911037 , mean_reward:  15.267098486379904 , time_score:  500 , memory:  434152\n","Episode:  1045  , Epsilon:  0.01 , Reward 84.46846504760839 , mean_reward:  16.404183225638533 , time_score:  500 , memory:  436652\n","Episode:  1050  , Epsilon:  0.01 , Reward -14.46678064262938 , mean_reward:  16.048631449760496 , time_score:  500 , memory:  439152\n","Episode:  1055  , Epsilon:  0.01 , Reward 5.737303413411735 , mean_reward:  15.528749952989822 , time_score:  500 , memory:  441652\n","Episode:  1060  , Epsilon:  0.01 , Reward -14.02055148298598 , mean_reward:  15.629969442243514 , time_score:  500 , memory:  444152\n","Episode:  1065  , Epsilon:  0.01 , Reward 20.811975521841568 , mean_reward:  18.655325948589788 , time_score:  500 , memory:  446652\n","Episode:  1070  , Epsilon:  0.01 , Reward -1.4145923758982328 , mean_reward:  20.478418083553574 , time_score:  500 , memory:  449152\n","Episode:  1075  , Epsilon:  0.01 , Reward 20.00139756409998 , mean_reward:  19.162944226672398 , time_score:  500 , memory:  451652\n","Episode:  1080  , Epsilon:  0.01 , Reward 30.048505604925417 , mean_reward:  18.519692060423004 , time_score:  500 , memory:  454152\n","Episode:  1085  , Epsilon:  0.01 , Reward -6.016052161711642 , mean_reward:  18.56608802521259 , time_score:  500 , memory:  456652\n","Episode:  1090  , Epsilon:  0.01 , Reward 12.559596882112066 , mean_reward:  18.78472031888336 , time_score:  500 , memory:  459152\n","Episode:  1095  , Epsilon:  0.01 , Reward 21.101027551144625 , mean_reward:  18.93941482002094 , time_score:  500 , memory:  461652\n","Episode:  1100  , Epsilon:  0.01 , Reward 17.880779003771426 , mean_reward:  18.633506972176107 , time_score:  500 , memory:  464152\n","Episode:  1105  , Epsilon:  0.01 , Reward 4.3450509154659835 , mean_reward:  17.45881500246388 , time_score:  500 , memory:  466652\n","Episode:  1110  , Epsilon:  0.01 , Reward 9.703736500257204 , mean_reward:  17.38371993923739 , time_score:  500 , memory:  469152\n","Episode:  1115  , Epsilon:  0.01 , Reward -10.316833290845903 , mean_reward:  16.017709242504633 , time_score:  500 , memory:  471652\n","Episode:  1120  , Epsilon:  0.01 , Reward -16.787756798688655 , mean_reward:  15.115196957996254 , time_score:  500 , memory:  474152\n","Episode:  1125  , Epsilon:  0.01 , Reward 29.801695998212203 , mean_reward:  14.880683089579186 , time_score:  500 , memory:  476652\n","Episode:  1130  , Epsilon:  0.01 , Reward -0.4672003079624727 , mean_reward:  14.602176309362685 , time_score:  500 , memory:  479152\n","Episode:  1135  , Epsilon:  0.01 , Reward -38.88994718403036 , mean_reward:  13.250232760739113 , time_score:  500 , memory:  481652\n","Episode:  1140  , Epsilon:  0.01 , Reward -22.85811125423341 , mean_reward:  11.862105361265783 , time_score:  500 , memory:  484152\n","Episode:  1145  , Epsilon:  0.01 , Reward -22.92292981854319 , mean_reward:  10.593854111152066 , time_score:  500 , memory:  486652\n","Episode:  1150  , Epsilon:  0.01 , Reward 50.1172544488316 , mean_reward:  11.936600009924824 , time_score:  500 , memory:  489152\n","Episode:  1155  , Epsilon:  0.01 , Reward -33.207344932075415 , mean_reward:  11.920928215909793 , time_score:  500 , memory:  491652\n","Episode:  1160  , Epsilon:  0.01 , Reward 84.65817377212055 , mean_reward:  12.843338421234533 , time_score:  500 , memory:  494152\n","Episode:  1165  , Epsilon:  0.01 , Reward 33.57267368280573 , mean_reward:  11.737507164155979 , time_score:  500 , memory:  496652\n","Episode:  1170  , Epsilon:  0.01 , Reward 25.207703830968164 , mean_reward:  11.955359600349263 , time_score:  500 , memory:  499152\n","Episode:  1175  , Epsilon:  0.01 , Reward -15.011623265517166 , mean_reward:  13.466952831374108 , time_score:  500 , memory:  501652\n","Episode:  1180  , Epsilon:  0.01 , Reward 10.478308097508842 , mean_reward:  15.22119107490923 , time_score:  500 , memory:  504152\n","Episode:  1185  , Epsilon:  0.01 , Reward 70.5350688720537 , mean_reward:  16.771279544953355 , time_score:  500 , memory:  506652\n","Episode:  1190  , Epsilon:  0.01 , Reward 33.27440266563502 , mean_reward:  16.65742495984506 , time_score:  500 , memory:  509152\n","Episode:  1195  , Epsilon:  0.01 , Reward -5.277791052832139 , mean_reward:  17.379783162124994 , time_score:  500 , memory:  511652\n","Episode:  1200  , Epsilon:  0.01 , Reward 81.69432563724831 , mean_reward:  17.512503367793595 , time_score:  500 , memory:  514152\n","Episode:  1205  , Epsilon:  0.01 , Reward -11.961454176040512 , mean_reward:  18.460735632983027 , time_score:  500 , memory:  516652\n","Episode:  1210  , Epsilon:  0.01 , Reward 70.53798399580339 , mean_reward:  19.116318231509634 , time_score:  500 , memory:  519152\n","Episode:  1215  , Epsilon:  0.01 , Reward -10.687333213270856 , mean_reward:  20.125128991596014 , time_score:  500 , memory:  521652\n","Episode:  1220  , Epsilon:  0.01 , Reward 15.975454553899116 , mean_reward:  21.065562147165274 , time_score:  500 , memory:  524152\n","Episode:  1225  , Epsilon:  0.01 , Reward 28.13389939154637 , mean_reward:  21.445876811096323 , time_score:  500 , memory:  526652\n","Episode:  1230  , Epsilon:  0.01 , Reward 73.52958928907488 , mean_reward:  21.156553093489677 , time_score:  500 , memory:  529152\n","Episode:  1235  , Epsilon:  0.01 , Reward 46.50320399009732 , mean_reward:  23.055853897541123 , time_score:  500 , memory:  531652\n","Episode:  1240  , Epsilon:  0.01 , Reward 18.666342444794648 , mean_reward:  25.345270598414704 , time_score:  500 , memory:  534152\n","Episode:  1245  , Epsilon:  0.01 , Reward 63.39030608566479 , mean_reward:  26.636874568803044 , time_score:  500 , memory:  536652\n","Episode:  1250  , Epsilon:  0.01 , Reward 36.274837093227745 , mean_reward:  27.74377898006498 , time_score:  500 , memory:  539152\n","Episode:  1255  , Epsilon:  0.01 , Reward 78.79167750687004 , mean_reward:  30.189680278271204 , time_score:  500 , memory:  541652\n","Episode:  1260  , Epsilon:  0.01 , Reward -45.842304207270146 , mean_reward:  28.973447504191384 , time_score:  500 , memory:  544152\n","Episode:  1265  , Epsilon:  0.01 , Reward 107.38816607010854 , mean_reward:  29.54311281011421 , time_score:  500 , memory:  546652\n","Episode:  1270  , Epsilon:  0.01 , Reward 7.5172656387716845 , mean_reward:  29.530187064677975 , time_score:  500 , memory:  549152\n","Episode:  1275  , Epsilon:  0.01 , Reward -6.0674463825136336 , mean_reward:  28.076265465235046 , time_score:  500 , memory:  551652\n","Episode:  1280  , Epsilon:  0.01 , Reward -23.398632524946763 , mean_reward:  26.40893893537969 , time_score:  500 , memory:  554152\n","Episode:  1285  , Epsilon:  0.01 , Reward 36.66142322779692 , mean_reward:  27.266230007296404 , time_score:  500 , memory:  556652\n","Episode:  1290  , Epsilon:  0.01 , Reward 17.311272058711175 , mean_reward:  30.02988432604143 , time_score:  500 , memory:  559152\n","Episode:  1295  , Epsilon:  0.01 , Reward 68.40494502958002 , mean_reward:  30.821225575501963 , time_score:  500 , memory:  561652\n","Episode:  1300  , Epsilon:  0.01 , Reward -23.079938540506056 , mean_reward:  30.760734053841155 , time_score:  500 , memory:  564152\n","Episode:  1305  , Epsilon:  0.01 , Reward -4.839105185814373 , mean_reward:  30.982902228737256 , time_score:  500 , memory:  566652\n","Episode:  1310  , Epsilon:  0.01 , Reward 11.602618061489064 , mean_reward:  30.64959597207633 , time_score:  500 , memory:  569152\n","Episode:  1315  , Epsilon:  0.01 , Reward 73.21197031758736 , mean_reward:  31.67613934080362 , time_score:  500 , memory:  571652\n","Episode:  1320  , Epsilon:  0.01 , Reward -0.929124358004316 , mean_reward:  30.855692379934155 , time_score:  500 , memory:  574152\n","Episode:  1325  , Epsilon:  0.01 , Reward -35.95485748696704 , mean_reward:  30.044311663265493 , time_score:  500 , memory:  576652\n","Episode:  1330  , Epsilon:  0.01 , Reward 60.269889751825644 , mean_reward:  30.23371642399133 , time_score:  500 , memory:  579152\n","Episode:  1335  , Epsilon:  0.01 , Reward 15.984726245996569 , mean_reward:  27.57956197636892 , time_score:  500 , memory:  581640\n","Episode:  1340  , Epsilon:  0.01 , Reward -5.1710351819461 , mean_reward:  27.84800252158636 , time_score:  500 , memory:  584140\n","Episode:  1345  , Epsilon:  0.01 , Reward -18.482304030314648 , mean_reward:  26.164774129990345 , time_score:  500 , memory:  586640\n","Episode:  1350  , Epsilon:  0.01 , Reward -37.635531770373895 , mean_reward:  23.77586823505502 , time_score:  500 , memory:  589140\n","Episode:  1355  , Epsilon:  0.01 , Reward -38.452107342161746 , mean_reward:  20.027232393129424 , time_score:  500 , memory:  591640\n","Episode:  1360  , Epsilon:  0.01 , Reward -40.36108806644494 , mean_reward:  16.672920204983008 , time_score:  500 , memory:  594140\n","Episode:  1365  , Epsilon:  0.01 , Reward -4.393831590961656 , mean_reward:  13.535272237522527 , time_score:  500 , memory:  596640\n","Episode:  1370  , Epsilon:  0.01 , Reward -63.29405243226384 , mean_reward:  10.98323285938279 , time_score:  500 , memory:  599140\n","Episode:  1375  , Epsilon:  0.01 , Reward -29.39168666853119 , mean_reward:  10.19989512333717 , time_score:  500 , memory:  601640\n","Episode:  1380  , Epsilon:  0.01 , Reward -9.87337186707016 , mean_reward:  8.77601418537 , time_score:  500 , memory:  604140\n","Episode:  1385  , Epsilon:  0.01 , Reward 42.31804966781125 , mean_reward:  6.505425895450951 , time_score:  500 , memory:  606640\n","Episode:  1390  , Epsilon:  0.01 , Reward 0.284020137042193 , mean_reward:  2.726675543626503 , time_score:  500 , memory:  609140\n","Episode:  1395  , Epsilon:  0.01 , Reward -6.895171125923021 , mean_reward:  -0.18494133009713687 , time_score:  500 , memory:  611640\n","Episode:  1400  , Epsilon:  0.01 , Reward -13.600637130831778 , mean_reward:  -2.302102833341046 , time_score:  500 , memory:  614140\n","Episode:  1405  , Epsilon:  0.01 , Reward 6.587658225156718 , mean_reward:  -5.24411263543969 , time_score:  500 , memory:  616640\n","Episode:  1410  , Epsilon:  0.01 , Reward -60.04235236201358 , mean_reward:  -8.92000919312738 , time_score:  500 , memory:  619140\n","Episode:  1415  , Epsilon:  0.01 , Reward -47.8028759297001 , mean_reward:  -12.244863632973198 , time_score:  500 , memory:  621640\n","Episode:  1420  , Epsilon:  0.01 , Reward -23.948655852980767 , mean_reward:  -13.922006943581366 , time_score:  500 , memory:  624140\n","Episode:  1425  , Epsilon:  0.01 , Reward -34.53595714202758 , mean_reward:  -13.977490810479692 , time_score:  500 , memory:  626640\n","Episode:  1430  , Epsilon:  0.01 , Reward -31.43217681039721 , mean_reward:  -16.39750420438803 , time_score:  500 , memory:  629140\n","Episode:  1435  , Epsilon:  0.01 , Reward 23.1139000410855 , mean_reward:  -16.061967452350117 , time_score:  500 , memory:  631640\n","Episode:  1440  , Epsilon:  0.01 , Reward 42.81952368035931 , mean_reward:  -19.270575245909246 , time_score:  500 , memory:  634140\n","Episode:  1445  , Epsilon:  0.01 , Reward 23.323115180357743 , mean_reward:  -19.997569596928006 , time_score:  500 , memory:  636640\n","Episode:  1450  , Epsilon:  0.01 , Reward -22.787137474566823 , mean_reward:  -21.423524262468792 , time_score:  500 , memory:  639140\n","Episode:  1455  , Epsilon:  0.01 , Reward -66.85334933748189 , mean_reward:  -20.359107791614168 , time_score:  500 , memory:  641640\n","Episode:  1460  , Epsilon:  0.01 , Reward -44.868132133505256 , mean_reward:  -20.345945815371394 , time_score:  500 , memory:  644140\n","Episode:  1465  , Epsilon:  0.01 , Reward -52.04328742055344 , mean_reward:  -20.563991284707832 , time_score:  500 , memory:  646640\n","Episode:  1470  , Epsilon:  0.01 , Reward -35.77057138806437 , mean_reward:  -20.067695365740025 , time_score:  500 , memory:  649140\n","Episode:  1475  , Epsilon:  0.01 , Reward -14.517827370826726 , mean_reward:  -20.099031626817588 , time_score:  500 , memory:  651640\n","Episode:  1480  , Epsilon:  0.01 , Reward -62.266108308278575 , mean_reward:  -19.774078551350478 , time_score:  500 , memory:  654140\n","Episode:  1485  , Epsilon:  0.01 , Reward -3.1704096769105856 , mean_reward:  -22.248990372240034 , time_score:  500 , memory:  656640\n","Episode:  1490  , Epsilon:  0.01 , Reward 8.529080343374906 , mean_reward:  -21.090967170971844 , time_score:  500 , memory:  659140\n","Episode:  1495  , Epsilon:  0.01 , Reward -38.49149360588679 , mean_reward:  -22.180923706329708 , time_score:  500 , memory:  661640\n","Episode:  1500  , Epsilon:  0.01 , Reward -39.64212950572841 , mean_reward:  -22.057530269054045 , time_score:  500 , memory:  664140\n","Episode:  1505  , Epsilon:  0.01 , Reward -29.733668610705415 , mean_reward:  -21.386381020308747 , time_score:  500 , memory:  666640\n","Episode:  1510  , Epsilon:  0.01 , Reward -28.688179799614105 , mean_reward:  -20.128482180075167 , time_score:  500 , memory:  669140\n","Episode:  1515  , Epsilon:  0.01 , Reward -1.389362459300414 , mean_reward:  -18.761203445273807 , time_score:  500 , memory:  671640\n","Episode:  1520  , Epsilon:  0.01 , Reward 52.25630571792904 , mean_reward:  -18.13341244427326 , time_score:  500 , memory:  674140\n","Episode:  1525  , Epsilon:  0.01 , Reward 25.068555507339045 , mean_reward:  -17.995746148771598 , time_score:  500 , memory:  676640\n","Episode:  1530  , Epsilon:  0.01 , Reward 55.13491191440124 , mean_reward:  -15.664278971445041 , time_score:  500 , memory:  679140\n","Episode:  1535  , Epsilon:  0.01 , Reward -28.39274295870746 , mean_reward:  -15.425829949361296 , time_score:  500 , memory:  681640\n","Episode:  1540  , Epsilon:  0.01 , Reward -13.356076510295532 , mean_reward:  -14.100773022593769 , time_score:  500 , memory:  684140\n","Episode:  1545  , Epsilon:  0.01 , Reward -10.944882500590225 , mean_reward:  -12.872083703569583 , time_score:  500 , memory:  686640\n","Episode:  1550  , Epsilon:  0.01 , Reward 58.5736028385991 , mean_reward:  -11.253518258876351 , time_score:  500 , memory:  689140\n","Episode:  1555  , Epsilon:  0.01 , Reward -3.568192459853681 , mean_reward:  -10.615483994390452 , time_score:  500 , memory:  691640\n","Episode:  1560  , Epsilon:  0.01 , Reward -45.02396372283432 , mean_reward:  -8.466894354489082 , time_score:  500 , memory:  694140\n","Episode:  1565  , Epsilon:  0.01 , Reward 5.139201746710553 , mean_reward:  -5.833204470491279 , time_score:  500 , memory:  696640\n","Episode:  1570  , Epsilon:  0.01 , Reward -11.672935350704176 , mean_reward:  -4.896687776549217 , time_score:  500 , memory:  699140\n","Episode:  1575  , Epsilon:  0.01 , Reward -31.450806817145498 , mean_reward:  -4.928427654442224 , time_score:  500 , memory:  701640\n","Episode:  1580  , Epsilon:  0.01 , Reward -30.419040239203476 , mean_reward:  -4.57157440504898 , time_score:  500 , memory:  704140\n","Episode:  1585  , Epsilon:  0.01 , Reward -9.123862824248747 , mean_reward:  -2.5087392020139223 , time_score:  500 , memory:  706640\n","Episode:  1590  , Epsilon:  0.01 , Reward 22.06146852768736 , mean_reward:  -1.3711857082107064 , time_score:  500 , memory:  709140\n","Episode:  1595  , Epsilon:  0.01 , Reward 16.417332536466162 , mean_reward:  0.37791551504945153 , time_score:  500 , memory:  711640\n","Episode:  1600  , Epsilon:  0.01 , Reward 54.094347923330254 , mean_reward:  2.290763782524343 , time_score:  500 , memory:  714140\n","Episode:  1605  , Epsilon:  0.01 , Reward 28.410146061463973 , mean_reward:  3.4328723394768614 , time_score:  500 , memory:  716640\n","Episode:  1610  , Epsilon:  0.01 , Reward 13.593278823782452 , mean_reward:  5.654841067426085 , time_score:  500 , memory:  719140\n","Episode:  1615  , Epsilon:  0.01 , Reward -37.28650072706079 , mean_reward:  5.756300916564089 , time_score:  500 , memory:  721640\n","Episode:  1620  , Epsilon:  0.01 , Reward -42.07683905787042 , mean_reward:  6.301105240820761 , time_score:  500 , memory:  724140\n","Episode:  1625  , Epsilon:  0.01 , Reward -6.967387531333853 , mean_reward:  6.494289042604318 , time_score:  500 , memory:  726640\n","Episode:  1630  , Epsilon:  0.01 , Reward -20.803035852984006 , mean_reward:  5.47746062583025 , time_score:  500 , memory:  729140\n","Episode:  1635  , Epsilon:  0.01 , Reward -5.702910277905936 , mean_reward:  6.433854857292759 , time_score:  500 , memory:  731640\n","Episode:  1640  , Epsilon:  0.01 , Reward 27.981303415667032 , mean_reward:  6.724640928812287 , time_score:  500 , memory:  734140\n","Episode:  1645  , Epsilon:  0.01 , Reward -4.069228055381993 , mean_reward:  6.461079449062321 , time_score:  500 , memory:  736640\n","Episode:  1650  , Epsilon:  0.01 , Reward 39.79025631051347 , mean_reward:  8.19870439624676 , time_score:  500 , memory:  739140\n","Episode:  1655  , Epsilon:  0.01 , Reward -31.614430351061106 , mean_reward:  6.709478461013078 , time_score:  500 , memory:  741640\n","Episode:  1660  , Epsilon:  0.01 , Reward 52.906269986418636 , mean_reward:  8.361863971613083 , time_score:  500 , memory:  744140\n","Episode:  1665  , Epsilon:  0.01 , Reward 68.9147716541476 , mean_reward:  9.059457117588853 , time_score:  500 , memory:  746640\n","Episode:  1670  , Epsilon:  0.01 , Reward -0.09650217323485233 , mean_reward:  9.008225050278222 , time_score:  500 , memory:  749140\n","Episode:  1675  , Epsilon:  0.01 , Reward -4.610323079804565 , mean_reward:  9.00188679098337 , time_score:  500 , memory:  751640\n","Episode:  1680  , Epsilon:  0.01 , Reward 36.49913750262133 , mean_reward:  10.457196795123474 , time_score:  500 , memory:  754140\n","Episode:  1685  , Epsilon:  0.01 , Reward -64.65930751997703 , mean_reward:  9.490667718303316 , time_score:  500 , memory:  756640\n","Episode:  1690  , Epsilon:  0.01 , Reward -3.225919049381459 , mean_reward:  9.465596406090294 , time_score:  500 , memory:  759140\n","Episode:  1695  , Epsilon:  0.01 , Reward -39.16993048455049 , mean_reward:  8.567902330723701 , time_score:  500 , memory:  761640\n","Episode:  1700  , Epsilon:  0.01 , Reward 5.781793354510218 , mean_reward:  6.723343637843823 , time_score:  500 , memory:  764130\n","Episode:  1705  , Epsilon:  0.01 , Reward 38.62210611372275 , mean_reward:  5.768095285976957 , time_score:  500 , memory:  766630\n","Episode:  1710  , Epsilon:  0.01 , Reward 49.67461725024559 , mean_reward:  5.254840363095851 , time_score:  500 , memory:  769130\n","Episode:  1715  , Epsilon:  0.01 , Reward -24.988099553386178 , mean_reward:  5.7364446910012825 , time_score:  500 , memory:  771630\n","Episode:  1720  , Epsilon:  0.01 , Reward 50.10186549296993 , mean_reward:  7.882590809745778 , time_score:  500 , memory:  774130\n","Episode:  1725  , Epsilon:  0.01 , Reward 14.519741950924683 , mean_reward:  8.013293680127068 , time_score:  500 , memory:  776630\n","Episode:  1730  , Epsilon:  0.01 , Reward -34.213947015786204 , mean_reward:  7.970272461603054 , time_score:  500 , memory:  779130\n","Episode:  1735  , Epsilon:  0.01 , Reward 7.25696519275919 , mean_reward:  8.439555062626129 , time_score:  500 , memory:  781630\n","Episode:  1740  , Epsilon:  0.01 , Reward -10.42392135385786 , mean_reward:  7.90846612326506 , time_score:  500 , memory:  784130\n","Episode:  1745  , Epsilon:  0.01 , Reward -170.37157003541108 , mean_reward:  4.122089062774741 , time_score:  469 , memory:  786546\n","Episode:  1750  , Epsilon:  0.01 , Reward 25.309951834511036 , mean_reward:  3.94682813826082 , time_score:  500 , memory:  789046\n","Episode:  1755  , Epsilon:  0.01 , Reward -6.843432818309488 , mean_reward:  4.128270812211213 , time_score:  500 , memory:  791530\n","Episode:  1760  , Epsilon:  0.01 , Reward -22.09990348409487 , mean_reward:  1.4962169533466478 , time_score:  500 , memory:  793995\n","Episode:  1765  , Epsilon:  0.01 , Reward 1.6479374398325735 , mean_reward:  -0.4943511317127615 , time_score:  500 , memory:  796495\n","Episode:  1770  , Epsilon:  0.01 , Reward 4.004505793370401 , mean_reward:  -0.5812703915795151 , time_score:  500 , memory:  798995\n","Episode:  1775  , Epsilon:  0.01 , Reward 26.359466344584508 , mean_reward:  0.20711206569061807 , time_score:  500 , memory:  801495\n","Episode:  1780  , Epsilon:  0.01 , Reward 8.231073688780693 , mean_reward:  -0.7372828771740442 , time_score:  500 , memory:  803995\n","Episode:  1785  , Epsilon:  0.01 , Reward -36.76341878748842 , mean_reward:  -0.3563430012582441 , time_score:  500 , memory:  806495\n","Episode:  1790  , Epsilon:  0.01 , Reward -61.98381703154326 , mean_reward:  -2.3415793841788535 , time_score:  500 , memory:  808995\n","Episode:  1795  , Epsilon:  0.01 , Reward -17.501474179599338 , mean_reward:  -2.863777768897899 , time_score:  500 , memory:  811495\n","Episode:  1800  , Epsilon:  0.01 , Reward 20.537514234870176 , mean_reward:  -2.5322546320107646 , time_score:  500 , memory:  813995\n","Episode:  1805  , Epsilon:  0.01 , Reward -6.049340809224358 , mean_reward:  -1.5845209286778748 , time_score:  500 , memory:  816495\n","Episode:  1810  , Epsilon:  0.01 , Reward -26.37570875344964 , mean_reward:  -2.1423319755467403 , time_score:  500 , memory:  818995\n","Episode:  1815  , Epsilon:  0.01 , Reward 33.02707042486371 , mean_reward:  -1.2994316315903194 , time_score:  500 , memory:  821495\n","Episode:  1820  , Epsilon:  0.01 , Reward -14.851343334085204 , mean_reward:  -4.434202958222187 , time_score:  500 , memory:  823995\n","Episode:  1825  , Epsilon:  0.01 , Reward 46.6958102493212 , mean_reward:  -3.082353252377885 , time_score:  500 , memory:  826495\n","Episode:  1830  , Epsilon:  0.01 , Reward -140.70460688027333 , mean_reward:  -3.5828820358150724 , time_score:  467 , memory:  828962\n","Episode:  1835  , Epsilon:  0.01 , Reward 37.73430630796288 , mean_reward:  -3.240738973764828 , time_score:  500 , memory:  831462\n","Episode:  1840  , Epsilon:  0.01 , Reward 43.66991619453789 , mean_reward:  -2.5734092153007238 , time_score:  500 , memory:  833962\n","Episode:  1845  , Epsilon:  0.01 , Reward 37.47364585021374 , mean_reward:  2.40515653916156 , time_score:  500 , memory:  836462\n","Episode:  1850  , Epsilon:  0.01 , Reward 27.09653703894407 , mean_reward:  2.5384848661968817 , time_score:  500 , memory:  838962\n","Episode:  1855  , Epsilon:  0.01 , Reward 32.8684824776726 , mean_reward:  4.645413021546222 , time_score:  500 , memory:  841462\n","Episode:  1860  , Epsilon:  0.01 , Reward 16.391629640294337 , mean_reward:  7.144791484800823 , time_score:  500 , memory:  843962\n","Episode:  1865  , Epsilon:  0.01 , Reward 27.00089156465771 , mean_reward:  8.846733700303394 , time_score:  500 , memory:  846462\n","Episode:  1870  , Epsilon:  0.01 , Reward 32.85389437025621 , mean_reward:  11.391503084438632 , time_score:  500 , memory:  848962\n","Episode:  1875  , Epsilon:  0.01 , Reward 56.96249146061147 , mean_reward:  12.153992979249155 , time_score:  500 , memory:  851462\n","Episode:  1880  , Epsilon:  0.01 , Reward 3.013735993008762 , mean_reward:  12.74128147451993 , time_score:  500 , memory:  853962\n","Episode:  1885  , Epsilon:  0.01 , Reward 24.909867125292585 , mean_reward:  13.747395406591629 , time_score:  500 , memory:  856462\n","Episode:  1890  , Epsilon:  0.01 , Reward 1.7759545150660407 , mean_reward:  14.804982091863465 , time_score:  500 , memory:  858962\n","Episode:  1895  , Epsilon:  0.01 , Reward 59.31572577086641 , mean_reward:  17.447325133035086 , time_score:  500 , memory:  861462\n","Episode:  1900  , Epsilon:  0.01 , Reward -94.28997864615529 , mean_reward:  16.926250409635276 , time_score:  483 , memory:  863945\n","Episode:  1905  , Epsilon:  0.01 , Reward 58.3581266144218 , mean_reward:  18.193231170058606 , time_score:  500 , memory:  866445\n","Episode:  1910  , Epsilon:  0.01 , Reward 20.13293239814146 , mean_reward:  19.822845885598454 , time_score:  500 , memory:  868945\n","Episode:  1915  , Epsilon:  0.01 , Reward -22.739444276516682 , mean_reward:  18.505095935997314 , time_score:  500 , memory:  871445\n","Episode:  1920  , Epsilon:  0.01 , Reward 62.52030086399335 , mean_reward:  19.479541532406056 , time_score:  500 , memory:  873945\n","Episode:  1925  , Epsilon:  0.01 , Reward 71.53982670114469 , mean_reward:  20.250445762901478 , time_score:  500 , memory:  876445\n","Episode:  1930  , Epsilon:  0.01 , Reward -52.19860020644909 , mean_reward:  22.459527924091727 , time_score:  500 , memory:  878945\n","Episode:  1935  , Epsilon:  0.01 , Reward -4.845780614910059 , mean_reward:  22.96166618680598 , time_score:  500 , memory:  881445\n","Episode:  1940  , Epsilon:  0.01 , Reward 84.53772536179812 , mean_reward:  23.89588108979634 , time_score:  500 , memory:  883945\n","Episode:  1945  , Epsilon:  0.01 , Reward 56.49358748162897 , mean_reward:  23.020233295829662 , time_score:  500 , memory:  886445\n","Episode:  1950  , Epsilon:  0.01 , Reward 65.02180160178702 , mean_reward:  23.75817583680464 , time_score:  500 , memory:  888945\n","Episode:  1955  , Epsilon:  0.01 , Reward 35.05864119674544 , mean_reward:  23.741363001757207 , time_score:  500 , memory:  891445\n","Episode:  1960  , Epsilon:  0.01 , Reward 70.06149510829862 , mean_reward:  23.721096325308935 , time_score:  500 , memory:  893945\n","Episode:  1965  , Epsilon:  0.01 , Reward 71.66290647537086 , mean_reward:  24.51408428357834 , time_score:  500 , memory:  896445\n","Episode:  1970  , Epsilon:  0.01 , Reward 71.2153276371606 , mean_reward:  24.867027930236922 , time_score:  500 , memory:  898945\n","Episode:  1975  , Epsilon:  0.01 , Reward 66.88592999444246 , mean_reward:  27.4974853738406 , time_score:  500 , memory:  901445\n","Episode:  1980  , Epsilon:  0.01 , Reward 56.89533240673317 , mean_reward:  29.753226172468704 , time_score:  500 , memory:  903945\n","Episode:  1985  , Epsilon:  0.01 , Reward -2.2508360274526824 , mean_reward:  31.610088283691844 , time_score:  500 , memory:  906445\n","Episode:  1990  , Epsilon:  0.01 , Reward -6.078386891629133 , mean_reward:  34.48663384312334 , time_score:  500 , memory:  908945\n","Episode:  1995  , Epsilon:  0.01 , Reward 37.72313803936648 , mean_reward:  36.014478193598535 , time_score:  500 , memory:  911445\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}