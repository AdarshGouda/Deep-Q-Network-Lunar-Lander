{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_0.995_0.1_0.001.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1FliYvrl6sYx1TkoZq_rvl69TU-7wY_p0","authorship_tag":"ABX9TyNfaKXo8Y8ppMvU1kBhdbVQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624385880165,"user_tz":360,"elapsed":34110,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4903c42e-8fec-433d-bb83-13eeba9fcc0a"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624647768541,"user_tz":360,"elapsed":5976,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"684d62ee-1e21-455d-a470-b2ceae294e1f"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BovO1DfbwF0O","executionInfo":{"status":"ok","timestamp":1624647768543,"user_tz":360,"elapsed":6,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8","executionInfo":{"status":"ok","timestamp":1624647768849,"user_tz":360,"elapsed":311,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.1_0.001/0p1_0p001.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.1_0.001/0p1_0p001.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","   "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624656085730,"user_tz":360,"elapsed":8316883,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"c3b2cb05-3005-44be-e4d3-1ae32b273b49"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1, epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.1, lr=0.001)\n","df = dqn.play()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -295.7941029151219 , mean_reward:  -295.7941029151219 , time_score:  100 , memory:  100\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -266.88700732639984 , mean_reward:  -273.2882881728287 , time_score:  110 , memory:  619\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -212.24782792048282 , mean_reward:  -242.97769828644624 , time_score:  82 , memory:  1110\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -198.18578680651865 , mean_reward:  -208.80957644872026 , time_score:  73 , memory:  1549\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward -216.82499627464483 , mean_reward:  -204.82349874510362 , time_score:  110 , memory:  2062\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -162.9363623360976 , mean_reward:  -189.59738104146697 , time_score:  95 , memory:  2561\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -119.51454694304212 , mean_reward:  -183.48098870537876 , time_score:  96 , memory:  2989\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -183.48500097583437 , mean_reward:  -184.77494384552494 , time_score:  107 , memory:  3531\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -114.97321716706915 , mean_reward:  -174.88613281675785 , time_score:  106 , memory:  4048\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -84.74201165792407 , mean_reward:  -171.38145522740845 , time_score:  91 , memory:  4606\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -114.76770791492173 , mean_reward:  -164.49890431394627 , time_score:  82 , memory:  5098\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -106.55302083385996 , mean_reward:  -155.8875169944607 , time_score:  119 , memory:  5624\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -94.57858469287153 , mean_reward:  -151.2986768204842 , time_score:  114 , memory:  6114\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -145.19896268650223 , mean_reward:  -147.0406740725704 , time_score:  116 , memory:  6718\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -44.56083988039974 , mean_reward:  -143.98377861153654 , time_score:  133 , memory:  7426\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward -163.7318500042208 , mean_reward:  -139.7080680159283 , time_score:  118 , memory:  8090\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -110.00345001759872 , mean_reward:  -138.36121726004652 , time_score:  86 , memory:  8729\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -99.25976696283877 , mean_reward:  -133.86067123090845 , time_score:  112 , memory:  9409\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -80.26417217934828 , mean_reward:  -128.97993358798934 , time_score:  128 , memory:  10168\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -58.15353372786875 , mean_reward:  -127.21479894661404 , time_score:  113 , memory:  10759\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward -19.491183555902268 , mean_reward:  -121.54130015382617 , time_score:  146 , memory:  11471\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -13.585460360424179 , mean_reward:  -111.14657114907116 , time_score:  192 , memory:  12013\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward -35.81014367959324 , mean_reward:  -103.24073901836637 , time_score:  155 , memory:  12937\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward -69.47051981898193 , mean_reward:  -98.94894603264531 , time_score:  116 , memory:  13868\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -129.91362260216331 , mean_reward:  -92.71133870917883 , time_score:  131 , memory:  14672\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward -3.4465795778761503 , mean_reward:  -89.40511010642342 , time_score:  179 , memory:  15505\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -37.84820143777421 , mean_reward:  -85.51600247611343 , time_score:  137 , memory:  16387\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -22.886790260574045 , mean_reward:  -79.28932537165609 , time_score:  216 , memory:  17373\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -54.56266852961345 , mean_reward:  -76.76134535859836 , time_score:  134 , memory:  18275\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -110.89181420526175 , mean_reward:  -71.90981986745186 , time_score:  236 , memory:  19471\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward 51.25493734719453 , mean_reward:  -68.02982086543602 , time_score:  500 , memory:  21440\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward 87.40761295992807 , mean_reward:  -63.78102773991438 , time_score:  500 , memory:  23606\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward -38.60739631504045 , mean_reward:  -60.6453552601014 , time_score:  297 , memory:  24668\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward 64.71812970779324 , mean_reward:  -58.48342203886399 , time_score:  500 , memory:  26376\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward -83.77162474963178 , mean_reward:  -56.21958580414987 , time_score:  175 , memory:  27881\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward -30.40368273706339 , mean_reward:  -55.49654470011702 , time_score:  125 , memory:  29211\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward -144.22695653280655 , mean_reward:  -49.362643918589136 , time_score:  418 , memory:  31629\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward 88.35908341978762 , mean_reward:  -43.95318053741813 , time_score:  500 , memory:  33810\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward 59.92956765512585 , mean_reward:  -39.185275569761735 , time_score:  500 , memory:  36057\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward 55.097649717973276 , mean_reward:  -32.98436576445795 , time_score:  500 , memory:  38285\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward 107.8442479873079 , mean_reward:  -26.633127488072237 , time_score:  500 , memory:  40785\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward 42.81930682653426 , mean_reward:  -22.525693004680953 , time_score:  500 , memory:  42705\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward 76.29842445626367 , mean_reward:  -18.262723406738232 , time_score:  500 , memory:  45016\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward 90.22074371162293 , mean_reward:  -12.16513082543818 , time_score:  500 , memory:  47516\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward 101.17636147138045 , mean_reward:  -4.768519463378779 , time_score:  500 , memory:  50016\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward 125.21863213335276 , mean_reward:  2.6284450030531046 , time_score:  500 , memory:  52516\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward 12.971424095785615 , mean_reward:  8.658596741983864 , time_score:  500 , memory:  54821\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward 90.33453497186754 , mean_reward:  15.255138279839468 , time_score:  500 , memory:  57321\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward 84.53129948750669 , mean_reward:  20.382837902220714 , time_score:  500 , memory:  59452\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward 94.84226158652854 , mean_reward:  28.521028861592168 , time_score:  500 , memory:  61920\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward 104.72869586831995 , mean_reward:  30.922861442485246 , time_score:  500 , memory:  64329\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward -19.630326488740454 , mean_reward:  34.61208317152108 , time_score:  500 , memory:  66766\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward 100.8933783609867 , mean_reward:  39.73245699610798 , time_score:  500 , memory:  69266\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward -134.9442700710346 , mean_reward:  45.27711401233796 , time_score:  312 , memory:  71578\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward 68.3427270771965 , mean_reward:  53.48526423003798 , time_score:  500 , memory:  74078\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward 84.97903070908905 , mean_reward:  61.68097642592062 , time_score:  500 , memory:  76578\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward 78.3976775546612 , mean_reward:  65.55761924489157 , time_score:  500 , memory:  79078\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward 45.18233790165628 , mean_reward:  66.31816640340762 , time_score:  500 , memory:  81578\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward 26.56538145935778 , mean_reward:  67.89754991557442 , time_score:  115 , memory:  83693\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward 108.41971289083705 , mean_reward:  70.83153012478029 , time_score:  500 , memory:  86193\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward 111.39207661680973 , mean_reward:  69.99493659189578 , time_score:  500 , memory:  88348\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward 26.913134375924418 , mean_reward:  72.84424241369842 , time_score:  500 , memory:  90848\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward 83.66798234203549 , mean_reward:  75.14757594009085 , time_score:  500 , memory:  93348\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward 66.51096708685617 , mean_reward:  75.72000857588063 , time_score:  134 , memory:  95482\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward 45.04053758327324 , mean_reward:  76.40448757545045 , time_score:  500 , memory:  97982\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward 75.63382912444253 , mean_reward:  75.93838878446581 , time_score:  500 , memory:  100482\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward 69.75170392355979 , mean_reward:  77.43033898727644 , time_score:  500 , memory:  102982\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward 48.82079744238644 , mean_reward:  78.51330504013283 , time_score:  500 , memory:  105482\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward 88.1135557866479 , mean_reward:  80.75472510080854 , time_score:  500 , memory:  107982\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward 89.75783192319008 , mean_reward:  78.63806061670765 , time_score:  500 , memory:  110482\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward 104.26842361702218 , mean_reward:  82.04139222342297 , time_score:  500 , memory:  112982\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward 69.6950282594342 , mean_reward:  82.2240110829986 , time_score:  500 , memory:  115089\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward 71.2681761932016 , mean_reward:  83.44860942597933 , time_score:  500 , memory:  117589\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward 124.9255528265623 , mean_reward:  84.62638115491286 , time_score:  500 , memory:  120089\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward 91.46790620764173 , mean_reward:  84.20596367115817 , time_score:  500 , memory:  122589\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward 72.73830973520658 , mean_reward:  88.42370645737442 , time_score:  500 , memory:  124834\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward 106.77671912711618 , mean_reward:  89.79403262069565 , time_score:  500 , memory:  127334\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward 86.16336378369392 , mean_reward:  92.41356665703167 , time_score:  500 , memory:  129834\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward 141.4390671531485 , mean_reward:  95.69423907387818 , time_score:  500 , memory:  132248\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward 292.5290970812702 , mean_reward:  96.91130253525832 , time_score:  377 , memory:  134175\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward 193.00681587527464 , mean_reward:  100.61920266300979 , time_score:  435 , memory:  136610\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward 262.72051030007526 , mean_reward:  103.59677512537179 , time_score:  359 , memory:  138969\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward 178.65899887235275 , mean_reward:  108.73535597371865 , time_score:  500 , memory:  141278\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward 215.7421055670055 , mean_reward:  111.03847384281626 , time_score:  411 , memory:  143228\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward 102.44779262015291 , mean_reward:  112.1744445598317 , time_score:  500 , memory:  145610\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward 126.81279605087404 , mean_reward:  114.75140087874182 , time_score:  500 , memory:  148047\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward 130.4375186316149 , mean_reward:  116.94040388533767 , time_score:  500 , memory:  150547\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward 260.39566413138846 , mean_reward:  124.74725619508507 , time_score:  369 , memory:  152501\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward 261.8100225732629 , mean_reward:  130.32044317648138 , time_score:  328 , memory:  154601\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward 253.1248133362065 , mean_reward:  137.65069856588553 , time_score:  343 , memory:  156748\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward 150.17658951374378 , mean_reward:  143.4007928888441 , time_score:  500 , memory:  158759\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward -179.19232517415134 , mean_reward:  143.5239602955974 , time_score:  467 , memory:  160922\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward 228.94040453012167 , mean_reward:  152.1230477967888 , time_score:  414 , memory:  162790\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward 113.45782900487589 , mean_reward:  152.3990079894539 , time_score:  500 , memory:  164670\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward 228.96621896144342 , mean_reward:  155.5793462484036 , time_score:  276 , memory:  166764\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward 274.0133594305072 , mean_reward:  158.52969273341535 , time_score:  422 , memory:  168846\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward 256.3894231970296 , mean_reward:  165.73529529957912 , time_score:  313 , memory:  170760\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward 252.65280036443727 , mean_reward:  171.16527587994216 , time_score:  334 , memory:  172802\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward 199.81104916359746 , mean_reward:  175.4968673120722 , time_score:  291 , memory:  174695\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward 261.4065734921704 , mean_reward:  181.26811190956204 , time_score:  370 , memory:  176852\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward 240.14956234796074 , mean_reward:  185.94025413553686 , time_score:  427 , memory:  178965\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward 271.93260619573255 , mean_reward:  190.03095996253057 , time_score:  277 , memory:  180844\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward 253.61456381559353 , mean_reward:  192.8852188400059 , time_score:  251 , memory:  182598\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward 123.38079732067831 , mean_reward:  194.90714591233777 , time_score:  500 , memory:  184713\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward 200.1020248709165 , mean_reward:  198.97448259162576 , time_score:  483 , memory:  186961\n","BRAVO, GOAL ACHIEVED!!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QzXeEPyZkZx5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"3P7Ud5kSDIL0","executionInfo":{"status":"error","timestamp":1624656314525,"user_tz":360,"elapsed":546,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"5ed2ce04-1e97-429e-b167-b88580da4b13"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"]}]},{"cell_type":"code","metadata":{"id":"dmu7jobCkZ0S","executionInfo":{"status":"ok","timestamp":1624656361094,"user_tz":360,"elapsed":783,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["df.to_csv(\"0.995_0.1_0.001.csv\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}