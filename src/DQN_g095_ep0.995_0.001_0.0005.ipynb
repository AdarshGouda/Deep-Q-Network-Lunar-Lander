{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5526,
     "status": "ok",
     "timestamp": 1624403011807,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "mWJAoAVDkEZV",
    "outputId": "9d8f7137-15c6-4a26-89e7-b307d7cda3b0"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import random\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this code to disable the GPU execution\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1624403019276,
     "user": {
      "displayName": "Adarsh Gouda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64",
      "userId": "10706865863009541265"
     },
     "user_tz": 360
    },
    "id": "skFSI-YokZl8"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n",
    "                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n",
    "        \n",
    "        self.ep = epsilon\n",
    "        self.ep_decay = epsilon_decay\n",
    "        self.ep_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.episodes = episodes\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.retrain = retrain\n",
    "        \n",
    "        self.frames = []\n",
    "        \n",
    "        seed = 983827\n",
    "        mem = 1000000\n",
    "\n",
    "        self.csv_filename = \"g95_ep0p995_0p001_0p0005.csv\"\n",
    "        self.model_filename = \"g95_ep0p995_0p001_0p0005.h5\"\n",
    "\n",
    "        \n",
    "        self.env = gym.make(game)\n",
    "        self.env.seed(seed)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "        \n",
    "        print(\"state size is: \",self.nS)\n",
    "        print(\"action size is: \", self.nA)\n",
    "       \n",
    "        \n",
    "        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n",
    "\n",
    "        if self.retrain == False:\n",
    "          self.Q_model = self.setup_dnn()\n",
    "          self.Q_hat_model = self.setup_dnn()\n",
    "          print(\"NEW MODEL CREATED!\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "          self.Q_model = tf.keras.models.load_model(self.model_filename)\n",
    "          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n",
    "          print(\"MODEL LOADED!\")\n",
    "          self.Q_model.summary()\n",
    "\n",
    "\n",
    "        self.counter = 0\n",
    "        self.update_freq = 4\n",
    "\n",
    "        \n",
    "        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n",
    "        \n",
    "    def setup_dnn(self):\n",
    "        \n",
    "        input_ = tf.keras.layers.Input(shape = (self.nS))\n",
    "        \n",
    "        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n",
    "        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n",
    "        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n",
    "        \n",
    "        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n",
    "        opt_ = tf.keras.optimizers.Adam(self.lr)\n",
    "        model_.compile(optimizer = opt_, loss = \"mse\")\n",
    "        \n",
    "        return model_\n",
    "    \n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n",
    "            \n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def weights_update(self):\n",
    "        Q_w = self.Q_model.get_weights()\n",
    "        Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "        for w in range(len(Q_hat_w)):\n",
    "            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "        self.Q_hat_model.set_weights(Q_hat_weights)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.ep > self.ep_min:\n",
    "            self.ep *= self.ep_decay\n",
    "        \n",
    "        samples = random.choices(self.memory, k = self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            end_target = self.model.predict(state)\n",
    "            end_target[0][action] = target\n",
    "            \n",
    "            self.history = self.model.fit(state, end_target, verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    def learn_batch(self):\n",
    "             \n",
    "        self.counter = (self.counter + 1) % self.update_freq\n",
    "        \n",
    "        if self.counter == 0:\n",
    "            #print(\"Learning...\")\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            states, end_targets = [], []\n",
    "            \n",
    "            samples = random.choices(self.memory, k = self.batch_size)\n",
    "            \n",
    "            for state, action, reward, next_state, done in samples:\n",
    "                target = reward\n",
    "            \n",
    "                if not done:\n",
    "                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n",
    "            \n",
    "                end_target = self.Q_model.predict(state)\n",
    "                end_target[0][action] = target\n",
    "                \n",
    "                states.append(state[0])\n",
    "                end_targets.append(end_target[0])\n",
    "            \n",
    "            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n",
    "            \n",
    "            Q_w = self.Q_model.get_weights()\n",
    "            Q_hat_w = self.Q_hat_model.get_weights()\n",
    "        \n",
    "            for w in range(len(Q_hat_w)):\n",
    "                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n",
    "        \n",
    "            self.Q_hat_model.set_weights(Q_hat_w)\n",
    "    \n",
    "    \n",
    "    def play(self): \n",
    "        \n",
    "        new_row = {}\n",
    "        R = []\n",
    "        R_moving = deque(maxlen=100)\n",
    "        steps = 500\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            current_state = self.env.reset()\n",
    "            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n",
    "         \n",
    "            time = 0\n",
    "            r = 0\n",
    "            \n",
    "            for s in range(steps):\n",
    "\n",
    "                action_ = self.action(current_state, self.ep)\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action_)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "                \n",
    "                self.store(current_state, action_, reward, next_state, done)\n",
    "                \n",
    "                r = r+reward\n",
    "                \n",
    "                #self.learn()\n",
    "                self.learn_batch()\n",
    "                \n",
    "                current_state = next_state\n",
    "                time = time+1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #self.learn_batch()\n",
    "            R.append(r)\n",
    "            R_moving.append(r)\n",
    "\n",
    "                    \n",
    "            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n",
    "            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n",
    "            \n",
    "            \n",
    "            if e % 5 == 0:\n",
    "              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n",
    "\n",
    "            if e % 100 == 0:\n",
    "\n",
    "              self.Q_model.save(self.model_filename)\n",
    "              \n",
    "\n",
    "            if self.ep > self.ep_min:\n",
    "              self.ep *= self.ep_decay\n",
    "            else:\n",
    "              self.ep = 0.01\n",
    "            \n",
    "            if np.mean(R_moving)>= 200.0:\n",
    "                print(\"BRAVO, GOAL ACHIEVED!!!\")\n",
    "                break\n",
    "\n",
    "        with open(self.csv_filename, 'a') as f:\n",
    "          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n",
    "             \n",
    "            \n",
    "        self.Q_model.save(self.model_filename)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        return self.df_ddqn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8Y5T6-ukZoN",
    "outputId": "08631fbc-a90e-4a07-d724-3d4eee9fcdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is:  8\n",
      "action size is:  4\n",
      "NEW MODEL CREATED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  , Epsilon:  1 , Reward -106.09645547351784 , mean_reward:  -106.09645547351784 , time_score:  81 , memory:  81\n",
      "Episode:  5  , Epsilon:  0.9752487531218751 , Reward -233.4030888582633 , mean_reward:  -141.04450670229767 , time_score:  107 , memory:  488\n",
      "Episode:  10  , Epsilon:  0.9511101304657719 , Reward -274.2387441768923 , mean_reward:  -162.21182791262044 , time_score:  82 , memory:  1348\n",
      "Episode:  15  , Epsilon:  0.9275689688183278 , Reward -302.2633937174548 , mean_reward:  -152.30618424895277 , time_score:  107 , memory:  1792\n",
      "Episode:  20  , Epsilon:  0.9046104802746175 , Reward -59.80649591076312 , mean_reward:  -153.5570652451657 , time_score:  109 , memory:  2262\n",
      "Episode:  25  , Epsilon:  0.8822202429488013 , Reward -97.49108595850512 , mean_reward:  -151.9941668443146 , time_score:  154 , memory:  2790\n",
      "Episode:  30  , Epsilon:  0.8603841919146962 , Reward -62.25164040616831 , mean_reward:  -152.83307005800478 , time_score:  98 , memory:  3286\n",
      "Episode:  35  , Epsilon:  0.8390886103705794 , Reward -596.2451844732494 , mean_reward:  -159.3247590998327 , time_score:  120 , memory:  3808\n",
      "Episode:  40  , Epsilon:  0.8183201210226743 , Reward -253.69273882960937 , mean_reward:  -159.10579868295133 , time_score:  130 , memory:  4355\n",
      "Episode:  45  , Epsilon:  0.798065677681905 , Reward -176.8663600086947 , mean_reward:  -167.65420092535476 , time_score:  97 , memory:  4818\n",
      "Episode:  50  , Epsilon:  0.778312557068642 , Reward -366.6835881497627 , mean_reward:  -165.95585091830839 , time_score:  147 , memory:  5492\n",
      "Episode:  55  , Epsilon:  0.7590483508202912 , Reward -163.57289616973026 , mean_reward:  -163.43058670797498 , time_score:  117 , memory:  5970\n",
      "Episode:  60  , Epsilon:  0.7402609576967045 , Reward -34.10904095881283 , mean_reward:  -159.21871507182797 , time_score:  130 , memory:  6517\n",
      "Episode:  65  , Epsilon:  0.7219385759785162 , Reward -117.2252201481015 , mean_reward:  -158.07502230781773 , time_score:  118 , memory:  7153\n",
      "Episode:  70  , Epsilon:  0.7040696960536299 , Reward -103.29932290190546 , mean_reward:  -157.63290909327623 , time_score:  111 , memory:  7718\n",
      "Episode:  75  , Epsilon:  0.6866430931872001 , Reward -123.30033481310707 , mean_reward:  -157.49019891833402 , time_score:  127 , memory:  8210\n",
      "Episode:  80  , Epsilon:  0.6696478204705644 , Reward -129.19814079592317 , mean_reward:  -155.42505267306035 , time_score:  75 , memory:  8787\n",
      "Episode:  85  , Epsilon:  0.653073201944699 , Reward -79.59670996853163 , mean_reward:  -154.1592712987037 , time_score:  133 , memory:  9425\n",
      "Episode:  90  , Epsilon:  0.6369088258938781 , Reward -2.2636981156748277 , mean_reward:  -153.16937578134835 , time_score:  132 , memory:  10162\n",
      "Episode:  95  , Epsilon:  0.6211445383053219 , Reward -277.6809431734292 , mean_reward:  -153.30613194545376 , time_score:  131 , memory:  10893\n",
      "Episode:  100  , Epsilon:  0.6057704364907278 , Reward -96.5210841766715 , mean_reward:  -151.36065252367067 , time_score:  133 , memory:  11576\n",
      "Episode:  105  , Epsilon:  0.5907768628656763 , Reward -106.3037039941129 , mean_reward:  -151.76929408489414 , time_score:  101 , memory:  12463\n",
      "Episode:  110  , Epsilon:  0.5761543988830038 , Reward -201.1351956019814 , mean_reward:  -147.77672707523578 , time_score:  116 , memory:  13479\n",
      "Episode:  115  , Epsilon:  0.5618938591163328 , Reward -331.9688365563314 , mean_reward:  -149.2884247389414 , time_score:  207 , memory:  14328\n",
      "Episode:  120  , Epsilon:  0.547986285490042 , Reward -162.84896553432628 , mean_reward:  -148.5540105535459 , time_score:  118 , memory:  14923\n",
      "Episode:  125  , Epsilon:  0.5344229416520513 , Reward -95.9214109837347 , mean_reward:  -144.90103920508926 , time_score:  124 , memory:  15572\n",
      "Episode:  130  , Epsilon:  0.5211953074858876 , Reward -209.99758966380227 , mean_reward:  -144.55076284819387 , time_score:  137 , memory:  16287\n",
      "Episode:  135  , Epsilon:  0.5082950737585841 , Reward -48.511946633492016 , mean_reward:  -138.89975476527667 , time_score:  246 , memory:  17479\n",
      "Episode:  140  , Epsilon:  0.49571413690105054 , Reward -275.075471667958 , mean_reward:  -137.33103663025247 , time_score:  126 , memory:  18219\n",
      "Episode:  145  , Epsilon:  0.483444593917636 , Reward 115.37421687181458 , mean_reward:  -128.19733506949777 , time_score:  500 , memory:  19332\n",
      "Episode:  150  , Epsilon:  0.47147873742168567 , Reward -79.73598097392062 , mean_reward:  -123.43762320664308 , time_score:  256 , memory:  20681\n",
      "Episode:  155  , Epsilon:  0.4598090507939749 , Reward -52.709177924084926 , mean_reward:  -118.74142345027887 , time_score:  113 , memory:  21528\n",
      "Episode:  160  , Epsilon:  0.4484282034609769 , Reward -56.113458578883595 , mean_reward:  -118.51896132966267 , time_score:  233 , memory:  22620\n",
      "Episode:  165  , Epsilon:  0.43732904629000013 , Reward -15.602696881425686 , mean_reward:  -116.59635127819763 , time_score:  258 , memory:  23921\n",
      "Episode:  170  , Epsilon:  0.42650460709830135 , Reward -117.8840580515032 , mean_reward:  -115.30267532952017 , time_score:  221 , memory:  24603\n",
      "Episode:  175  , Epsilon:  0.4159480862733536 , Reward -364.8421843949299 , mean_reward:  -113.53051826439943 , time_score:  133 , memory:  25396\n",
      "Episode:  180  , Epsilon:  0.40565285250151817 , Reward -12.600290284188631 , mean_reward:  -110.87169661698582 , time_score:  283 , memory:  26393\n",
      "Episode:  185  , Epsilon:  0.39561243860243744 , Reward -44.68803328920501 , mean_reward:  -107.60790897263463 , time_score:  234 , memory:  27773\n",
      "Episode:  190  , Epsilon:  0.3858205374665315 , Reward 55.92930873424302 , mean_reward:  -105.12226322942315 , time_score:  250 , memory:  28918\n",
      "Episode:  195  , Epsilon:  0.37627099809304654 , Reward -10.276613645149112 , mean_reward:  -102.50745259706427 , time_score:  302 , memory:  30039\n",
      "Episode:  200  , Epsilon:  0.3669578217261671 , Reward -177.78319414663002 , mean_reward:  -100.10780775114155 , time_score:  248 , memory:  31475\n",
      "Episode:  205  , Epsilon:  0.3578751580867638 , Reward -98.58918355961865 , mean_reward:  -99.80597475061977 , time_score:  213 , memory:  32289\n",
      "Episode:  210  , Epsilon:  0.34901730169741024 , Reward -151.06940677834194 , mean_reward:  -104.72272177057903 , time_score:  168 , memory:  33465\n",
      "Episode:  215  , Epsilon:  0.3403786882983606 , Reward -133.56845661164004 , mean_reward:  -100.10623725359156 , time_score:  207 , memory:  34740\n",
      "Episode:  220  , Epsilon:  0.33195389135223546 , Reward -40.16100970260743 , mean_reward:  -97.22432200651859 , time_score:  204 , memory:  35967\n",
      "Episode:  225  , Epsilon:  0.3237376186352221 , Reward -79.13351340919867 , mean_reward:  -98.04076982596314 , time_score:  245 , memory:  37342\n",
      "Episode:  230  , Epsilon:  0.3157247089126454 , Reward -192.07279500445458 , mean_reward:  -95.35670768295624 , time_score:  235 , memory:  38743\n",
      "Episode:  235  , Epsilon:  0.3079101286968243 , Reward -44.59573352832756 , mean_reward:  -90.21884537960732 , time_score:  223 , memory:  40731\n",
      "Episode:  240  , Epsilon:  0.30028896908517405 , Reward 114.03893753881033 , mean_reward:  -87.39819602337482 , time_score:  500 , memory:  42366\n",
      "Episode:  245  , Epsilon:  0.29285644267656924 , Reward -3.9018600188144177 , mean_reward:  -83.46340800318832 , time_score:  211 , memory:  44365\n",
      "Episode:  250  , Epsilon:  0.285607880564032 , Reward -107.78455680793499 , mean_reward:  -83.36589157246728 , time_score:  183 , memory:  45547\n",
      "Episode:  255  , Epsilon:  0.27853872940185365 , Reward -166.08534084686107 , mean_reward:  -86.53714401490754 , time_score:  380 , memory:  47554\n",
      "Episode:  260  , Epsilon:  0.27164454854530906 , Reward -14.29431669048114 , mean_reward:  -83.11355716406038 , time_score:  500 , memory:  49998\n",
      "Episode:  265  , Epsilon:  0.2649210072611673 , Reward 85.22738135586607 , mean_reward:  -78.74765947106707 , time_score:  500 , memory:  52071\n",
      "Episode:  270  , Epsilon:  0.2583638820072446 , Reward -266.51411122713284 , mean_reward:  -78.31776128352585 , time_score:  494 , memory:  54046\n",
      "Episode:  275  , Epsilon:  0.2519690537792925 , Reward -51.47926343945764 , mean_reward:  -74.29723291858481 , time_score:  500 , memory:  56067\n",
      "Episode:  280  , Epsilon:  0.2457325055235537 , Reward 8.957166341862873 , mean_reward:  -69.42169541085558 , time_score:  453 , memory:  58384\n",
      "Episode:  285  , Epsilon:  0.23965031961336 , Reward 47.95689185065993 , mean_reward:  -67.38593849596792 , time_score:  253 , memory:  59906\n",
      "Episode:  290  , Epsilon:  0.23371867538818816 , Reward -2.5398064534465306 , mean_reward:  -63.365724526024245 , time_score:  264 , memory:  61773\n",
      "Episode:  295  , Epsilon:  0.22793384675362674 , Reward 42.41115091400971 , mean_reward:  -57.46677322695365 , time_score:  201 , memory:  63853\n",
      "Episode:  300  , Epsilon:  0.22229219984074702 , Reward 29.900693577666004 , mean_reward:  -53.68741273415472 , time_score:  500 , memory:  66036\n",
      "Episode:  305  , Epsilon:  0.2167901907234072 , Reward 20.85133932101461 , mean_reward:  -45.73451848957498 , time_score:  500 , memory:  68217\n",
      "Episode:  310  , Epsilon:  0.21142436319205632 , Reward -33.03756371289718 , mean_reward:  -34.76083671706551 , time_score:  500 , memory:  70635\n",
      "Episode:  315  , Epsilon:  0.20619134658263935 , Reward -32.179239103627474 , mean_reward:  -31.112280753989904 , time_score:  500 , memory:  73060\n",
      "Episode:  320  , Epsilon:  0.2010878536592394 , Reward -38.56372993518389 , mean_reward:  -28.688371914308537 , time_score:  500 , memory:  75172\n",
      "Episode:  325  , Epsilon:  0.19611067854912728 , Reward -31.51439180998077 , mean_reward:  -25.84613108041705 , time_score:  500 , memory:  77672\n",
      "Episode:  330  , Epsilon:  0.1912566947289212 , Reward -41.670891121712366 , mean_reward:  -22.529964762834364 , time_score:  500 , memory:  80172\n",
      "Episode:  335  , Epsilon:  0.1865228530605915 , Reward 52.1550588244131 , mean_reward:  -25.57288059148113 , time_score:  500 , memory:  82327\n",
      "Episode:  340  , Epsilon:  0.18190617987607657 , Reward -3.88527181297857 , mean_reward:  -22.797576465028925 , time_score:  500 , memory:  84827\n",
      "Episode:  345  , Epsilon:  0.17740377510930716 , Reward 119.59643768490936 , mean_reward:  -23.608241775227192 , time_score:  500 , memory:  87227\n",
      "Episode:  350  , Epsilon:  0.1730128104744653 , Reward -14.681232017501983 , mean_reward:  -20.990251763689827 , time_score:  500 , memory:  89727\n",
      "Episode:  355  , Epsilon:  0.16873052768933355 , Reward -27.034002825460274 , mean_reward:  -16.04509464920553 , time_score:  500 , memory:  92227\n",
      "Episode:  360  , Epsilon:  0.16455423674261854 , Reward -16.904233898143367 , mean_reward:  -14.130514430616664 , time_score:  500 , memory:  94727\n",
      "Episode:  365  , Epsilon:  0.16048131420416054 , Reward 20.044213300109913 , mean_reward:  -12.836231891589728 , time_score:  500 , memory:  96916\n",
      "Episode:  370  , Epsilon:  0.15650920157696743 , Reward 25.875018488186274 , mean_reward:  -7.250861930048627 , time_score:  500 , memory:  99416\n",
      "Episode:  375  , Epsilon:  0.1526354036900377 , Reward -10.969354382664946 , mean_reward:  -5.339707132137083 , time_score:  500 , memory:  101916\n",
      "Episode:  380  , Epsilon:  0.14885748713096328 , Reward -60.199038406610384 , mean_reward:  -7.584879362154521 , time_score:  500 , memory:  104416\n",
      "Episode:  385  , Epsilon:  0.1451730787173275 , Reward -93.86569671281869 , mean_reward:  -7.83695136007846 , time_score:  500 , memory:  106916\n",
      "Episode:  390  , Epsilon:  0.14157986400593744 , Reward -35.28174360158806 , mean_reward:  -9.020144008209884 , time_score:  500 , memory:  109416\n",
      "Episode:  395  , Epsilon:  0.13807558583895513 , Reward -27.632766921019925 , mean_reward:  -10.343478798576287 , time_score:  500 , memory:  111916\n",
      "Episode:  400  , Epsilon:  0.1346580429260134 , Reward 20.850267862999083 , mean_reward:  -12.91751075202226 , time_score:  500 , memory:  114416\n",
      "Episode:  405  , Epsilon:  0.1313250884614265 , Reward -53.880786932352436 , mean_reward:  -14.075297891593532 , time_score:  500 , memory:  116916\n",
      "Episode:  410  , Epsilon:  0.12807462877562611 , Reward 25.34585766879189 , mean_reward:  -14.833631696475267 , time_score:  500 , memory:  119416\n",
      "Episode:  415  , Epsilon:  0.12490462201997637 , Reward -41.62988834391965 , mean_reward:  -15.337868852973493 , time_score:  500 , memory:  121916\n",
      "Episode:  420  , Epsilon:  0.12181307688414106 , Reward -28.537231191638927 , mean_reward:  -13.02117394245889 , time_score:  500 , memory:  124416\n",
      "Episode:  425  , Epsilon:  0.11879805134519765 , Reward 40.71342394356763 , mean_reward:  -11.73449739724449 , time_score:  500 , memory:  126916\n",
      "Episode:  430  , Epsilon:  0.11585765144771248 , Reward -29.0220954735731 , mean_reward:  -11.061103196377848 , time_score:  500 , memory:  129416\n",
      "Episode:  435  , Epsilon:  0.11299003011401039 , Reward 13.733292234281743 , mean_reward:  -9.550480388569152 , time_score:  500 , memory:  131916\n",
      "Episode:  440  , Epsilon:  0.11019338598389174 , Reward 16.738204306016502 , mean_reward:  -10.107592244387662 , time_score:  500 , memory:  134416\n",
      "Episode:  445  , Epsilon:  0.10746596228306791 , Reward -37.54959259566175 , mean_reward:  -11.753463617949643 , time_score:  500 , memory:  136916\n",
      "Episode:  450  , Epsilon:  0.10480604571960442 , Reward 13.383909291987347 , mean_reward:  -11.797765579548571 , time_score:  500 , memory:  139416\n",
      "Episode:  455  , Epsilon:  0.10221196540767843 , Reward 47.93878077174314 , mean_reward:  -11.913125615916208 , time_score:  500 , memory:  141916\n",
      "Episode:  460  , Epsilon:  0.0996820918179746 , Reward 44.39931541798619 , mean_reward:  -11.057149127979027 , time_score:  500 , memory:  144416\n",
      "Episode:  465  , Epsilon:  0.09721483575406 , Reward -7.8485679125726024 , mean_reward:  -12.703156135094364 , time_score:  500 , memory:  146916\n",
      "Episode:  470  , Epsilon:  0.09480864735409487 , Reward -3.2510309105982325 , mean_reward:  -12.664727177512583 , time_score:  500 , memory:  149416\n",
      "Episode:  475  , Epsilon:  0.09246201511725258 , Reward 44.141216934499425 , mean_reward:  -11.261492825283769 , time_score:  500 , memory:  151916\n",
      "Episode:  480  , Epsilon:  0.09017346495423652 , Reward -73.23109175458511 , mean_reward:  -10.371556137339724 , time_score:  500 , memory:  154416\n",
      "Episode:  485  , Epsilon:  0.08794155926129824 , Reward 52.554408815205655 , mean_reward:  -7.641702429527518 , time_score:  500 , memory:  156916\n",
      "Episode:  490  , Epsilon:  0.08576489601717459 , Reward 48.21548254281386 , mean_reward:  -5.83683532500263 , time_score:  500 , memory:  159416\n",
      "Episode:  495  , Epsilon:  0.08364210790237678 , Reward -35.24290997269472 , mean_reward:  -4.299272696199399 , time_score:  500 , memory:  161916\n",
      "Episode:  500  , Epsilon:  0.08157186144027828 , Reward -21.890716800349526 , mean_reward:  -2.186054306693699 , time_score:  500 , memory:  164416\n",
      "Episode:  505  , Epsilon:  0.07955285615946175 , Reward -7.84068394912535 , mean_reward:  -0.0025195447142557104 , time_score:  500 , memory:  166916\n",
      "Episode:  510  , Epsilon:  0.07758382377679894 , Reward 49.71176687941967 , mean_reward:  1.3286041694285626 , time_score:  500 , memory:  169416\n",
      "Episode:  515  , Epsilon:  0.07566352740075044 , Reward 113.69767449270488 , mean_reward:  1.6718082779835595 , time_score:  500 , memory:  171916\n",
      "Episode:  520  , Epsilon:  0.07379076075438468 , Reward -25.9393753903614 , mean_reward:  0.03547439747675305 , time_score:  500 , memory:  174416\n",
      "Episode:  525  , Epsilon:  0.07196434741762824 , Reward 38.98832030831059 , mean_reward:  1.3762134888416897 , time_score:  500 , memory:  176916\n",
      "Episode:  530  , Epsilon:  0.07018314008827135 , Reward -26.840152025084443 , mean_reward:  3.884397189938932 , time_score:  500 , memory:  179416\n",
      "Episode:  535  , Epsilon:  0.06844601986126451 , Reward 32.66007409604607 , mean_reward:  5.242626308755474 , time_score:  500 , memory:  181916\n",
      "Episode:  540  , Epsilon:  0.0667518955258533 , Reward -8.154499771986726 , mean_reward:  7.821700129612119 , time_score:  500 , memory:  184416\n",
      "Episode:  545  , Epsilon:  0.06509970288011008 , Reward 7.025134774990697 , mean_reward:  10.933661860700537 , time_score:  500 , memory:  186916\n",
      "Episode:  550  , Epsilon:  0.06348840406243188 , Reward -12.450963375893839 , mean_reward:  11.44628963889581 , time_score:  500 , memory:  189416\n",
      "Episode:  555  , Epsilon:  0.06191698689958447 , Reward 83.18896981134782 , mean_reward:  14.534747353173024 , time_score:  500 , memory:  191916\n",
      "Episode:  560  , Epsilon:  0.06038446427088321 , Reward 68.81337277711071 , mean_reward:  15.29154358287766 , time_score:  500 , memory:  194416\n",
      "Episode:  565  , Epsilon:  0.058889873488111255 , Reward -44.37515820619361 , mean_reward:  16.205971947759966 , time_score:  500 , memory:  196916\n",
      "Episode:  570  , Epsilon:  0.05743227569078546 , Reward 8.08796007625946 , mean_reward:  16.761527878415013 , time_score:  500 , memory:  199416\n",
      "Episode:  575  , Epsilon:  0.05601075525639029 , Reward 86.93412516156398 , mean_reward:  16.00009869561598 , time_score:  500 , memory:  201916\n",
      "Episode:  580  , Epsilon:  0.05462441922520914 , Reward -52.60320388688023 , mean_reward:  15.29857314776282 , time_score:  500 , memory:  204416\n",
      "Episode:  585  , Epsilon:  0.05327239673939179 , Reward -28.476972598552567 , mean_reward:  14.07218501436179 , time_score:  500 , memory:  206916\n",
      "Episode:  590  , Epsilon:  0.05195383849590569 , Reward 43.68679660502718 , mean_reward:  13.26314398050691 , time_score:  500 , memory:  209416\n",
      "Episode:  595  , Epsilon:  0.05066791621302729 , Reward -16.05137508268365 , mean_reward:  11.958147587262483 , time_score:  500 , memory:  211916\n",
      "Episode:  600  , Epsilon:  0.0494138221100385 , Reward 47.850534473063156 , mean_reward:  13.315452377539286 , time_score:  500 , memory:  214416\n",
      "Episode:  605  , Epsilon:  0.048190768399801194 , Reward 34.3190088278201 , mean_reward:  13.692356614350679 , time_score:  500 , memory:  216916\n",
      "Episode:  610  , Epsilon:  0.046997986793891174 , Reward 67.31319474955548 , mean_reward:  11.859188726320312 , time_score:  500 , memory:  219416\n",
      "Episode:  615  , Epsilon:  0.04583472801998072 , Reward -39.616085126293 , mean_reward:  12.73915121605222 , time_score:  500 , memory:  221916\n",
      "Episode:  620  , Epsilon:  0.04470026135116646 , Reward 68.56083421779455 , mean_reward:  14.769744225178144 , time_score:  500 , memory:  224416\n",
      "Episode:  625  , Epsilon:  0.04359387414694703 , Reward 74.66434015262851 , mean_reward:  14.341258457221915 , time_score:  500 , memory:  226916\n",
      "Episode:  630  , Epsilon:  0.04251487140556204 , Reward 48.17449531295631 , mean_reward:  14.405350512809155 , time_score:  500 , memory:  229416\n",
      "Episode:  635  , Epsilon:  0.04146257532741124 , Reward -21.85195841077937 , mean_reward:  16.33278682122424 , time_score:  500 , memory:  231916\n",
      "Episode:  640  , Epsilon:  0.04043632488927963 , Reward -13.75183609764444 , mean_reward:  14.804552090823988 , time_score:  500 , memory:  234416\n",
      "Episode:  645  , Epsilon:  0.039435475429100995 , Reward 21.802101258220215 , mean_reward:  12.432103190237084 , time_score:  500 , memory:  236916\n",
      "Episode:  650  , Epsilon:  0.03845939824099909 , Reward -5.090453813164537 , mean_reward:  12.178038637131378 , time_score:  500 , memory:  239416\n",
      "Episode:  655  , Epsilon:  0.03750748018035199 , Reward 27.86973069419324 , mean_reward:  9.935331987852804 , time_score:  500 , memory:  241916\n",
      "Episode:  660  , Epsilon:  0.03657912327863173 , Reward 60.238551082287 , mean_reward:  10.434943060825601 , time_score:  500 , memory:  244416\n",
      "Episode:  665  , Epsilon:  0.035673744367776934 , Reward 25.701464498690907 , mean_reward:  11.181324217203144 , time_score:  500 , memory:  246916\n",
      "Episode:  670  , Epsilon:  0.03479077471386296 , Reward -61.19536812244914 , mean_reward:  9.643029575756815 , time_score:  500 , memory:  249416\n",
      "Episode:  675  , Epsilon:  0.03392965965983891 , Reward -46.759624095953775 , mean_reward:  8.256147658730395 , time_score:  500 , memory:  251916\n",
      "Episode:  680  , Epsilon:  0.03308985827710748 , Reward 13.406393160343207 , mean_reward:  9.249355787822315 , time_score:  500 , memory:  254416\n",
      "Episode:  685  , Epsilon:  0.03227084302572862 , Reward -35.05308564125811 , mean_reward:  9.588634754030236 , time_score:  500 , memory:  256916\n",
      "Episode:  690  , Epsilon:  0.03147209942303359 , Reward -51.95282122833585 , mean_reward:  9.427720197476146 , time_score:  500 , memory:  259416\n",
      "Episode:  695  , Epsilon:  0.030693125720441184 , Reward 3.96502208120676 , mean_reward:  11.852709074798742 , time_score:  500 , memory:  261916\n",
      "Episode:  700  , Epsilon:  0.029933432588273214 , Reward -59.16100732456103 , mean_reward:  9.980350133647198 , time_score:  500 , memory:  264416\n",
      "Episode:  705  , Epsilon:  0.029192542808371146 , Reward 13.23644231548785 , mean_reward:  7.625621507782735 , time_score:  500 , memory:  266916\n",
      "Episode:  710  , Epsilon:  0.028469990974320916 , Reward 37.68858717144684 , mean_reward:  10.289965808880789 , time_score:  500 , memory:  269416\n",
      "Episode:  715  , Epsilon:  0.027765323199097504 , Reward 62.620452069011165 , mean_reward:  10.112938264562562 , time_score:  500 , memory:  271916\n",
      "Episode:  720  , Epsilon:  0.02707809682994571 , Reward 45.214941085762206 , mean_reward:  10.325942649431932 , time_score:  500 , memory:  274416\n",
      "Episode:  725  , Epsilon:  0.026407880170317945 , Reward 3.3183613864181285 , mean_reward:  10.059305739630494 , time_score:  500 , memory:  276916\n",
      "Episode:  730  , Epsilon:  0.025754252208694463 , Reward -10.026464248546526 , mean_reward:  9.079585180256915 , time_score:  500 , memory:  279416\n",
      "Episode:  735  , Epsilon:  0.025116802354115567 , Reward 24.94967634651568 , mean_reward:  6.646154694868178 , time_score:  500 , memory:  281916\n",
      "Episode:  740  , Epsilon:  0.02449513017825978 , Reward 62.41259306213866 , mean_reward:  7.785454177590494 , time_score:  500 , memory:  284416\n",
      "Episode:  745  , Epsilon:  0.023888845163905856 , Reward -67.57729221108934 , mean_reward:  7.5689468177203 , time_score:  500 , memory:  286916\n",
      "Episode:  750  , Epsilon:  0.023297566459620722 , Reward -13.38795720642517 , mean_reward:  9.10340373467431 , time_score:  500 , memory:  289416\n",
      "Episode:  755  , Epsilon:  0.022720922640519125 , Reward 3.14253227159168 , mean_reward:  10.013714579285176 , time_score:  500 , memory:  291916\n",
      "Episode:  760  , Epsilon:  0.022158551474944856 , Reward -46.09300218795042 , mean_reward:  9.056733958096457 , time_score:  500 , memory:  294416\n",
      "Episode:  765  , Epsilon:  0.021610099696926857 , Reward -4.155945520281721 , mean_reward:  8.748379891009757 , time_score:  500 , memory:  296916\n",
      "Episode:  770  , Epsilon:  0.021075222784267326 , Reward 48.08050451241846 , mean_reward:  11.584509818378503 , time_score:  500 , memory:  299416\n",
      "Episode:  775  , Epsilon:  0.020553584742122436 , Reward -14.857017883875152 , mean_reward:  12.134635479156788 , time_score:  500 , memory:  301916\n",
      "Episode:  780  , Epsilon:  0.020044857891939702 , Reward -33.040048834379164 , mean_reward:  12.823882118820732 , time_score:  500 , memory:  304416\n",
      "Episode:  785  , Epsilon:  0.01954872266561937 , Reward 32.04374245381751 , mean_reward:  13.305297931094696 , time_score:  500 , memory:  306916\n",
      "Episode:  790  , Epsilon:  0.019064867404770626 , Reward 17.225550313228023 , mean_reward:  12.777654337209437 , time_score:  500 , memory:  309416\n",
      "Episode:  795  , Epsilon:  0.018592988164936427 , Reward 30.22972318026104 , mean_reward:  11.36638210058928 , time_score:  500 , memory:  311916\n",
      "Episode:  800  , Epsilon:  0.018132788524664028 , Reward 5.112684393400407 , mean_reward:  12.413313971917894 , time_score:  500 , memory:  314416\n",
      "Episode:  805  , Epsilon:  0.017683979399301233 , Reward 15.57384106440476 , mean_reward:  13.217116979423531 , time_score:  500 , memory:  316916\n",
      "Episode:  810  , Epsilon:  0.01724627885940145 , Reward -40.23510519609349 , mean_reward:  11.433122897195657 , time_score:  500 , memory:  319416\n",
      "Episode:  815  , Epsilon:  0.01681941195362342 , Reward -1.2336751449852128 , mean_reward:  10.961188998759821 , time_score:  500 , memory:  321916\n",
      "Episode:  820  , Epsilon:  0.0164031105360144 , Reward -17.735237572451215 , mean_reward:  9.421635517929598 , time_score:  500 , memory:  324416\n",
      "Episode:  825  , Epsilon:  0.015997113097568336 , Reward 23.206144898264714 , mean_reward:  9.027862602202113 , time_score:  500 , memory:  326916\n",
      "Episode:  830  , Epsilon:  0.015601164601953134 , Reward 12.24721358620017 , mean_reward:  8.735649939007494 , time_score:  500 , memory:  329416\n",
      "Episode:  835  , Epsilon:  0.015215016325303928 , Reward -63.60326430928182 , mean_reward:  8.801775802845723 , time_score:  500 , memory:  331916\n",
      "Episode:  840  , Epsilon:  0.014838425699981627 , Reward -32.0908671526211 , mean_reward:  6.297418502226807 , time_score:  500 , memory:  334416\n",
      "Episode:  845  , Epsilon:  0.014471156162198668 , Reward -37.45793040188999 , mean_reward:  7.451182759334938 , time_score:  500 , memory:  336916\n",
      "Episode:  850  , Epsilon:  0.014112977003416188 , Reward -1.3422714245918779 , mean_reward:  5.757199565739192 , time_score:  500 , memory:  339416\n",
      "Episode:  855  , Epsilon:  0.013763663225419333 , Reward 64.99019050086702 , mean_reward:  6.04061224615865 , time_score:  500 , memory:  341916\n",
      "Episode:  860  , Epsilon:  0.013422995398979608 , Reward 69.33954323782874 , mean_reward:  5.462927057727055 , time_score:  500 , memory:  344416\n",
      "Episode:  865  , Epsilon:  0.013090759526015528 , Reward 52.713293105933644 , mean_reward:  5.515317196446497 , time_score:  500 , memory:  346916\n",
      "Episode:  870  , Epsilon:  0.012766746905164949 , Reward -9.909918801425555 , mean_reward:  4.8719383640531495 , time_score:  500 , memory:  349416\n",
      "Episode:  875  , Epsilon:  0.012450754000684672 , Reward -42.862753879280454 , mean_reward:  3.9984380243603868 , time_score:  500 , memory:  351916\n",
      "Episode:  880  , Epsilon:  0.012142582314594924 , Reward 42.774651306214736 , mean_reward:  3.897558078991724 , time_score:  500 , memory:  354416\n",
      "Episode:  885  , Epsilon:  0.01184203826198843 , Reward 7.684860563649861 , mean_reward:  3.011379841660944 , time_score:  500 , memory:  356916\n",
      "Episode:  890  , Epsilon:  0.01154893304942575 , Reward -69.19495461066991 , mean_reward:  2.55512507524942 , time_score:  500 , memory:  359416\n",
      "Episode:  895  , Epsilon:  0.011263082556340478 , Reward -42.471884968672995 , mean_reward:  1.1737745840603977 , time_score:  500 , memory:  361916\n",
      "Episode:  900  , Epsilon:  0.01098430721937979 , Reward -37.97475376575733 , mean_reward:  -0.03686813760688139 , time_score:  500 , memory:  364416\n",
      "Episode:  905  , Epsilon:  0.01071243191960775 , Reward -22.55624834776874 , mean_reward:  -0.9488851546292865 , time_score:  500 , memory:  366916\n",
      "Episode:  910  , Epsilon:  0.010447285872500434 , Reward 15.581624444412245 , mean_reward:  -1.2456088773815217 , time_score:  500 , memory:  369416\n",
      "Episode:  915  , Epsilon:  0.010188702520663827 , Reward -14.030721357526025 , mean_reward:  -1.6445051838599605 , time_score:  500 , memory:  371916\n",
      "Episode:  920  , Epsilon:  0.01 , Reward 1.3298722893484152 , mean_reward:  -1.4373832347933853 , time_score:  500 , memory:  374416\n",
      "Episode:  925  , Epsilon:  0.01 , Reward 13.475903416999822 , mean_reward:  -1.9335714804652722 , time_score:  500 , memory:  376916\n",
      "Episode:  930  , Epsilon:  0.01 , Reward 36.94866873398456 , mean_reward:  -1.8160359719621442 , time_score:  500 , memory:  379416\n",
      "Episode:  935  , Epsilon:  0.01 , Reward 47.9448782603166 , mean_reward:  -0.8814728939412315 , time_score:  500 , memory:  381916\n",
      "Episode:  940  , Epsilon:  0.01 , Reward 17.851285480320268 , mean_reward:  0.12547060165920856 , time_score:  500 , memory:  384416\n",
      "Episode:  945  , Epsilon:  0.01 , Reward -70.36138106308243 , mean_reward:  -0.9692412968448637 , time_score:  500 , memory:  386916\n",
      "Episode:  950  , Epsilon:  0.01 , Reward -14.681420043420653 , mean_reward:  -2.3409103773652813 , time_score:  500 , memory:  389416\n",
      "Episode:  955  , Epsilon:  0.01 , Reward 24.4450733965287 , mean_reward:  -3.9256087299017564 , time_score:  500 , memory:  391916\n",
      "Episode:  960  , Epsilon:  0.01 , Reward -19.861258121176423 , mean_reward:  -4.496138989616466 , time_score:  500 , memory:  394416\n",
      "Episode:  965  , Epsilon:  0.01 , Reward 40.65177966009415 , mean_reward:  -4.6912246522462535 , time_score:  500 , memory:  396916\n",
      "Episode:  970  , Epsilon:  0.01 , Reward 61.26314058820294 , mean_reward:  -4.790492796555788 , time_score:  500 , memory:  399416\n",
      "Episode:  975  , Epsilon:  0.01 , Reward 47.22292272590012 , mean_reward:  -3.34585114924011 , time_score:  500 , memory:  401916\n",
      "Episode:  980  , Epsilon:  0.01 , Reward -6.693664129221468 , mean_reward:  -4.071522435351769 , time_score:  500 , memory:  404416\n",
      "Episode:  985  , Epsilon:  0.01 , Reward -12.7914190220121 , mean_reward:  -3.856941476960485 , time_score:  500 , memory:  406916\n",
      "Episode:  990  , Epsilon:  0.01 , Reward -23.786284336102803 , mean_reward:  -1.826771439522293 , time_score:  500 , memory:  409416\n",
      "Episode:  995  , Epsilon:  0.01 , Reward 2.7168948540425104 , mean_reward:  -1.4497186928432362 , time_score:  500 , memory:  411916\n",
      "Episode:  1000  , Epsilon:  0.01 , Reward -6.956037327293608 , mean_reward:  -1.7310142992811257 , time_score:  500 , memory:  414416\n",
      "Episode:  1005  , Epsilon:  0.01 , Reward -2.6166448153456097 , mean_reward:  -2.2642702738964964 , time_score:  500 , memory:  416916\n",
      "Episode:  1010  , Epsilon:  0.01 , Reward 46.906522763599256 , mean_reward:  -1.6467845762754245 , time_score:  500 , memory:  419416\n",
      "Episode:  1015  , Epsilon:  0.01 , Reward 64.45128733455503 , mean_reward:  -1.6918277170325375 , time_score:  500 , memory:  421916\n",
      "Episode:  1020  , Epsilon:  0.01 , Reward -22.47178253071001 , mean_reward:  -2.2653570362285222 , time_score:  500 , memory:  424416\n",
      "Episode:  1025  , Epsilon:  0.01 , Reward -26.540398166824097 , mean_reward:  -2.0817931947742587 , time_score:  500 , memory:  426916\n",
      "Episode:  1030  , Epsilon:  0.01 , Reward 6.506574675696756 , mean_reward:  -3.540071883987132 , time_score:  500 , memory:  429416\n",
      "Episode:  1035  , Epsilon:  0.01 , Reward -2.50258607990565 , mean_reward:  -5.137302599523962 , time_score:  500 , memory:  431916\n",
      "Episode:  1040  , Epsilon:  0.01 , Reward -46.03503426853725 , mean_reward:  -5.750129618451329 , time_score:  500 , memory:  434416\n",
      "Episode:  1045  , Epsilon:  0.01 , Reward -40.543362969248356 , mean_reward:  -5.412627008883893 , time_score:  500 , memory:  436916\n",
      "Episode:  1050  , Epsilon:  0.01 , Reward 12.705103323097877 , mean_reward:  -3.313361556480162 , time_score:  500 , memory:  439416\n",
      "Episode:  1055  , Epsilon:  0.01 , Reward 30.325647809203097 , mean_reward:  -2.241839978716132 , time_score:  500 , memory:  441916\n",
      "Episode:  1060  , Epsilon:  0.01 , Reward -22.873658440877428 , mean_reward:  -2.368271203427036 , time_score:  500 , memory:  444416\n",
      "Episode:  1065  , Epsilon:  0.01 , Reward -31.363341390979365 , mean_reward:  -2.794129535032442 , time_score:  500 , memory:  446916\n",
      "Episode:  1070  , Epsilon:  0.01 , Reward -71.11779655966612 , mean_reward:  -4.276441596752131 , time_score:  500 , memory:  449416\n",
      "Episode:  1075  , Epsilon:  0.01 , Reward -46.53671743442649 , mean_reward:  -5.72211721445677 , time_score:  500 , memory:  451916\n",
      "Episode:  1080  , Epsilon:  0.01 , Reward -37.38071642543383 , mean_reward:  -5.9048191563822385 , time_score:  500 , memory:  454416\n",
      "Episode:  1085  , Epsilon:  0.01 , Reward -45.92392633067756 , mean_reward:  -6.5711633766436375 , time_score:  500 , memory:  456916\n",
      "Episode:  1090  , Epsilon:  0.01 , Reward -37.531420239681495 , mean_reward:  -7.935832832636604 , time_score:  500 , memory:  459416\n",
      "Episode:  1095  , Epsilon:  0.01 , Reward -29.383070891443236 , mean_reward:  -8.201865865495028 , time_score:  500 , memory:  461916\n",
      "Episode:  1100  , Epsilon:  0.01 , Reward -60.09105254864345 , mean_reward:  -8.348570226283577 , time_score:  500 , memory:  464416\n",
      "Episode:  1105  , Epsilon:  0.01 , Reward -7.064596126130276 , mean_reward:  -7.865372707770041 , time_score:  500 , memory:  466916\n",
      "Episode:  1110  , Epsilon:  0.01 , Reward -72.38282824153957 , mean_reward:  -9.91943350938716 , time_score:  500 , memory:  469416\n",
      "Episode:  1115  , Epsilon:  0.01 , Reward -36.55371677762392 , mean_reward:  -10.5538301677287 , time_score:  500 , memory:  471916\n",
      "Episode:  1120  , Epsilon:  0.01 , Reward -4.880445042306241 , mean_reward:  -8.062619945840328 , time_score:  500 , memory:  474416\n",
      "Episode:  1125  , Epsilon:  0.01 , Reward -12.43642523037293 , mean_reward:  -8.465372926318977 , time_score:  500 , memory:  476916\n",
      "Episode:  1130  , Epsilon:  0.01 , Reward -21.242508825453964 , mean_reward:  -8.971824006871115 , time_score:  500 , memory:  479416\n",
      "Episode:  1135  , Epsilon:  0.01 , Reward 15.79858806402672 , mean_reward:  -9.128500217840323 , time_score:  500 , memory:  481916\n",
      "Episode:  1140  , Epsilon:  0.01 , Reward -42.3932347192589 , mean_reward:  -8.628125834073433 , time_score:  500 , memory:  484416\n",
      "Episode:  1145  , Epsilon:  0.01 , Reward -20.742754690989788 , mean_reward:  -7.7424552284186925 , time_score:  500 , memory:  486916\n",
      "Episode:  1150  , Epsilon:  0.01 , Reward -24.368943070779135 , mean_reward:  -9.691712837024482 , time_score:  500 , memory:  489416\n",
      "Episode:  1155  , Epsilon:  0.01 , Reward -40.043054269522116 , mean_reward:  -11.72865968433141 , time_score:  500 , memory:  491916\n",
      "Episode:  1160  , Epsilon:  0.01 , Reward 10.902598566902995 , mean_reward:  -11.76165653512024 , time_score:  500 , memory:  494416\n",
      "Episode:  1165  , Epsilon:  0.01 , Reward 20.2042891661817 , mean_reward:  -11.329193400456068 , time_score:  500 , memory:  496916\n",
      "Episode:  1170  , Epsilon:  0.01 , Reward 22.789473727377057 , mean_reward:  -10.8983713391342 , time_score:  500 , memory:  499416\n",
      "Episode:  1175  , Epsilon:  0.01 , Reward 25.032180122768576 , mean_reward:  -9.955946073326778 , time_score:  500 , memory:  501916\n",
      "Episode:  1180  , Epsilon:  0.01 , Reward -19.417927309995566 , mean_reward:  -10.014639005688753 , time_score:  500 , memory:  504416\n",
      "Episode:  1185  , Epsilon:  0.01 , Reward 1.8018402023015738 , mean_reward:  -8.569083828484674 , time_score:  500 , memory:  506916\n",
      "Episode:  1190  , Epsilon:  0.01 , Reward -30.728262618073668 , mean_reward:  -8.564839266009178 , time_score:  500 , memory:  509416\n",
      "Episode:  1195  , Epsilon:  0.01 , Reward 14.522532423048931 , mean_reward:  -9.482702699257544 , time_score:  500 , memory:  511916\n",
      "Episode:  1200  , Epsilon:  0.01 , Reward 10.337243292063725 , mean_reward:  -9.42401094184077 , time_score:  500 , memory:  514416\n",
      "Episode:  1205  , Epsilon:  0.01 , Reward -27.888516636487463 , mean_reward:  -10.606979038586683 , time_score:  500 , memory:  516916\n",
      "Episode:  1210  , Epsilon:  0.01 , Reward 14.846656753934521 , mean_reward:  -9.045090781721974 , time_score:  500 , memory:  519416\n",
      "Episode:  1215  , Epsilon:  0.01 , Reward -40.433860901334754 , mean_reward:  -9.88754541859891 , time_score:  500 , memory:  521916\n",
      "Episode:  1220  , Epsilon:  0.01 , Reward -33.382706045957825 , mean_reward:  -12.663211744058366 , time_score:  500 , memory:  524416\n",
      "Episode:  1225  , Epsilon:  0.01 , Reward -17.86327685037212 , mean_reward:  -11.813479184876535 , time_score:  500 , memory:  526916\n",
      "Episode:  1230  , Epsilon:  0.01 , Reward -36.380352521395785 , mean_reward:  -12.005591996655735 , time_score:  500 , memory:  529416\n",
      "Episode:  1235  , Epsilon:  0.01 , Reward -53.055284113391934 , mean_reward:  -12.982124147074947 , time_score:  231 , memory:  531647\n",
      "Episode:  1240  , Epsilon:  0.01 , Reward 37.55711992231685 , mean_reward:  -13.52056133207564 , time_score:  500 , memory:  534147\n",
      "Episode:  1245  , Epsilon:  0.01 , Reward -3.0382369167988807 , mean_reward:  -14.840960344794341 , time_score:  500 , memory:  536647\n",
      "Episode:  1250  , Epsilon:  0.01 , Reward 41.112940408088775 , mean_reward:  -13.574706261252937 , time_score:  500 , memory:  539147\n",
      "Episode:  1255  , Epsilon:  0.01 , Reward -65.36274413873893 , mean_reward:  -12.982895900605836 , time_score:  500 , memory:  541647\n",
      "Episode:  1260  , Epsilon:  0.01 , Reward -58.5646178724357 , mean_reward:  -12.960478416246756 , time_score:  500 , memory:  544147\n",
      "Episode:  1265  , Epsilon:  0.01 , Reward 18.70846184103469 , mean_reward:  -13.724926563983015 , time_score:  500 , memory:  546647\n",
      "Episode:  1270  , Epsilon:  0.01 , Reward 0.740852964375867 , mean_reward:  -13.123224102395746 , time_score:  500 , memory:  549147\n",
      "Episode:  1275  , Epsilon:  0.01 , Reward -34.85156755675189 , mean_reward:  -13.367196927210234 , time_score:  500 , memory:  551647\n",
      "Episode:  1280  , Epsilon:  0.01 , Reward -26.01950812838276 , mean_reward:  -14.830058712037488 , time_score:  500 , memory:  554147\n",
      "Episode:  1285  , Epsilon:  0.01 , Reward -30.788561841300094 , mean_reward:  -16.66984603439977 , time_score:  500 , memory:  556647\n",
      "Episode:  1290  , Epsilon:  0.01 , Reward -57.345487884048666 , mean_reward:  -16.808182267972636 , time_score:  500 , memory:  559147\n",
      "Episode:  1295  , Epsilon:  0.01 , Reward -5.478821774549639 , mean_reward:  -15.850103005062236 , time_score:  500 , memory:  561647\n",
      "Episode:  1300  , Epsilon:  0.01 , Reward 3.5991384623987273 , mean_reward:  -15.15209167326078 , time_score:  500 , memory:  564147\n",
      "Episode:  1305  , Epsilon:  0.01 , Reward -35.283174030173804 , mean_reward:  -14.597741073565725 , time_score:  500 , memory:  566647\n",
      "Episode:  1310  , Epsilon:  0.01 , Reward -66.61486457097294 , mean_reward:  -15.973287504159792 , time_score:  500 , memory:  569147\n",
      "Episode:  1315  , Epsilon:  0.01 , Reward -28.243898227235665 , mean_reward:  -14.801559350929626 , time_score:  500 , memory:  571647\n",
      "Episode:  1320  , Epsilon:  0.01 , Reward -25.734118433033476 , mean_reward:  -14.7407713951447 , time_score:  500 , memory:  574147\n",
      "Episode:  1325  , Epsilon:  0.01 , Reward -10.153724691098468 , mean_reward:  -14.860000831281573 , time_score:  500 , memory:  576647\n",
      "Episode:  1330  , Epsilon:  0.01 , Reward -7.983443749699851 , mean_reward:  -13.327871077169096 , time_score:  500 , memory:  579147\n",
      "Episode:  1335  , Epsilon:  0.01 , Reward -36.30826809361424 , mean_reward:  -12.370335482102687 , time_score:  500 , memory:  581647\n",
      "Episode:  1340  , Epsilon:  0.01 , Reward -46.11769346082968 , mean_reward:  -12.073104052116745 , time_score:  500 , memory:  584147\n",
      "Episode:  1345  , Epsilon:  0.01 , Reward 1.2481687736216875 , mean_reward:  -13.27316584751626 , time_score:  500 , memory:  586647\n",
      "Episode:  1350  , Epsilon:  0.01 , Reward 7.4680276849532286 , mean_reward:  -13.311379497557073 , time_score:  500 , memory:  589147\n",
      "Episode:  1355  , Epsilon:  0.01 , Reward -47.744696016252384 , mean_reward:  -13.03047118124006 , time_score:  500 , memory:  591647\n",
      "Episode:  1360  , Epsilon:  0.01 , Reward 30.436939171222498 , mean_reward:  -13.157592146327978 , time_score:  500 , memory:  594147\n",
      "Episode:  1365  , Epsilon:  0.01 , Reward 9.282343483099243 , mean_reward:  -12.919249929562191 , time_score:  500 , memory:  596647\n",
      "Episode:  1370  , Epsilon:  0.01 , Reward 27.547860862280153 , mean_reward:  -14.050128229927413 , time_score:  500 , memory:  599147\n",
      "Episode:  1375  , Epsilon:  0.01 , Reward 46.29990322721973 , mean_reward:  -13.461588507576609 , time_score:  500 , memory:  601647\n",
      "Episode:  1380  , Epsilon:  0.01 , Reward -24.703501893163526 , mean_reward:  -12.722436808145506 , time_score:  500 , memory:  604147\n",
      "Episode:  1385  , Epsilon:  0.01 , Reward -49.62174989898483 , mean_reward:  -13.038459332526436 , time_score:  500 , memory:  606647\n",
      "Episode:  1390  , Epsilon:  0.01 , Reward 10.118587656469407 , mean_reward:  -11.92343591183778 , time_score:  500 , memory:  609147\n",
      "Episode:  1395  , Epsilon:  0.01 , Reward -25.454504910241702 , mean_reward:  -12.286986019657883 , time_score:  500 , memory:  611647\n",
      "Episode:  1400  , Epsilon:  0.01 , Reward -28.27414863747177 , mean_reward:  -11.553171704458618 , time_score:  500 , memory:  614147\n",
      "Episode:  1405  , Epsilon:  0.01 , Reward -16.37567427834774 , mean_reward:  -11.358985229246718 , time_score:  500 , memory:  616647\n",
      "Episode:  1410  , Epsilon:  0.01 , Reward 0.5493303212311753 , mean_reward:  -10.241506350227697 , time_score:  500 , memory:  619147\n",
      "Episode:  1415  , Epsilon:  0.01 , Reward -41.08069777581752 , mean_reward:  -10.798878069088161 , time_score:  500 , memory:  621647\n",
      "Episode:  1420  , Epsilon:  0.01 , Reward -23.58153098468401 , mean_reward:  -9.800522944145687 , time_score:  500 , memory:  624147\n",
      "Episode:  1425  , Epsilon:  0.01 , Reward -25.06679404074471 , mean_reward:  -10.640755839937208 , time_score:  500 , memory:  626647\n",
      "Episode:  1430  , Epsilon:  0.01 , Reward -31.072631214651285 , mean_reward:  -11.560093153800057 , time_score:  500 , memory:  629147\n",
      "Episode:  1435  , Epsilon:  0.01 , Reward 12.544646125341403 , mean_reward:  -10.72783148659003 , time_score:  500 , memory:  631647\n",
      "Episode:  1440  , Epsilon:  0.01 , Reward -21.675809455286892 , mean_reward:  -10.43084982553482 , time_score:  500 , memory:  634147\n",
      "Episode:  1445  , Epsilon:  0.01 , Reward 11.009882474917346 , mean_reward:  -8.859580927867043 , time_score:  500 , memory:  636647\n",
      "Episode:  1450  , Epsilon:  0.01 , Reward 66.44691478175275 , mean_reward:  -8.752123132894488 , time_score:  500 , memory:  639147\n",
      "Episode:  1455  , Epsilon:  0.01 , Reward 25.317593374039383 , mean_reward:  -7.741796790462964 , time_score:  500 , memory:  641647\n",
      "Episode:  1460  , Epsilon:  0.01 , Reward -52.90032648052264 , mean_reward:  -9.015565816839308 , time_score:  500 , memory:  644147\n",
      "Episode:  1465  , Epsilon:  0.01 , Reward -55.26544137654767 , mean_reward:  -9.68864705855178 , time_score:  500 , memory:  646647\n",
      "Episode:  1470  , Epsilon:  0.01 , Reward 10.40654531674592 , mean_reward:  -8.971842562775429 , time_score:  500 , memory:  649147\n",
      "Episode:  1475  , Epsilon:  0.01 , Reward 4.56911637845038 , mean_reward:  -8.5523861253906 , time_score:  500 , memory:  651647\n",
      "Episode:  1480  , Epsilon:  0.01 , Reward -38.59987683491606 , mean_reward:  -9.020627329870505 , time_score:  500 , memory:  654147\n",
      "Episode:  1485  , Epsilon:  0.01 , Reward -22.492420844362698 , mean_reward:  -8.229424586778084 , time_score:  500 , memory:  656647\n",
      "Episode:  1490  , Epsilon:  0.01 , Reward -20.314453507398305 , mean_reward:  -7.753327181370463 , time_score:  500 , memory:  659147\n",
      "Episode:  1495  , Epsilon:  0.01 , Reward -7.99627232184828 , mean_reward:  -6.729371816251749 , time_score:  500 , memory:  661647\n",
      "Episode:  1500  , Epsilon:  0.01 , Reward -11.333082116994344 , mean_reward:  -9.136822575761895 , time_score:  500 , memory:  664147\n",
      "Episode:  1505  , Epsilon:  0.01 , Reward -34.89786464978617 , mean_reward:  -7.550995053721699 , time_score:  500 , memory:  666647\n",
      "Episode:  1510  , Epsilon:  0.01 , Reward -36.89027999459514 , mean_reward:  -7.138673923437298 , time_score:  500 , memory:  669147\n",
      "Episode:  1515  , Epsilon:  0.01 , Reward -8.293738456298733 , mean_reward:  -7.4518858450873555 , time_score:  500 , memory:  671647\n",
      "Episode:  1520  , Epsilon:  0.01 , Reward -57.43593850284356 , mean_reward:  -8.778288971567866 , time_score:  500 , memory:  674147\n",
      "Episode:  1525  , Epsilon:  0.01 , Reward -18.618763059143248 , mean_reward:  -8.471521227306752 , time_score:  500 , memory:  676647\n",
      "Episode:  1530  , Epsilon:  0.01 , Reward -1.1565092402494905 , mean_reward:  -7.572350556014496 , time_score:  500 , memory:  679147\n",
      "Episode:  1535  , Epsilon:  0.01 , Reward 8.54228106314561 , mean_reward:  -8.462849451901388 , time_score:  500 , memory:  681647\n",
      "Episode:  1540  , Epsilon:  0.01 , Reward 9.376308080330348 , mean_reward:  -7.954072885486845 , time_score:  500 , memory:  684147\n",
      "Episode:  1545  , Epsilon:  0.01 , Reward -16.31831960086437 , mean_reward:  -8.541957369858897 , time_score:  500 , memory:  686647\n",
      "Episode:  1550  , Epsilon:  0.01 , Reward -32.43993785500303 , mean_reward:  -9.316305429280922 , time_score:  500 , memory:  689147\n",
      "Episode:  1555  , Epsilon:  0.01 , Reward -19.54542464311833 , mean_reward:  -10.397960135044308 , time_score:  500 , memory:  691647\n",
      "Episode:  1560  , Epsilon:  0.01 , Reward 4.443439023234129 , mean_reward:  -9.826431830991341 , time_score:  500 , memory:  694147\n",
      "Episode:  1565  , Epsilon:  0.01 , Reward -14.906951013257931 , mean_reward:  -8.589383740863022 , time_score:  500 , memory:  696647\n",
      "Episode:  1570  , Epsilon:  0.01 , Reward 10.668728343025421 , mean_reward:  -9.0926410962411 , time_score:  500 , memory:  699147\n",
      "Episode:  1575  , Epsilon:  0.01 , Reward -7.480658688375836 , mean_reward:  -10.618601826325769 , time_score:  500 , memory:  701647\n",
      "Episode:  1580  , Epsilon:  0.01 , Reward 19.504967020779315 , mean_reward:  -9.276564539555865 , time_score:  500 , memory:  704147\n",
      "Episode:  1585  , Epsilon:  0.01 , Reward -58.03846479647761 , mean_reward:  -9.328525696029226 , time_score:  174 , memory:  706321\n",
      "Episode:  1590  , Epsilon:  0.01 , Reward -52.92034819351791 , mean_reward:  -11.026749368699784 , time_score:  500 , memory:  708821\n",
      "Episode:  1595  , Epsilon:  0.01 , Reward -37.72292588110825 , mean_reward:  -11.954023929904015 , time_score:  500 , memory:  711321\n",
      "Episode:  1600  , Epsilon:  0.01 , Reward -40.39223537776789 , mean_reward:  -10.960580230352209 , time_score:  500 , memory:  713821\n",
      "Episode:  1605  , Epsilon:  0.01 , Reward 38.61368041522873 , mean_reward:  -12.198097667955325 , time_score:  500 , memory:  716321\n",
      "Episode:  1610  , Epsilon:  0.01 , Reward -0.13718542044013632 , mean_reward:  -12.844378497390784 , time_score:  500 , memory:  718821\n",
      "Episode:  1615  , Epsilon:  0.01 , Reward -2.049295629888991 , mean_reward:  -12.032265706728694 , time_score:  500 , memory:  721321\n",
      "Episode:  1620  , Epsilon:  0.01 , Reward -1.007964164819205 , mean_reward:  -11.293384798323686 , time_score:  500 , memory:  723821\n",
      "Episode:  1625  , Epsilon:  0.01 , Reward 1.9459199220550227 , mean_reward:  -11.315509151382093 , time_score:  500 , memory:  726321\n",
      "Episode:  1630  , Epsilon:  0.01 , Reward -37.23737342492997 , mean_reward:  -12.071242479160574 , time_score:  500 , memory:  728821\n",
      "Episode:  1635  , Epsilon:  0.01 , Reward 17.004566647619622 , mean_reward:  -12.286365662242538 , time_score:  500 , memory:  731321\n",
      "Episode:  1640  , Epsilon:  0.01 , Reward -134.03166924545548 , mean_reward:  -14.021685219865669 , time_score:  117 , memory:  733438\n",
      "Episode:  1645  , Epsilon:  0.01 , Reward -44.344699021155094 , mean_reward:  -14.046873284349463 , time_score:  500 , memory:  735938\n",
      "Episode:  1650  , Epsilon:  0.01 , Reward 11.010568186098817 , mean_reward:  -14.818173809537184 , time_score:  500 , memory:  738438\n",
      "Episode:  1655  , Epsilon:  0.01 , Reward 27.302564559905836 , mean_reward:  -16.598965760121065 , time_score:  500 , memory:  740938\n",
      "Episode:  1660  , Epsilon:  0.01 , Reward 9.84447562155592 , mean_reward:  -16.648346471372832 , time_score:  500 , memory:  743438\n",
      "Episode:  1665  , Epsilon:  0.01 , Reward -0.6237495342122268 , mean_reward:  -17.647646956584893 , time_score:  500 , memory:  745938\n",
      "Episode:  1670  , Epsilon:  0.01 , Reward -56.98205749063056 , mean_reward:  -17.515761875786964 , time_score:  500 , memory:  748438\n",
      "Episode:  1675  , Epsilon:  0.01 , Reward -16.186242482315667 , mean_reward:  -17.528456226750382 , time_score:  500 , memory:  750938\n",
      "Episode:  1680  , Epsilon:  0.01 , Reward 10.635444510462506 , mean_reward:  -17.438750936034964 , time_score:  500 , memory:  753438\n",
      "Episode:  1685  , Epsilon:  0.01 , Reward 43.6847222676586 , mean_reward:  -17.13588444452216 , time_score:  500 , memory:  755938\n",
      "Episode:  1690  , Epsilon:  0.01 , Reward -41.52920501249754 , mean_reward:  -17.191959121278888 , time_score:  500 , memory:  758438\n",
      "Episode:  1695  , Epsilon:  0.01 , Reward -26.851589661101702 , mean_reward:  -16.855404917651246 , time_score:  500 , memory:  760938\n",
      "Episode:  1700  , Epsilon:  0.01 , Reward 37.3538339834608 , mean_reward:  -16.61848595983545 , time_score:  500 , memory:  763438\n",
      "Episode:  1705  , Epsilon:  0.01 , Reward -61.752082999551725 , mean_reward:  -17.156559871620917 , time_score:  500 , memory:  765938\n",
      "Episode:  1710  , Epsilon:  0.01 , Reward 1.3280340016834742 , mean_reward:  -17.28488320706368 , time_score:  500 , memory:  768438\n",
      "Episode:  1715  , Epsilon:  0.01 , Reward -32.92437714723842 , mean_reward:  -17.23033612150034 , time_score:  500 , memory:  770938\n",
      "Episode:  1720  , Epsilon:  0.01 , Reward -21.05918882705455 , mean_reward:  -17.609225817144694 , time_score:  500 , memory:  773438\n",
      "Episode:  1725  , Epsilon:  0.01 , Reward -15.627620693978672 , mean_reward:  -16.68771535161263 , time_score:  500 , memory:  775938\n",
      "Episode:  1730  , Epsilon:  0.01 , Reward 27.455152238345857 , mean_reward:  -15.742250283151733 , time_score:  500 , memory:  778438\n",
      "Episode:  1735  , Epsilon:  0.01 , Reward 56.26394049844968 , mean_reward:  -15.518577846216047 , time_score:  500 , memory:  780938\n",
      "Episode:  1740  , Epsilon:  0.01 , Reward 28.298542011330298 , mean_reward:  -13.651900949219401 , time_score:  500 , memory:  783438\n",
      "Episode:  1745  , Epsilon:  0.01 , Reward -18.667849932631732 , mean_reward:  -13.685835132830032 , time_score:  500 , memory:  785938\n",
      "Episode:  1750  , Epsilon:  0.01 , Reward -34.31577383504938 , mean_reward:  -13.410219023857557 , time_score:  500 , memory:  788438\n",
      "Episode:  1755  , Epsilon:  0.01 , Reward -49.48642713605463 , mean_reward:  -12.400711091877998 , time_score:  500 , memory:  790938\n",
      "Episode:  1760  , Epsilon:  0.01 , Reward -2.0613681517956657 , mean_reward:  -11.660064724865133 , time_score:  500 , memory:  793438\n",
      "Episode:  1765  , Epsilon:  0.01 , Reward 4.6582482588256156 , mean_reward:  -11.774030162732654 , time_score:  500 , memory:  795938\n",
      "Episode:  1770  , Epsilon:  0.01 , Reward -31.616771065119888 , mean_reward:  -12.44203586792497 , time_score:  500 , memory:  798438\n",
      "Episode:  1775  , Epsilon:  0.01 , Reward 6.421370534991984 , mean_reward:  -12.293692071885493 , time_score:  500 , memory:  800938\n",
      "Episode:  1780  , Epsilon:  0.01 , Reward -14.142310026919496 , mean_reward:  -13.11130615595867 , time_score:  500 , memory:  803438\n",
      "Episode:  1785  , Epsilon:  0.01 , Reward -60.01311558808178 , mean_reward:  -12.092571483284893 , time_score:  500 , memory:  805938\n",
      "Episode:  1790  , Epsilon:  0.01 , Reward -44.280800170504826 , mean_reward:  -11.903721710311695 , time_score:  500 , memory:  808438\n",
      "Episode:  1795  , Epsilon:  0.01 , Reward -9.544010329608223 , mean_reward:  -11.741741690732391 , time_score:  500 , memory:  810938\n",
      "Episode:  1800  , Epsilon:  0.01 , Reward -19.928377772113727 , mean_reward:  -12.29037674758962 , time_score:  500 , memory:  813438\n",
      "Episode:  1805  , Epsilon:  0.01 , Reward 34.60127851202722 , mean_reward:  -11.1377485392345 , time_score:  500 , memory:  815938\n",
      "Episode:  1810  , Epsilon:  0.01 , Reward 14.658754632547499 , mean_reward:  -11.218606449737392 , time_score:  500 , memory:  818438\n",
      "Episode:  1815  , Epsilon:  0.01 , Reward 26.071490982043453 , mean_reward:  -11.061789746676212 , time_score:  500 , memory:  820613\n",
      "Episode:  1820  , Epsilon:  0.01 , Reward -39.58420219542285 , mean_reward:  -10.878623325125453 , time_score:  500 , memory:  823113\n",
      "Episode:  1825  , Epsilon:  0.01 , Reward -48.24464302481013 , mean_reward:  -12.801710065409328 , time_score:  500 , memory:  825273\n",
      "Episode:  1830  , Epsilon:  0.01 , Reward 3.8830555940483524 , mean_reward:  -14.09531074218587 , time_score:  500 , memory:  827773\n",
      "Episode:  1835  , Epsilon:  0.01 , Reward 9.15518586663728 , mean_reward:  -13.614277440121597 , time_score:  500 , memory:  830273\n",
      "Episode:  1840  , Epsilon:  0.01 , Reward -29.694426801061077 , mean_reward:  -14.217315552672758 , time_score:  500 , memory:  832773\n",
      "Episode:  1845  , Epsilon:  0.01 , Reward -4.119679781308222 , mean_reward:  -13.503852094892657 , time_score:  500 , memory:  835273\n",
      "Episode:  1850  , Epsilon:  0.01 , Reward 12.604674277994956 , mean_reward:  -13.315092999290302 , time_score:  500 , memory:  837773\n",
      "Episode:  1855  , Epsilon:  0.01 , Reward -38.43548249735433 , mean_reward:  -13.170313290500737 , time_score:  500 , memory:  840273\n",
      "Episode:  1860  , Epsilon:  0.01 , Reward -7.981568622127087 , mean_reward:  -13.393137552954352 , time_score:  500 , memory:  842773\n",
      "Episode:  1865  , Epsilon:  0.01 , Reward -13.929612815507326 , mean_reward:  -13.409554393189321 , time_score:  500 , memory:  845273\n",
      "Episode:  1870  , Epsilon:  0.01 , Reward -14.506662500588043 , mean_reward:  -13.398651226635227 , time_score:  500 , memory:  847773\n",
      "Episode:  1875  , Epsilon:  0.01 , Reward -9.529702101049732 , mean_reward:  -13.7534613570058 , time_score:  500 , memory:  850273\n",
      "Episode:  1880  , Epsilon:  0.01 , Reward -31.088796346416377 , mean_reward:  -13.772846500380485 , time_score:  500 , memory:  852773\n",
      "Episode:  1885  , Epsilon:  0.01 , Reward -5.789631770883431 , mean_reward:  -16.178355724429565 , time_score:  500 , memory:  855273\n",
      "Episode:  1890  , Epsilon:  0.01 , Reward -16.771621488719017 , mean_reward:  -15.924921423477251 , time_score:  500 , memory:  857773\n",
      "Episode:  1895  , Epsilon:  0.01 , Reward -10.920562822191721 , mean_reward:  -16.21842262504192 , time_score:  500 , memory:  860273\n",
      "Episode:  1900  , Epsilon:  0.01 , Reward -57.3719894501302 , mean_reward:  -16.159370181636334 , time_score:  500 , memory:  862773\n",
      "Episode:  1905  , Epsilon:  0.01 , Reward -12.199376050272384 , mean_reward:  -16.939163880331535 , time_score:  500 , memory:  865273\n",
      "Episode:  1910  , Epsilon:  0.01 , Reward -77.57741979409259 , mean_reward:  -16.863868913261435 , time_score:  500 , memory:  867773\n",
      "Episode:  1915  , Epsilon:  0.01 , Reward -11.314837603515695 , mean_reward:  -16.752484751040562 , time_score:  500 , memory:  870273\n",
      "Episode:  1920  , Epsilon:  0.01 , Reward 0.07383447921715397 , mean_reward:  -17.269682312087404 , time_score:  500 , memory:  872773\n",
      "Episode:  1925  , Epsilon:  0.01 , Reward 38.50975163684549 , mean_reward:  -16.113502130177707 , time_score:  500 , memory:  875273\n",
      "Episode:  1930  , Epsilon:  0.01 , Reward -10.263888367188809 , mean_reward:  -16.314139611353983 , time_score:  500 , memory:  877773\n",
      "Episode:  1935  , Epsilon:  0.01 , Reward -43.39653164391855 , mean_reward:  -16.972858670994764 , time_score:  500 , memory:  880273\n",
      "Episode:  1940  , Epsilon:  0.01 , Reward -17.137577072583948 , mean_reward:  -17.394294540215334 , time_score:  500 , memory:  882773\n",
      "Episode:  1945  , Epsilon:  0.01 , Reward 48.33893238217046 , mean_reward:  -16.15446863264357 , time_score:  500 , memory:  885273\n",
      "Episode:  1950  , Epsilon:  0.01 , Reward -44.04840473039038 , mean_reward:  -17.242810837343495 , time_score:  500 , memory:  887773\n",
      "Episode:  1955  , Epsilon:  0.01 , Reward -16.29145554156197 , mean_reward:  -18.238533715241328 , time_score:  500 , memory:  890273\n",
      "Episode:  1960  , Epsilon:  0.01 , Reward 29.37217067360911 , mean_reward:  -17.262256697136603 , time_score:  500 , memory:  892773\n",
      "Episode:  1965  , Epsilon:  0.01 , Reward -92.37182531130351 , mean_reward:  -17.098903586847257 , time_score:  358 , memory:  895131\n",
      "Episode:  1970  , Epsilon:  0.01 , Reward 17.368473627134776 , mean_reward:  -15.951750235644731 , time_score:  500 , memory:  897631\n",
      "Episode:  1975  , Epsilon:  0.01 , Reward 41.837469021510316 , mean_reward:  -14.36541579398398 , time_score:  500 , memory:  900131\n",
      "Episode:  1980  , Epsilon:  0.01 , Reward -18.45718458275431 , mean_reward:  -14.076757423621236 , time_score:  500 , memory:  902631\n",
      "Episode:  1985  , Epsilon:  0.01 , Reward 12.42914173597895 , mean_reward:  -12.556831568333035 , time_score:  500 , memory:  905131\n",
      "Episode:  1990  , Epsilon:  0.01 , Reward -36.26218992922261 , mean_reward:  -12.683475382874317 , time_score:  500 , memory:  907631\n",
      "Episode:  1995  , Epsilon:  0.01 , Reward -43.95633869137891 , mean_reward:  -13.011016408869184 , time_score:  500 , memory:  910131\n"
     ]
    }
   ],
   "source": [
    "game = \"LunarLander-v2\"\n",
    "dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.95, episodes=2000, alpha = 0.001, lr=0.0005)\n",
    "df = dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zEK6_8NkZvY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzXeEPyZkZx5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmu7jobCkZ0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LctZX16UkZ2z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oUZZ81CkZ5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigtDnbikZ7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pic26PzvkZ-I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM06jVdTkaA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb-td7BDkaDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjInw1qkaF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8MT-kCZkaIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHXj0aMkaLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3-NkHivkaNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOQX7wxBm0680LypbxNFGpd",
   "collapsed_sections": [],
   "mount_file_id": "1muIbyjnAsjYuWdNUWjqeuQv9cv8Grz2U",
   "name": "DQN_0.995_0.005_0.0005.ipynb",
   "provenance": [
    {
     "file_id": "1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp",
     "timestamp": 1624337011710
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
