{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_0.995_0.1_0.0001.ipynb","provenance":[{"file_id":"1lqAUGuT7CsohqVFy4vwS8YzO3YFUgWfp","timestamp":1624337011710}],"collapsed_sections":[],"mount_file_id":"1RJ6_vIouDpWC9PZgG79Gj2B5wrg1khjd","authorship_tag":"ABX9TyODmxEXuMzQg6MZtcj4+kI4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvWPBrq87_kP","executionInfo":{"status":"ok","timestamp":1624385880165,"user_tz":360,"elapsed":34110,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"4903c42e-8fec-433d-bb83-13eeba9fcc0a"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWJAoAVDkEZV","executionInfo":{"status":"ok","timestamp":1624737716494,"user_tz":360,"elapsed":7734,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"ee829347-7da7-4a47-ca4f-512ee1d8cb01"},"source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","import numpy as np\n","import gym\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","import random\n","from collections import deque\n","import pandas as pd\n","from tqdm import tqdm\n","import time as time\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting box2d-py\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\r\u001b[K     |▊                               | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 25.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 30.3MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 25.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 17.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 16.0MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wT-R5uU23KYi","executionInfo":{"status":"ok","timestamp":1624737716495,"user_tz":360,"elapsed":3,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFSI-YokZl8","executionInfo":{"status":"ok","timestamp":1624737716834,"user_tz":360,"elapsed":342,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}}},"source":["class DQN():\n","    \n","    def __init__(self, game, retrain = False, epsilon=1, epsilon_decay = 0.995, \n","                 epsilon_min = 0.1, batch_size = 64, discount_factor=0.99, episodes=1000, alpha = 0.01, lr=0.001):\n","        \n","        self.ep = epsilon\n","        self.ep_decay = epsilon_decay\n","        self.ep_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.gamma = discount_factor\n","        self.episodes = episodes\n","        self.game = game\n","        self.alpha = alpha\n","        self.lr = lr\n","        self.retrain = retrain\n","        \n","        self.frames = []\n","        \n","        seed = 983827\n","        mem = 1000000\n","\n","        self.csv_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.1_0.0001/0p1_0p0001.csv\"\n","        self.model_filename = \"/content/drive/MyDrive/Colab Notebooks/DQN-FinalFrontier/0.1_0.0001/0p1_0p0001.h5\"\n","\n","        \n","        self.env = gym.make(game)\n","        self.env.seed(seed)\n","        \n","        keras.backend.clear_session()\n","        \n","        tf.random.set_seed(seed)\n","        np.random.seed(seed)\n","        \n","        self.nS = self.env.observation_space.shape[0]\n","        self.nA = self.env.action_space.n\n","        \n","        print(\"state size is: \",self.nS)\n","        print(\"action size is: \", self.nA)\n","       \n","        \n","        self.memory = deque(maxlen=1000000)  #Creating a container to replay meomory, double linked list.\n","\n","        if self.retrain == False:\n","          self.Q_model = self.setup_dnn()\n","          self.Q_hat_model = self.setup_dnn()\n","          print(\"NEW MODEL CREATED!\")\n","        \n","        else:\n","\n","          self.Q_model = tf.keras.models.load_model(self.model_filename)\n","          self.Q_hat_model = tf.keras.models.load_model(self.model_filename)\n","          print(\"MODEL LOADED!\")\n","          self.Q_model.summary()\n","\n","\n","        self.counter = 0\n","        self.update_freq = 4\n","\n","        \n","        self.df_ddqn = pd.DataFrame(columns = [\"Episode\", \"Epsilon\", \"Reward\", \"Mean_Reward\", \"Time\"])\n","        \n","    def setup_dnn(self):\n","        \n","        input_ = tf.keras.layers.Input(shape = (self.nS))\n","        \n","        hidden1_ = tf.keras.layers.Dense(64, activation = \"relu\")(input_)\n","        hidden2_ = tf.keras.layers.Dense(64, activation = \"relu\")(hidden1_)\n","        output_ = tf.keras.layers.Dense(self.nA)(hidden2_)\n","        \n","        model_ = tf.keras.Model(inputs = [input_], outputs = [output_])\n","        opt_ = tf.keras.optimizers.Adam(self.lr)\n","        model_.compile(optimizer = opt_, loss = \"mse\")\n","        \n","        return model_\n","    \n","    def action(self, state, epsilon):\n","        \n","        if np.random.rand() < epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            Q_values = self.Q_model.predict(state) #Greedy policy w.r.t Q\n","            \n","        return np.argmax(Q_values[0])\n","    \n","    \n","    def store(self, state, action, reward, next_state, done):\n","        \n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    \n","    def weights_update(self):\n","        Q_w = self.Q_model.get_weights()\n","        Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","        for w in range(len(Q_hat_w)):\n","            Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","        self.Q_hat_model.set_weights(Q_hat_weights)\n","        \n","\n","    '''\n","        \n","    def learn(self):\n","        \n","        if self.ep > self.ep_min:\n","            self.ep *= self.ep_decay\n","        \n","        samples = random.choices(self.memory, k = self.batch_size)\n","        \n","        for state, action, reward, next_state, done in samples:\n","            target = reward\n","            \n","            if not done:\n","                target = reward + self.gamma*np.max(self.model.predict(next_state)[0])\n","            \n","            end_target = self.model.predict(state)\n","            end_target[0][action] = target\n","            \n","            self.history = self.model.fit(state, end_target, verbose = 0)\n","    '''\n","    \n","    def learn_batch(self):\n","             \n","        self.counter = (self.counter + 1) % self.update_freq\n","        \n","        if self.counter == 0:\n","            #print(\"Learning...\")\n","            if len(self.memory) < self.batch_size:\n","                return\n","            \n","            states, end_targets = [], []\n","            \n","            samples = random.choices(self.memory, k = self.batch_size)\n","            \n","            for state, action, reward, next_state, done in samples:\n","                target = reward\n","            \n","                if not done:\n","                    target = reward + self.gamma*np.max(self.Q_hat_model.predict(next_state)[0])\n","            \n","                end_target = self.Q_model.predict(state)\n","                end_target[0][action] = target\n","                \n","                states.append(state[0])\n","                end_targets.append(end_target[0])\n","            \n","            self.Q_model.fit(np.array(states), np.array(end_targets), verbose = 0, epochs = 1)\n","            \n","            Q_w = self.Q_model.get_weights()\n","            Q_hat_w = self.Q_hat_model.get_weights()\n","        \n","            for w in range(len(Q_hat_w)):\n","                Q_hat_w[w] = self.alpha * Q_w[w] + (1-self.alpha) * Q_hat_w[w]\n","        \n","            self.Q_hat_model.set_weights(Q_hat_w)\n","    \n","    \n","    def play(self): \n","        \n","        new_row = {}\n","        R = []\n","        R_moving = deque(maxlen=100)\n","        steps = 500\n","        \n","        for e in range(self.episodes):\n","            current_state = self.env.reset()\n","            current_state = np.reshape(current_state, [1,current_state.shape[0]])\n","         \n","            time = 0\n","            r = 0\n","            \n","            for s in range(steps):\n","\n","                action_ = self.action(current_state, self.ep)\n","               \n","                next_state, reward, done, info = self.env.step(action_)\n","                \n","                next_state = np.reshape(next_state, [1, next_state.shape[0]])\n","                \n","                self.store(current_state, action_, reward, next_state, done)\n","                \n","                r = r+reward\n","                \n","                #self.learn()\n","                self.learn_batch()\n","                \n","                current_state = next_state\n","                time = time+1\n","                \n","                if done:\n","                    break\n","            \n","            #self.learn_batch()\n","            R.append(r)\n","            R_moving.append(r)\n","\n","                    \n","            new_row = {'Episode':e, 'Epsilon':self.ep, 'Reward': r, 'Mean_Reward':np.mean(R_moving), 'Time':time}\n","            self.df_ddqn = self.df_ddqn.append(new_row, ignore_index = True)\n","            \n","            \n","            if e % 5 == 0:\n","              print(\"Episode: \", e, \" , Epsilon: \", self.ep, ', Reward', r,\", mean_reward: \",np.mean(R_moving) ,\", time_score: \", time, \", memory: \", len(self.memory))\n","\n","            if e % 100 == 0:\n","\n","              self.Q_model.save(self.model_filename)\n","              \n","\n","            if self.ep > self.ep_min:\n","              self.ep *= self.ep_decay\n","            else:\n","              self.ep = 0.01\n","            \n","            if np.mean(R_moving)>= 200.0:\n","                print(\"BRAVO, GOAL ACHIEVED!!!\")\n","                break\n","\n","        with open(self.csv_filename, 'a') as f:\n","          self.df_ddqn.to_csv(f, header=f.tell()==0, index=False)\n","             \n","            \n","        self.Q_model.save(self.model_filename)\n","        \n","        self.env.close()\n","        \n","        return self.df_ddqn\n","   "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Y5T6-ukZoN","executionInfo":{"status":"ok","timestamp":1624772578224,"user_tz":360,"elapsed":34861392,"user":{"displayName":"Adarsh Gouda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLxey_F4S6zXQQzRixQirKM2ByMsjtktDTWY5hdw=s64","userId":"10706865863009541265"}},"outputId":"b0f37735-58e5-4849-89ea-0006a6f96600"},"source":["game = \"LunarLander-v2\"\n","dqn = DQN(game, retrain = False, epsilon=1 , epsilon_decay = 0.995, epsilon_min = 0.01, batch_size = 64, discount_factor=0.99, episodes=2000, alpha = 0.1, lr=0.0001)\n","df = dqn.play()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["state size is:  8\n","action size is:  4\n","NEW MODEL CREATED!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["Episode:  0  , Epsilon:  1 , Reward -287.22152899428045 , mean_reward:  -287.22152899428045 , time_score:  83 , memory:  83\n","Episode:  5  , Epsilon:  0.9752487531218751 , Reward -85.79289698883059 , mean_reward:  -188.6496518247609 , time_score:  95 , memory:  588\n","Episode:  10  , Epsilon:  0.9511101304657719 , Reward -235.0478482048891 , mean_reward:  -168.1139225692916 , time_score:  99 , memory:  1111\n","Episode:  15  , Epsilon:  0.9275689688183278 , Reward -216.93117546041475 , mean_reward:  -169.39167593036422 , time_score:  100 , memory:  1533\n","Episode:  20  , Epsilon:  0.9046104802746175 , Reward -511.5728433890253 , mean_reward:  -205.03205734593328 , time_score:  165 , memory:  2235\n","Episode:  25  , Epsilon:  0.8822202429488013 , Reward -268.7356623661691 , mean_reward:  -210.9064365592387 , time_score:  110 , memory:  2862\n","Episode:  30  , Epsilon:  0.8603841919146962 , Reward -207.57902765884256 , mean_reward:  -205.9258449465038 , time_score:  124 , memory:  3396\n","Episode:  35  , Epsilon:  0.8390886103705794 , Reward -147.21656665464695 , mean_reward:  -201.53967495236108 , time_score:  63 , memory:  3961\n","Episode:  40  , Epsilon:  0.8183201210226743 , Reward -222.9287741612515 , mean_reward:  -202.57497717876598 , time_score:  142 , memory:  4560\n","Episode:  45  , Epsilon:  0.798065677681905 , Reward -67.03649847361697 , mean_reward:  -208.06935844878043 , time_score:  84 , memory:  5107\n","Episode:  50  , Epsilon:  0.778312557068642 , Reward -54.616705759776636 , mean_reward:  -201.3110444805474 , time_score:  78 , memory:  5701\n","Episode:  55  , Epsilon:  0.7590483508202912 , Reward -225.71394100954632 , mean_reward:  -198.79810127232435 , time_score:  152 , memory:  6274\n","Episode:  60  , Epsilon:  0.7402609576967045 , Reward -306.90383975868116 , mean_reward:  -197.79664644100734 , time_score:  96 , memory:  7199\n","Episode:  65  , Epsilon:  0.7219385759785162 , Reward -318.12168212510846 , mean_reward:  -204.6589242237089 , time_score:  162 , memory:  7988\n","Episode:  70  , Epsilon:  0.7040696960536299 , Reward -224.07742559240745 , mean_reward:  -196.86254420509127 , time_score:  145 , memory:  8705\n","Episode:  75  , Epsilon:  0.6866430931872001 , Reward 37.42518034782523 , mean_reward:  -191.38639974668 , time_score:  98 , memory:  9447\n","Episode:  80  , Epsilon:  0.6696478204705644 , Reward -34.65672168714944 , mean_reward:  -192.3891007253298 , time_score:  84 , memory:  10150\n","Episode:  85  , Epsilon:  0.653073201944699 , Reward -176.52624164773488 , mean_reward:  -194.3445093800562 , time_score:  156 , memory:  10878\n","Episode:  90  , Epsilon:  0.6369088258938781 , Reward -72.78061985491219 , mean_reward:  -191.86500800098153 , time_score:  133 , memory:  11492\n","Episode:  95  , Epsilon:  0.6211445383053219 , Reward -332.807555007466 , mean_reward:  -189.52376177511314 , time_score:  207 , memory:  12275\n","Episode:  100  , Epsilon:  0.6057704364907278 , Reward 9.688544386192474 , mean_reward:  -183.79024457862576 , time_score:  500 , memory:  13413\n","Episode:  105  , Epsilon:  0.5907768628656763 , Reward -71.4855514632784 , mean_reward:  -182.0072039663501 , time_score:  267 , memory:  14324\n","Episode:  110  , Epsilon:  0.5761543988830038 , Reward -23.1637683096082 , mean_reward:  -180.8566666879107 , time_score:  212 , memory:  15249\n","Episode:  115  , Epsilon:  0.5618938591163328 , Reward -80.33900690465245 , mean_reward:  -177.11903444183812 , time_score:  143 , memory:  15946\n","Episode:  120  , Epsilon:  0.547986285490042 , Reward -2.0568632174794317 , mean_reward:  -168.58595705340718 , time_score:  256 , memory:  17162\n","Episode:  125  , Epsilon:  0.5344229416520513 , Reward -160.3665283627907 , mean_reward:  -161.81035064457328 , time_score:  330 , memory:  18273\n","Episode:  130  , Epsilon:  0.5211953074858876 , Reward -90.27192834839917 , mean_reward:  -157.60090028645675 , time_score:  92 , memory:  19640\n","Episode:  135  , Epsilon:  0.5082950737585841 , Reward -160.9528297989897 , mean_reward:  -152.64470687606774 , time_score:  453 , memory:  21210\n","Episode:  140  , Epsilon:  0.49571413690105054 , Reward -83.53014804663874 , mean_reward:  -146.13249372520886 , time_score:  248 , memory:  22706\n","Episode:  145  , Epsilon:  0.483444593917636 , Reward -130.49806598034002 , mean_reward:  -139.44635652395132 , time_score:  237 , memory:  24049\n","Episode:  150  , Epsilon:  0.47147873742168567 , Reward -47.86650167397498 , mean_reward:  -137.40017165312315 , time_score:  124 , memory:  25223\n","Episode:  155  , Epsilon:  0.4598090507939749 , Reward -92.70918545361583 , mean_reward:  -135.22272675720873 , time_score:  150 , memory:  26320\n","Episode:  160  , Epsilon:  0.4484282034609769 , Reward -160.1059504386933 , mean_reward:  -133.80373110444046 , time_score:  147 , memory:  27750\n","Episode:  165  , Epsilon:  0.43732904629000013 , Reward -140.86566182520056 , mean_reward:  -124.76624173407599 , time_score:  500 , memory:  29262\n","Episode:  170  , Epsilon:  0.42650460709830135 , Reward -34.72037979217746 , mean_reward:  -123.52860785580923 , time_score:  500 , memory:  31014\n","Episode:  175  , Epsilon:  0.4159480862733536 , Reward -101.88259996273567 , mean_reward:  -121.5117355774273 , time_score:  150 , memory:  32621\n","Episode:  180  , Epsilon:  0.40565285250151817 , Reward -155.79738757414773 , mean_reward:  -116.51008969530707 , time_score:  338 , memory:  34345\n","Episode:  185  , Epsilon:  0.39561243860243744 , Reward -42.135749690865836 , mean_reward:  -104.88161721726347 , time_score:  277 , memory:  36290\n","Episode:  190  , Epsilon:  0.3858205374665315 , Reward -36.329234398469055 , mean_reward:  -100.00160335529901 , time_score:  307 , memory:  37391\n","Episode:  195  , Epsilon:  0.37627099809304654 , Reward -142.30080683369172 , mean_reward:  -98.97954640693692 , time_score:  123 , memory:  38890\n","Episode:  200  , Epsilon:  0.3669578217261671 , Reward -61.19747824713807 , mean_reward:  -99.17168446574583 , time_score:  283 , memory:  40345\n","Episode:  205  , Epsilon:  0.3578751580867638 , Reward -90.05919265667832 , mean_reward:  -99.69155658531585 , time_score:  162 , memory:  42112\n","Episode:  210  , Epsilon:  0.34901730169741024 , Reward -51.831188794019646 , mean_reward:  -97.12597870625545 , time_score:  500 , memory:  44280\n","Episode:  215  , Epsilon:  0.3403786882983606 , Reward -86.47469987873475 , mean_reward:  -95.04693716780325 , time_score:  266 , memory:  45662\n","Episode:  220  , Epsilon:  0.33195389135223546 , Reward 37.766389089323944 , mean_reward:  -92.49254154496248 , time_score:  500 , memory:  47209\n","Episode:  225  , Epsilon:  0.3237376186352221 , Reward -249.66261233949197 , mean_reward:  -92.90426237162528 , time_score:  103 , memory:  49042\n","Episode:  230  , Epsilon:  0.3157247089126454 , Reward -107.34634654543879 , mean_reward:  -92.06521999212386 , time_score:  500 , memory:  50756\n","Episode:  235  , Epsilon:  0.3079101286968243 , Reward -150.56614592257262 , mean_reward:  -95.12520824443327 , time_score:  500 , memory:  51776\n","Episode:  240  , Epsilon:  0.30028896908517405 , Reward -251.18271691546877 , mean_reward:  -96.27567947591048 , time_score:  201 , memory:  53372\n","Episode:  245  , Epsilon:  0.29285644267656924 , Reward -60.54049976588303 , mean_reward:  -95.46123421839101 , time_score:  500 , memory:  54448\n","Episode:  250  , Epsilon:  0.285607880564032 , Reward -59.143541785945914 , mean_reward:  -93.93792425107603 , time_score:  500 , memory:  56252\n","Episode:  255  , Epsilon:  0.27853872940185365 , Reward -62.22030832457258 , mean_reward:  -90.31119918478947 , time_score:  500 , memory:  58752\n","Episode:  260  , Epsilon:  0.27164454854530906 , Reward -111.68459055082772 , mean_reward:  -86.008073396686 , time_score:  240 , memory:  60727\n","Episode:  265  , Epsilon:  0.2649210072611673 , Reward -427.7823487831772 , mean_reward:  -93.0809884344995 , time_score:  176 , memory:  62069\n","Episode:  270  , Epsilon:  0.2583638820072446 , Reward -110.50569047777145 , mean_reward:  -92.55824161723163 , time_score:  500 , memory:  64227\n","Episode:  275  , Epsilon:  0.2519690537792925 , Reward -79.97281619905065 , mean_reward:  -90.98567868289712 , time_score:  500 , memory:  66288\n","Episode:  280  , Epsilon:  0.2457325055235537 , Reward -89.31497917979249 , mean_reward:  -89.05130982994172 , time_score:  329 , memory:  68356\n","Episode:  285  , Epsilon:  0.23965031961336 , Reward -29.65539363213121 , mean_reward:  -93.02586599527446 , time_score:  500 , memory:  70649\n","Episode:  290  , Epsilon:  0.23371867538818816 , Reward -42.60098557856181 , mean_reward:  -92.81147888564466 , time_score:  500 , memory:  72912\n","Episode:  295  , Epsilon:  0.22793384675362674 , Reward -39.56536548056189 , mean_reward:  -88.87595264633202 , time_score:  500 , memory:  75064\n","Episode:  300  , Epsilon:  0.22229219984074702 , Reward 18.85353946676911 , mean_reward:  -84.83794368747556 , time_score:  500 , memory:  76861\n","Episode:  305  , Epsilon:  0.2167901907234072 , Reward -30.739282411809654 , mean_reward:  -76.18116929477256 , time_score:  227 , memory:  78951\n","Episode:  310  , Epsilon:  0.21142436319205632 , Reward 6.125098266300463 , mean_reward:  -74.06761330123943 , time_score:  500 , memory:  81097\n","Episode:  315  , Epsilon:  0.20619134658263935 , Reward -13.35175179036193 , mean_reward:  -72.43113521443568 , time_score:  299 , memory:  83396\n","Episode:  320  , Epsilon:  0.2010878536592394 , Reward -36.287125169275384 , mean_reward:  -68.94477016647298 , time_score:  500 , memory:  85689\n","Episode:  325  , Epsilon:  0.19611067854912728 , Reward -9.73953289719893 , mean_reward:  -68.24961361363162 , time_score:  500 , memory:  87333\n","Episode:  330  , Epsilon:  0.1912566947289212 , Reward -42.634001680815295 , mean_reward:  -67.11175008763674 , time_score:  500 , memory:  89594\n","Episode:  335  , Epsilon:  0.1865228530605915 , Reward -97.83194939676612 , mean_reward:  -61.673650838224496 , time_score:  254 , memory:  91581\n","Episode:  340  , Epsilon:  0.18190617987607657 , Reward -86.24516541811496 , mean_reward:  -58.56133996766653 , time_score:  500 , memory:  93899\n","Episode:  345  , Epsilon:  0.17740377510930716 , Reward -68.1317919481998 , mean_reward:  -56.52597281481942 , time_score:  500 , memory:  95778\n","Episode:  350  , Epsilon:  0.1730128104744653 , Reward 28.977776897751127 , mean_reward:  -52.849677397859296 , time_score:  500 , memory:  97913\n","Episode:  355  , Epsilon:  0.16873052768933355 , Reward -28.357781397588333 , mean_reward:  -50.71878788039853 , time_score:  500 , memory:  100413\n","Episode:  360  , Epsilon:  0.16455423674261854 , Reward -56.557410342175146 , mean_reward:  -47.703667783848665 , time_score:  500 , memory:  102913\n","Episode:  365  , Epsilon:  0.16048131420416054 , Reward -20.445785413356845 , mean_reward:  -37.1938416978041 , time_score:  500 , memory:  105142\n","Episode:  370  , Epsilon:  0.15650920157696743 , Reward -12.636618793232682 , mean_reward:  -35.52460985443777 , time_score:  500 , memory:  107642\n","Episode:  375  , Epsilon:  0.1526354036900377 , Reward -133.34696840190054 , mean_reward:  -36.90867606939439 , time_score:  489 , memory:  110128\n","Episode:  380  , Epsilon:  0.14885748713096328 , Reward -44.08472399843371 , mean_reward:  -33.827496523986596 , time_score:  500 , memory:  112628\n","Episode:  385  , Epsilon:  0.1451730787173275 , Reward -13.546876144445575 , mean_reward:  -30.526591096390856 , time_score:  500 , memory:  115128\n","Episode:  390  , Epsilon:  0.14157986400593744 , Reward -83.68398246625222 , mean_reward:  -30.347170497064866 , time_score:  255 , memory:  117236\n","Episode:  395  , Epsilon:  0.13807558583895513 , Reward -91.80696733800397 , mean_reward:  -29.50657664072368 , time_score:  266 , memory:  119502\n","Episode:  400  , Epsilon:  0.1346580429260134 , Reward 20.487288265573998 , mean_reward:  -29.65215307514853 , time_score:  500 , memory:  121788\n","Episode:  405  , Epsilon:  0.1313250884614265 , Reward -29.139622126075526 , mean_reward:  -31.968179551675465 , time_score:  500 , memory:  124288\n","Episode:  410  , Epsilon:  0.12807462877562611 , Reward -54.40550145033311 , mean_reward:  -31.78151817744632 , time_score:  500 , memory:  126707\n","Episode:  415  , Epsilon:  0.12490462201997637 , Reward 4.825845915510885 , mean_reward:  -30.642313846637034 , time_score:  500 , memory:  129207\n","Episode:  420  , Epsilon:  0.12181307688414106 , Reward -65.85201492195107 , mean_reward:  -29.996461947519244 , time_score:  500 , memory:  131707\n","Episode:  425  , Epsilon:  0.11879805134519765 , Reward -111.41922954497684 , mean_reward:  -26.66573071740283 , time_score:  499 , memory:  134206\n","Episode:  430  , Epsilon:  0.11585765144771248 , Reward 20.45345551268069 , mean_reward:  -24.05174864476456 , time_score:  500 , memory:  136706\n","Episode:  435  , Epsilon:  0.11299003011401039 , Reward -15.242112182858573 , mean_reward:  -24.508565426486417 , time_score:  500 , memory:  139193\n","Episode:  440  , Epsilon:  0.11019338598389174 , Reward -14.106489984230738 , mean_reward:  -25.670418930395293 , time_score:  500 , memory:  141193\n","Episode:  445  , Epsilon:  0.10746596228306791 , Reward -13.424433350749942 , mean_reward:  -24.147539966079606 , time_score:  500 , memory:  143437\n","Episode:  450  , Epsilon:  0.10480604571960442 , Reward 22.10796820887825 , mean_reward:  -24.85503506350581 , time_score:  500 , memory:  145718\n","Episode:  455  , Epsilon:  0.10221196540767843 , Reward 15.78873626369449 , mean_reward:  -22.740464601328032 , time_score:  500 , memory:  148218\n","Episode:  460  , Epsilon:  0.0996820918179746 , Reward -23.89704885085401 , mean_reward:  -21.947431670020634 , time_score:  500 , memory:  150718\n","Episode:  465  , Epsilon:  0.09721483575406 , Reward 16.338736168755187 , mean_reward:  -19.787080728336456 , time_score:  500 , memory:  153218\n","Episode:  470  , Epsilon:  0.09480864735409487 , Reward -48.18365770418299 , mean_reward:  -19.032614260849734 , time_score:  500 , memory:  155718\n","Episode:  475  , Epsilon:  0.09246201511725258 , Reward -27.433661468810357 , mean_reward:  -15.528686744226183 , time_score:  500 , memory:  158218\n","Episode:  480  , Epsilon:  0.09017346495423652 , Reward -3.2674808228453927 , mean_reward:  -15.320034079471537 , time_score:  500 , memory:  160718\n","Episode:  485  , Epsilon:  0.08794155926129824 , Reward 14.206611177112531 , mean_reward:  -15.255215966157703 , time_score:  500 , memory:  163218\n","Episode:  490  , Epsilon:  0.08576489601717459 , Reward -38.51865148089871 , mean_reward:  -14.006234593125628 , time_score:  500 , memory:  165718\n","Episode:  495  , Epsilon:  0.08364210790237678 , Reward -18.41863335237573 , mean_reward:  -12.33139186744063 , time_score:  500 , memory:  168218\n","Episode:  500  , Epsilon:  0.08157186144027828 , Reward -24.823800578062276 , mean_reward:  -12.11656057883183 , time_score:  500 , memory:  170718\n","Episode:  505  , Epsilon:  0.07955285615946175 , Reward -3.2551718114654715 , mean_reward:  -11.28280190451876 , time_score:  500 , memory:  173218\n","Episode:  510  , Epsilon:  0.07758382377679894 , Reward -37.02290130839702 , mean_reward:  -10.580511813308592 , time_score:  500 , memory:  175718\n","Episode:  515  , Epsilon:  0.07566352740075044 , Reward -14.921092176462423 , mean_reward:  -10.80598193039299 , time_score:  500 , memory:  178218\n","Episode:  520  , Epsilon:  0.07379076075438468 , Reward 17.777566511793168 , mean_reward:  -10.00554708544702 , time_score:  500 , memory:  180718\n","Episode:  525  , Epsilon:  0.07196434741762824 , Reward -35.02142739075142 , mean_reward:  -8.903557578435635 , time_score:  500 , memory:  183218\n","Episode:  530  , Epsilon:  0.07018314008827135 , Reward 26.184524570404374 , mean_reward:  -8.835792944034342 , time_score:  500 , memory:  185718\n","Episode:  535  , Epsilon:  0.06844601986126451 , Reward 31.28189736582655 , mean_reward:  -5.702881284004073 , time_score:  500 , memory:  188218\n","Episode:  540  , Epsilon:  0.0667518955258533 , Reward 14.033185144881529 , mean_reward:  -2.5212804008411616 , time_score:  500 , memory:  190718\n","Episode:  545  , Epsilon:  0.06509970288011008 , Reward -0.8281288064573065 , mean_reward:  0.027095435940251215 , time_score:  500 , memory:  193218\n","Episode:  550  , Epsilon:  0.06348840406243188 , Reward 3.256821930840457 , mean_reward:  0.4603868551336548 , time_score:  500 , memory:  195718\n","Episode:  555  , Epsilon:  0.06191698689958447 , Reward -25.02129984369072 , mean_reward:  -1.3656885152787146 , time_score:  500 , memory:  198218\n","Episode:  560  , Epsilon:  0.06038446427088321 , Reward -22.549071788245225 , mean_reward:  -1.0217106402777252 , time_score:  500 , memory:  200718\n","Episode:  565  , Epsilon:  0.058889873488111255 , Reward -3.5471854692827405 , mean_reward:  -0.39324378717029407 , time_score:  500 , memory:  203218\n","Episode:  570  , Epsilon:  0.05743227569078546 , Reward 33.70358583376689 , mean_reward:  0.8103927492123375 , time_score:  500 , memory:  205718\n","Episode:  575  , Epsilon:  0.05601075525639029 , Reward 1.0300010460249318 , mean_reward:  1.2849813872096436 , time_score:  500 , memory:  208218\n","Episode:  580  , Epsilon:  0.05462441922520914 , Reward 15.679978661913411 , mean_reward:  1.490133749535754 , time_score:  500 , memory:  210718\n","Episode:  585  , Epsilon:  0.05327239673939179 , Reward 28.583734036959815 , mean_reward:  2.5286907739540685 , time_score:  500 , memory:  213218\n","Episode:  590  , Epsilon:  0.05195383849590569 , Reward 2.0887997106795986 , mean_reward:  4.680137803503147 , time_score:  500 , memory:  215718\n","Episode:  595  , Epsilon:  0.05066791621302729 , Reward -50.256515962214394 , mean_reward:  3.737116039178014 , time_score:  500 , memory:  218218\n","Episode:  600  , Epsilon:  0.0494138221100385 , Reward -41.73702747451051 , mean_reward:  5.0883079092353745 , time_score:  500 , memory:  220718\n","Episode:  605  , Epsilon:  0.048190768399801194 , Reward -31.75917097207076 , mean_reward:  4.30538264933469 , time_score:  500 , memory:  223218\n","Episode:  610  , Epsilon:  0.046997986793891174 , Reward -34.52532921822178 , mean_reward:  5.029235068682032 , time_score:  500 , memory:  225718\n","Episode:  615  , Epsilon:  0.04583472801998072 , Reward -18.360024905104478 , mean_reward:  5.728576093273179 , time_score:  500 , memory:  228218\n","Episode:  620  , Epsilon:  0.04470026135116646 , Reward -5.6048337756796185 , mean_reward:  5.822374226602059 , time_score:  500 , memory:  230718\n","Episode:  625  , Epsilon:  0.04359387414694703 , Reward -43.8268791145356 , mean_reward:  5.653253916760672 , time_score:  500 , memory:  233218\n","Episode:  630  , Epsilon:  0.04251487140556204 , Reward 1.0127457288393071 , mean_reward:  5.77215778130053 , time_score:  500 , memory:  235718\n","Episode:  635  , Epsilon:  0.04146257532741124 , Reward 1.8821288508234317 , mean_reward:  4.951390597003765 , time_score:  500 , memory:  238218\n","Episode:  640  , Epsilon:  0.04043632488927963 , Reward -45.99109729577127 , mean_reward:  4.689134242559285 , time_score:  500 , memory:  240718\n","Episode:  645  , Epsilon:  0.039435475429100995 , Reward -19.605753991195563 , mean_reward:  2.9539552494147334 , time_score:  500 , memory:  243218\n","Episode:  650  , Epsilon:  0.03845939824099909 , Reward -55.6152442921009 , mean_reward:  2.6079172500325085 , time_score:  500 , memory:  245718\n","Episode:  655  , Epsilon:  0.03750748018035199 , Reward 24.378372226579074 , mean_reward:  3.4619505376592845 , time_score:  500 , memory:  248218\n","Episode:  660  , Epsilon:  0.03657912327863173 , Reward -41.36847697037247 , mean_reward:  3.3561386336398895 , time_score:  500 , memory:  250718\n","Episode:  665  , Epsilon:  0.035673744367776934 , Reward 53.105163583970494 , mean_reward:  3.3552524565077406 , time_score:  500 , memory:  253218\n","Episode:  670  , Epsilon:  0.03479077471386296 , Reward 40.52089944231919 , mean_reward:  2.8382430820686313 , time_score:  500 , memory:  255718\n","Episode:  675  , Epsilon:  0.03392965965983891 , Reward 28.51420226271961 , mean_reward:  3.4966066715689994 , time_score:  500 , memory:  258218\n","Episode:  680  , Epsilon:  0.03308985827710748 , Reward 4.369833949900199 , mean_reward:  3.082217765397867 , time_score:  500 , memory:  260718\n","Episode:  685  , Epsilon:  0.03227084302572862 , Reward 1.260029617198401 , mean_reward:  1.9228377577593025 , time_score:  500 , memory:  263218\n","Episode:  690  , Epsilon:  0.03147209942303359 , Reward 36.14609744869375 , mean_reward:  1.506170793553892 , time_score:  500 , memory:  265718\n","Episode:  695  , Epsilon:  0.030693125720441184 , Reward 22.55045547521367 , mean_reward:  1.7571427561560227 , time_score:  500 , memory:  268218\n","Episode:  700  , Epsilon:  0.029933432588273214 , Reward 29.605160602102824 , mean_reward:  1.4523067839023767 , time_score:  500 , memory:  270718\n","Episode:  705  , Epsilon:  0.029192542808371146 , Reward -21.029362240664174 , mean_reward:  1.251612694019146 , time_score:  500 , memory:  273218\n","Episode:  710  , Epsilon:  0.028469990974320916 , Reward 5.0099606750348515 , mean_reward:  0.86533716121503 , time_score:  500 , memory:  275718\n","Episode:  715  , Epsilon:  0.027765323199097504 , Reward 8.399918638693292 , mean_reward:  0.49111225105473694 , time_score:  500 , memory:  278218\n","Episode:  720  , Epsilon:  0.02707809682994571 , Reward -12.990014496801496 , mean_reward:  0.3117897982161541 , time_score:  500 , memory:  280718\n","Episode:  725  , Epsilon:  0.026407880170317945 , Reward -31.72676158056733 , mean_reward:  0.3572372763563196 , time_score:  500 , memory:  283218\n","Episode:  730  , Epsilon:  0.025754252208694463 , Reward 4.301913592013154 , mean_reward:  0.5263215647497234 , time_score:  500 , memory:  285718\n","Episode:  735  , Epsilon:  0.025116802354115567 , Reward -8.943408478492813 , mean_reward:  0.22933655778300427 , time_score:  500 , memory:  288218\n","Episode:  740  , Epsilon:  0.02449513017825978 , Reward 17.574363863240993 , mean_reward:  0.1880722737294115 , time_score:  500 , memory:  290718\n","Episode:  745  , Epsilon:  0.023888845163905856 , Reward -20.7573983846227 , mean_reward:  1.0913316502784973 , time_score:  500 , memory:  293218\n","Episode:  750  , Epsilon:  0.023297566459620722 , Reward 19.337281391150682 , mean_reward:  1.6417019320493176 , time_score:  500 , memory:  295718\n","Episode:  755  , Epsilon:  0.022720922640519125 , Reward 0.5097208529747315 , mean_reward:  1.0215264703593223 , time_score:  500 , memory:  298218\n","Episode:  760  , Epsilon:  0.022158551474944856 , Reward 34.188293862477416 , mean_reward:  2.0156091380022376 , time_score:  500 , memory:  300718\n","Episode:  765  , Epsilon:  0.021610099696926857 , Reward 44.20496302728455 , mean_reward:  2.0251104408725236 , time_score:  500 , memory:  303218\n","Episode:  770  , Epsilon:  0.021075222784267326 , Reward 14.120435041854645 , mean_reward:  2.256206497525383 , time_score:  500 , memory:  305718\n","Episode:  775  , Epsilon:  0.020553584742122436 , Reward -3.7238973257520893 , mean_reward:  1.4159700472931311 , time_score:  500 , memory:  308218\n","Episode:  780  , Epsilon:  0.020044857891939702 , Reward -1.5210707655054394 , mean_reward:  1.8612356130536165 , time_score:  500 , memory:  310718\n","Episode:  785  , Epsilon:  0.01954872266561937 , Reward 40.11411363255872 , mean_reward:  3.551929127237523 , time_score:  500 , memory:  313218\n","Episode:  790  , Epsilon:  0.019064867404770626 , Reward -25.23461114216307 , mean_reward:  3.8449885984986922 , time_score:  500 , memory:  315718\n","Episode:  795  , Epsilon:  0.018592988164936427 , Reward -24.94565141217545 , mean_reward:  4.144176909050438 , time_score:  500 , memory:  318218\n","Episode:  800  , Epsilon:  0.018132788524664028 , Reward 57.41552257290212 , mean_reward:  4.6194745054110635 , time_score:  500 , memory:  320718\n","Episode:  805  , Epsilon:  0.017683979399301233 , Reward 50.468493808332084 , mean_reward:  5.797591739001469 , time_score:  500 , memory:  323218\n","Episode:  810  , Epsilon:  0.01724627885940145 , Reward -1.7306805495705437 , mean_reward:  5.446218348500018 , time_score:  500 , memory:  325718\n","Episode:  815  , Epsilon:  0.01681941195362342 , Reward 28.594136824072052 , mean_reward:  6.051255388311905 , time_score:  500 , memory:  328218\n","Episode:  820  , Epsilon:  0.0164031105360144 , Reward -6.172706440263369 , mean_reward:  6.281612299575037 , time_score:  500 , memory:  330718\n","Episode:  825  , Epsilon:  0.015997113097568336 , Reward -20.945311644738194 , mean_reward:  7.919405569279167 , time_score:  500 , memory:  333218\n","Episode:  830  , Epsilon:  0.015601164601953134 , Reward 27.392621714717386 , mean_reward:  8.645214666471539 , time_score:  500 , memory:  335718\n","Episode:  835  , Epsilon:  0.015215016325303928 , Reward -16.77956201216836 , mean_reward:  8.699986939684043 , time_score:  500 , memory:  338218\n","Episode:  840  , Epsilon:  0.014838425699981627 , Reward -17.103867969674102 , mean_reward:  9.25987113934393 , time_score:  500 , memory:  340718\n","Episode:  845  , Epsilon:  0.014471156162198668 , Reward -25.470478056053583 , mean_reward:  9.028596705159408 , time_score:  500 , memory:  343218\n","Episode:  850  , Epsilon:  0.014112977003416188 , Reward 63.45729377393465 , mean_reward:  9.627198204195984 , time_score:  500 , memory:  345718\n","Episode:  855  , Epsilon:  0.013763663225419333 , Reward -9.410628804635323 , mean_reward:  9.905501232626696 , time_score:  500 , memory:  348218\n","Episode:  860  , Epsilon:  0.013422995398979608 , Reward -30.20372500727124 , mean_reward:  7.603600371848768 , time_score:  500 , memory:  350718\n","Episode:  865  , Epsilon:  0.013090759526015528 , Reward -6.9964596119079525 , mean_reward:  7.299477571735671 , time_score:  500 , memory:  353218\n","Episode:  870  , Epsilon:  0.012766746905164949 , Reward 10.550436763328538 , mean_reward:  7.9368143482388485 , time_score:  500 , memory:  355718\n","Episode:  875  , Epsilon:  0.012450754000684672 , Reward 11.214880492277253 , mean_reward:  8.460075478328328 , time_score:  500 , memory:  358218\n","Episode:  880  , Epsilon:  0.012142582314594924 , Reward -3.3004588209564165 , mean_reward:  8.522142046610446 , time_score:  500 , memory:  360718\n","Episode:  885  , Epsilon:  0.01184203826198843 , Reward 18.7547645667182 , mean_reward:  8.79631185693342 , time_score:  500 , memory:  363218\n","Episode:  890  , Epsilon:  0.01154893304942575 , Reward 0.5408136071567196 , mean_reward:  8.047072689742523 , time_score:  500 , memory:  365718\n","Episode:  895  , Epsilon:  0.011263082556340478 , Reward 10.858370572462377 , mean_reward:  8.858592078791075 , time_score:  500 , memory:  368218\n","Episode:  900  , Epsilon:  0.01098430721937979 , Reward 33.05046996986361 , mean_reward:  9.942233622017302 , time_score:  500 , memory:  370718\n","Episode:  905  , Epsilon:  0.01071243191960775 , Reward 36.43359380317054 , mean_reward:  10.661298210502137 , time_score:  500 , memory:  373218\n","Episode:  910  , Epsilon:  0.010447285872500434 , Reward 3.0251162161133793 , mean_reward:  10.936900031738757 , time_score:  500 , memory:  375718\n","Episode:  915  , Epsilon:  0.010188702520663827 , Reward 75.41942819105857 , mean_reward:  11.57055570321219 , time_score:  500 , memory:  378218\n","Episode:  920  , Epsilon:  0.01 , Reward 45.23809159612345 , mean_reward:  12.457209463197145 , time_score:  500 , memory:  380718\n","Episode:  925  , Epsilon:  0.01 , Reward 21.061056044893043 , mean_reward:  11.80242466608687 , time_score:  500 , memory:  383218\n","Episode:  930  , Epsilon:  0.01 , Reward 13.072694427970166 , mean_reward:  11.4386310516849 , time_score:  500 , memory:  385718\n","Episode:  935  , Epsilon:  0.01 , Reward -31.899655401603177 , mean_reward:  12.940493742607106 , time_score:  500 , memory:  388218\n","Episode:  940  , Epsilon:  0.01 , Reward 14.995182800306898 , mean_reward:  12.877034546754421 , time_score:  500 , memory:  390718\n","Episode:  945  , Epsilon:  0.01 , Reward -4.251157996791619 , mean_reward:  12.809533506513283 , time_score:  500 , memory:  393218\n","Episode:  950  , Epsilon:  0.01 , Reward -4.304594937902378 , mean_reward:  12.641864611442193 , time_score:  500 , memory:  395718\n","Episode:  955  , Epsilon:  0.01 , Reward -18.083579202522614 , mean_reward:  13.391261800094837 , time_score:  500 , memory:  398218\n","Episode:  960  , Epsilon:  0.01 , Reward 46.96633512665365 , mean_reward:  15.89463654964066 , time_score:  500 , memory:  400718\n","Episode:  965  , Epsilon:  0.01 , Reward 25.482113410092065 , mean_reward:  14.909223538864671 , time_score:  500 , memory:  403218\n","Episode:  970  , Epsilon:  0.01 , Reward -10.433943147422411 , mean_reward:  13.258249594723843 , time_score:  500 , memory:  405718\n","Episode:  975  , Epsilon:  0.01 , Reward -23.244117410825282 , mean_reward:  12.904921221677563 , time_score:  500 , memory:  408218\n","Episode:  980  , Epsilon:  0.01 , Reward -17.953943810242595 , mean_reward:  14.5640604003597 , time_score:  500 , memory:  410718\n","Episode:  985  , Epsilon:  0.01 , Reward 67.0645620972069 , mean_reward:  13.994617296917061 , time_score:  500 , memory:  413218\n","Episode:  990  , Epsilon:  0.01 , Reward 11.478293165943203 , mean_reward:  13.561244427990816 , time_score:  500 , memory:  415718\n","Episode:  995  , Epsilon:  0.01 , Reward -15.991684897731965 , mean_reward:  12.523780099214068 , time_score:  500 , memory:  418218\n","Episode:  1000  , Epsilon:  0.01 , Reward -35.265777445276775 , mean_reward:  10.647206989192432 , time_score:  500 , memory:  420718\n","Episode:  1005  , Epsilon:  0.01 , Reward -30.634006622651942 , mean_reward:  9.144481250011411 , time_score:  500 , memory:  423218\n","Episode:  1010  , Epsilon:  0.01 , Reward -3.970211675356347 , mean_reward:  8.49943871534784 , time_score:  500 , memory:  425718\n","Episode:  1015  , Epsilon:  0.01 , Reward 18.0324016406816 , mean_reward:  8.11087685429168 , time_score:  500 , memory:  428218\n","Episode:  1020  , Epsilon:  0.01 , Reward 4.599607223005112 , mean_reward:  7.1101525520981275 , time_score:  500 , memory:  430718\n","Episode:  1025  , Epsilon:  0.01 , Reward -25.132298667337544 , mean_reward:  6.767554179065378 , time_score:  500 , memory:  433218\n","Episode:  1030  , Epsilon:  0.01 , Reward 80.23267973316034 , mean_reward:  6.441656234049348 , time_score:  500 , memory:  435718\n","Episode:  1035  , Epsilon:  0.01 , Reward 36.0154896143043 , mean_reward:  4.392868820735364 , time_score:  500 , memory:  438218\n","Episode:  1040  , Epsilon:  0.01 , Reward -53.12159151509238 , mean_reward:  3.8917488827671263 , time_score:  500 , memory:  440718\n","Episode:  1045  , Epsilon:  0.01 , Reward 106.36515189509292 , mean_reward:  5.105048361808898 , time_score:  500 , memory:  443218\n","Episode:  1050  , Epsilon:  0.01 , Reward 81.58010202965404 , mean_reward:  5.176257508346541 , time_score:  500 , memory:  445718\n","Episode:  1055  , Epsilon:  0.01 , Reward -40.14918687698197 , mean_reward:  4.050633025645326 , time_score:  500 , memory:  448218\n","Episode:  1060  , Epsilon:  0.01 , Reward -18.53723481654083 , mean_reward:  0.48901834992861126 , time_score:  500 , memory:  450718\n","Episode:  1065  , Epsilon:  0.01 , Reward 15.078529004580442 , mean_reward:  1.5089871699685657 , time_score:  500 , memory:  453218\n","Episode:  1070  , Epsilon:  0.01 , Reward -20.600032794166275 , mean_reward:  4.841605635029124 , time_score:  500 , memory:  455560\n","Episode:  1075  , Epsilon:  0.01 , Reward 94.30121432788968 , mean_reward:  4.008309114616923 , time_score:  500 , memory:  458060\n","Episode:  1080  , Epsilon:  0.01 , Reward -22.030659785336745 , mean_reward:  1.6604459755906043 , time_score:  500 , memory:  460560\n","Episode:  1085  , Epsilon:  0.01 , Reward 61.15516727037381 , mean_reward:  0.937992871444803 , time_score:  500 , memory:  463060\n","Episode:  1090  , Epsilon:  0.01 , Reward 4.5569540948879474 , mean_reward:  0.493942148212144 , time_score:  500 , memory:  465560\n","Episode:  1095  , Epsilon:  0.01 , Reward -71.11469621434885 , mean_reward:  0.1239059315213234 , time_score:  500 , memory:  468060\n","Episode:  1100  , Epsilon:  0.01 , Reward 58.90148751119651 , mean_reward:  0.007795984178355297 , time_score:  500 , memory:  470560\n","Episode:  1105  , Epsilon:  0.01 , Reward -46.0166637944851 , mean_reward:  0.6253148401493029 , time_score:  500 , memory:  473060\n","Episode:  1110  , Epsilon:  0.01 , Reward -13.93344195906403 , mean_reward:  2.0515436210318527 , time_score:  500 , memory:  475560\n","Episode:  1115  , Epsilon:  0.01 , Reward -5.267626733893262 , mean_reward:  0.7221799833381273 , time_score:  500 , memory:  478060\n","Episode:  1120  , Epsilon:  0.01 , Reward 21.308606519559103 , mean_reward:  1.192751665691343 , time_score:  500 , memory:  480560\n","Episode:  1125  , Epsilon:  0.01 , Reward 42.19049487520591 , mean_reward:  1.818203328910481 , time_score:  500 , memory:  483060\n","Episode:  1130  , Epsilon:  0.01 , Reward 1.2645109654814846 , mean_reward:  0.913938484378831 , time_score:  500 , memory:  485560\n","Episode:  1135  , Epsilon:  0.01 , Reward 12.84044206903047 , mean_reward:  1.5798139965238946 , time_score:  500 , memory:  488060\n","Episode:  1140  , Epsilon:  0.01 , Reward 63.174260876637206 , mean_reward:  2.467216215043802 , time_score:  500 , memory:  490560\n","Episode:  1145  , Epsilon:  0.01 , Reward -0.45527707260648986 , mean_reward:  1.8368690381133936 , time_score:  500 , memory:  493060\n","Episode:  1150  , Epsilon:  0.01 , Reward 7.237916652456164 , mean_reward:  3.1282572212398883 , time_score:  500 , memory:  495560\n","Episode:  1155  , Epsilon:  0.01 , Reward 19.68866928286614 , mean_reward:  3.333574207058066 , time_score:  500 , memory:  498060\n","Episode:  1160  , Epsilon:  0.01 , Reward -8.022756447394812 , mean_reward:  5.576566958403149 , time_score:  500 , memory:  500560\n","Episode:  1165  , Epsilon:  0.01 , Reward -2.8622766241825297 , mean_reward:  6.070427303503236 , time_score:  500 , memory:  503060\n","Episode:  1170  , Epsilon:  0.01 , Reward -10.119834851681599 , mean_reward:  3.491437889252037 , time_score:  500 , memory:  505560\n","Episode:  1175  , Epsilon:  0.01 , Reward -11.316792708328986 , mean_reward:  3.855091658566701 , time_score:  500 , memory:  508060\n","Episode:  1180  , Epsilon:  0.01 , Reward -3.4612196671282565 , mean_reward:  4.778519221372163 , time_score:  500 , memory:  510560\n","Episode:  1185  , Epsilon:  0.01 , Reward -4.780287215932712 , mean_reward:  4.362431107252436 , time_score:  500 , memory:  513060\n","Episode:  1190  , Epsilon:  0.01 , Reward 7.549698846720465 , mean_reward:  5.723365821498456 , time_score:  500 , memory:  515560\n","Episode:  1195  , Epsilon:  0.01 , Reward -15.828118343766274 , mean_reward:  7.228735140499542 , time_score:  500 , memory:  518060\n","Episode:  1200  , Epsilon:  0.01 , Reward 19.06881909595696 , mean_reward:  7.642165971196952 , time_score:  500 , memory:  520560\n","Episode:  1205  , Epsilon:  0.01 , Reward -16.624810671695545 , mean_reward:  7.426242001149823 , time_score:  500 , memory:  523060\n","Episode:  1210  , Epsilon:  0.01 , Reward 4.2636201558762306 , mean_reward:  6.837183116226747 , time_score:  500 , memory:  525560\n","Episode:  1215  , Epsilon:  0.01 , Reward -6.984622701373975 , mean_reward:  7.468316311203872 , time_score:  500 , memory:  528060\n","Episode:  1220  , Epsilon:  0.01 , Reward -6.088722684739682 , mean_reward:  7.023838652702184 , time_score:  500 , memory:  530560\n","Episode:  1225  , Epsilon:  0.01 , Reward 30.985219847389352 , mean_reward:  6.857724671680595 , time_score:  500 , memory:  533060\n","Episode:  1230  , Epsilon:  0.01 , Reward 42.62898981937798 , mean_reward:  8.41862547881097 , time_score:  500 , memory:  535560\n","Episode:  1235  , Epsilon:  0.01 , Reward -27.522430868807586 , mean_reward:  7.756829931719053 , time_score:  500 , memory:  538060\n","Episode:  1240  , Epsilon:  0.01 , Reward -9.392231905523232 , mean_reward:  6.076322623289513 , time_score:  500 , memory:  540560\n","Episode:  1245  , Epsilon:  0.01 , Reward 34.69221652552375 , mean_reward:  6.757224225711923 , time_score:  500 , memory:  543060\n","Episode:  1250  , Epsilon:  0.01 , Reward 48.61108596356554 , mean_reward:  5.411191503321219 , time_score:  500 , memory:  545560\n","Episode:  1255  , Epsilon:  0.01 , Reward 1.3215622725704455 , mean_reward:  5.156550490926991 , time_score:  500 , memory:  547752\n","Episode:  1260  , Epsilon:  0.01 , Reward 53.88031479572709 , mean_reward:  4.940231994335356 , time_score:  500 , memory:  550081\n","Episode:  1265  , Epsilon:  0.01 , Reward 39.63696537436208 , mean_reward:  4.233961190726395 , time_score:  500 , memory:  552581\n","Episode:  1270  , Epsilon:  0.01 , Reward -54.98775920372405 , mean_reward:  4.356529151660368 , time_score:  313 , memory:  554894\n","Episode:  1275  , Epsilon:  0.01 , Reward -10.207269072471004 , mean_reward:  5.212250033809906 , time_score:  500 , memory:  557394\n","Episode:  1280  , Epsilon:  0.01 , Reward 23.51039308963242 , mean_reward:  3.568674742142763 , time_score:  500 , memory:  559556\n","Episode:  1285  , Epsilon:  0.01 , Reward -5.143925352877432 , mean_reward:  5.219921503592697 , time_score:  500 , memory:  562056\n","Episode:  1290  , Epsilon:  0.01 , Reward -43.66634954668555 , mean_reward:  4.406544563011802 , time_score:  500 , memory:  564556\n","Episode:  1295  , Epsilon:  0.01 , Reward 59.51769106760398 , mean_reward:  4.735686591407966 , time_score:  500 , memory:  567056\n","Episode:  1300  , Epsilon:  0.01 , Reward 29.2207555276562 , mean_reward:  5.083386367225219 , time_score:  500 , memory:  569475\n","Episode:  1305  , Epsilon:  0.01 , Reward -25.51898166271668 , mean_reward:  6.3484047538225745 , time_score:  500 , memory:  571975\n","Episode:  1310  , Epsilon:  0.01 , Reward 99.58745075456278 , mean_reward:  9.35428953989725 , time_score:  500 , memory:  574475\n","Episode:  1315  , Epsilon:  0.01 , Reward 61.75177356616341 , mean_reward:  11.966302173626257 , time_score:  500 , memory:  576975\n","Episode:  1320  , Epsilon:  0.01 , Reward 85.31018324395603 , mean_reward:  13.199173696326511 , time_score:  500 , memory:  579475\n","Episode:  1325  , Epsilon:  0.01 , Reward 66.85647810319972 , mean_reward:  14.481346632146574 , time_score:  500 , memory:  581975\n","Episode:  1330  , Epsilon:  0.01 , Reward 64.71432739502254 , mean_reward:  15.619724421754807 , time_score:  500 , memory:  584475\n","Episode:  1335  , Epsilon:  0.01 , Reward 56.20645452589535 , mean_reward:  16.487744724705266 , time_score:  500 , memory:  586975\n","Episode:  1340  , Epsilon:  0.01 , Reward -75.11644105885995 , mean_reward:  17.086127124330158 , time_score:  500 , memory:  589475\n","Episode:  1345  , Epsilon:  0.01 , Reward -15.038078394930688 , mean_reward:  16.904345918968186 , time_score:  500 , memory:  591975\n","Episode:  1350  , Epsilon:  0.01 , Reward 30.751067380394225 , mean_reward:  18.286814298792304 , time_score:  500 , memory:  594475\n","Episode:  1355  , Epsilon:  0.01 , Reward 32.991498693422045 , mean_reward:  21.160052429945043 , time_score:  500 , memory:  596975\n","Episode:  1360  , Epsilon:  0.01 , Reward 65.23933009689941 , mean_reward:  22.61501255627999 , time_score:  500 , memory:  599475\n","Episode:  1365  , Epsilon:  0.01 , Reward 35.54614690821103 , mean_reward:  24.263657433953345 , time_score:  500 , memory:  601975\n","Episode:  1370  , Epsilon:  0.01 , Reward -21.63058808052982 , mean_reward:  22.75665510749262 , time_score:  500 , memory:  604384\n","Episode:  1375  , Epsilon:  0.01 , Reward -3.9173718275179055 , mean_reward:  23.780414979942808 , time_score:  500 , memory:  606884\n","Episode:  1380  , Epsilon:  0.01 , Reward 44.36883770974804 , mean_reward:  25.0796269142731 , time_score:  500 , memory:  609140\n","Episode:  1385  , Epsilon:  0.01 , Reward 13.093084549722718 , mean_reward:  25.144706761697975 , time_score:  500 , memory:  611640\n","Episode:  1390  , Epsilon:  0.01 , Reward 26.8059797835447 , mean_reward:  23.81111696264648 , time_score:  500 , memory:  614125\n","Episode:  1395  , Epsilon:  0.01 , Reward -45.82261727325146 , mean_reward:  24.235964428680862 , time_score:  500 , memory:  616625\n","Episode:  1400  , Epsilon:  0.01 , Reward 62.727552174011606 , mean_reward:  23.13773771190631 , time_score:  500 , memory:  619125\n","Episode:  1405  , Epsilon:  0.01 , Reward -43.85253680554261 , mean_reward:  16.753449698608883 , time_score:  500 , memory:  621282\n","Episode:  1410  , Epsilon:  0.01 , Reward 14.668097256948728 , mean_reward:  14.908561108566218 , time_score:  500 , memory:  623782\n","Episode:  1415  , Epsilon:  0.01 , Reward 38.10281754510681 , mean_reward:  13.867948404899826 , time_score:  500 , memory:  626282\n","Episode:  1420  , Epsilon:  0.01 , Reward 19.677146924603083 , mean_reward:  13.543995210369571 , time_score:  500 , memory:  628782\n","Episode:  1425  , Epsilon:  0.01 , Reward 31.581237718495025 , mean_reward:  13.065446482418396 , time_score:  500 , memory:  631282\n","Episode:  1430  , Epsilon:  0.01 , Reward 0.2402941207168947 , mean_reward:  12.307472299597148 , time_score:  500 , memory:  633782\n","Episode:  1435  , Epsilon:  0.01 , Reward 30.453445677547876 , mean_reward:  13.20255642592919 , time_score:  500 , memory:  636282\n","Episode:  1440  , Epsilon:  0.01 , Reward -27.984046671540636 , mean_reward:  13.493945560261736 , time_score:  500 , memory:  638782\n","Episode:  1445  , Epsilon:  0.01 , Reward 33.3840313568698 , mean_reward:  13.36476958941382 , time_score:  500 , memory:  641282\n","Episode:  1450  , Epsilon:  0.01 , Reward 73.31567473610836 , mean_reward:  13.066995746079245 , time_score:  500 , memory:  643782\n","Episode:  1455  , Epsilon:  0.01 , Reward -159.5712283205816 , mean_reward:  8.638297699906802 , time_score:  463 , memory:  646245\n","Episode:  1460  , Epsilon:  0.01 , Reward -9.774232492735964 , mean_reward:  4.945343709345658 , time_score:  500 , memory:  648321\n","Episode:  1465  , Epsilon:  0.01 , Reward -7.363608970267017 , mean_reward:  -0.8972748639828061 , time_score:  500 , memory:  650050\n","Episode:  1470  , Epsilon:  0.01 , Reward -84.31586045524963 , mean_reward:  -6.458110381256992 , time_score:  192 , memory:  651511\n","Episode:  1475  , Epsilon:  0.01 , Reward -119.26057363386968 , mean_reward:  -14.323717099316776 , time_score:  296 , memory:  652763\n","Episode:  1480  , Epsilon:  0.01 , Reward -97.04227332052112 , mean_reward:  -20.780060969546035 , time_score:  312 , memory:  654477\n","Episode:  1485  , Epsilon:  0.01 , Reward -42.76529235006199 , mean_reward:  -23.60762702122555 , time_score:  500 , memory:  656821\n","Episode:  1490  , Epsilon:  0.01 , Reward -34.63460509657814 , mean_reward:  -23.488620247686328 , time_score:  500 , memory:  659321\n","Episode:  1495  , Epsilon:  0.01 , Reward 12.353596485442306 , mean_reward:  -24.99266469201187 , time_score:  500 , memory:  661821\n","Episode:  1500  , Epsilon:  0.01 , Reward -17.620545260455206 , mean_reward:  -24.75583540110292 , time_score:  500 , memory:  664321\n","Episode:  1505  , Epsilon:  0.01 , Reward 29.516835840561725 , mean_reward:  -20.263373233144566 , time_score:  500 , memory:  666525\n","Episode:  1510  , Epsilon:  0.01 , Reward -109.85342265856421 , mean_reward:  -23.571740014752145 , time_score:  500 , memory:  669025\n","Episode:  1515  , Epsilon:  0.01 , Reward -2.6882102814814512 , mean_reward:  -25.340246456116557 , time_score:  500 , memory:  671525\n","Episode:  1520  , Epsilon:  0.01 , Reward -82.51665998632248 , mean_reward:  -27.92411487730727 , time_score:  500 , memory:  674025\n","Episode:  1525  , Epsilon:  0.01 , Reward -119.95647343171878 , mean_reward:  -33.82029132946874 , time_score:  500 , memory:  676522\n","Episode:  1530  , Epsilon:  0.01 , Reward -79.65992425119923 , mean_reward:  -39.42137225402404 , time_score:  500 , memory:  678959\n","Episode:  1535  , Epsilon:  0.01 , Reward -201.69842490029697 , mean_reward:  -46.94299318245405 , time_score:  413 , memory:  681339\n","Episode:  1540  , Epsilon:  0.01 , Reward -145.45870866672004 , mean_reward:  -54.201402912154634 , time_score:  448 , memory:  683455\n","Episode:  1545  , Epsilon:  0.01 , Reward -194.26071635599385 , mean_reward:  -62.13121348217137 , time_score:  386 , memory:  685109\n","Episode:  1550  , Epsilon:  0.01 , Reward -172.26023335224787 , mean_reward:  -72.56464664549463 , time_score:  254 , memory:  686484\n","Episode:  1555  , Epsilon:  0.01 , Reward -186.0887376407344 , mean_reward:  -78.06440356315278 , time_score:  409 , memory:  688165\n","Episode:  1560  , Epsilon:  0.01 , Reward -172.27063205556908 , mean_reward:  -85.02472140835934 , time_score:  275 , memory:  689735\n","Episode:  1565  , Epsilon:  0.01 , Reward -186.60025306270103 , mean_reward:  -90.72947316819281 , time_score:  356 , memory:  691897\n","Episode:  1570  , Epsilon:  0.01 , Reward -174.57320199499173 , mean_reward:  -92.3450145117486 , time_score:  421 , memory:  693882\n","Episode:  1575  , Epsilon:  0.01 , Reward -194.01836919206954 , mean_reward:  -94.46351980999547 , time_score:  443 , memory:  696044\n","Episode:  1580  , Epsilon:  0.01 , Reward -200.765142791127 , mean_reward:  -96.62452470732138 , time_score:  401 , memory:  697920\n","Episode:  1585  , Epsilon:  0.01 , Reward -141.0156681924199 , mean_reward:  -102.96836485358097 , time_score:  452 , memory:  699963\n","Episode:  1590  , Epsilon:  0.01 , Reward -148.56378081700413 , mean_reward:  -109.14432670187063 , time_score:  406 , memory:  702127\n","Episode:  1595  , Epsilon:  0.01 , Reward -200.99005200411887 , mean_reward:  -117.24633000484437 , time_score:  402 , memory:  704166\n","Episode:  1600  , Epsilon:  0.01 , Reward -157.34180245850956 , mean_reward:  -125.592599250675 , time_score:  418 , memory:  706324\n","Episode:  1605  , Epsilon:  0.01 , Reward -134.40439870324417 , mean_reward:  -131.09862617541467 , time_score:  338 , memory:  708192\n","Episode:  1610  , Epsilon:  0.01 , Reward -179.750956044296 , mean_reward:  -137.19193437171643 , time_score:  401 , memory:  710242\n","Episode:  1615  , Epsilon:  0.01 , Reward -143.47913904234096 , mean_reward:  -142.79590953960394 , time_score:  494 , memory:  712494\n","Episode:  1620  , Epsilon:  0.01 , Reward -48.003481057666136 , mean_reward:  -144.64200770383061 , time_score:  500 , memory:  714949\n","Episode:  1625  , Epsilon:  0.01 , Reward -15.515885589224016 , mean_reward:  -143.4584635882053 , time_score:  500 , memory:  717394\n","Episode:  1630  , Epsilon:  0.01 , Reward -4.745119466340804 , mean_reward:  -141.38064866299473 , time_score:  500 , memory:  719821\n","Episode:  1635  , Epsilon:  0.01 , Reward -125.62103140317328 , mean_reward:  -137.3936625613355 , time_score:  391 , memory:  722212\n","Episode:  1640  , Epsilon:  0.01 , Reward -22.37831344877571 , mean_reward:  -132.86861448827563 , time_score:  500 , memory:  724488\n","Episode:  1645  , Epsilon:  0.01 , Reward -77.27422900196767 , mean_reward:  -127.86587427713535 , time_score:  396 , memory:  726770\n","Episode:  1650  , Epsilon:  0.01 , Reward 27.99167548855555 , mean_reward:  -122.56519030879117 , time_score:  500 , memory:  729004\n","Episode:  1655  , Epsilon:  0.01 , Reward 11.03726937008554 , mean_reward:  -119.92654415050558 , time_score:  500 , memory:  731261\n","Episode:  1660  , Epsilon:  0.01 , Reward -159.1308236665557 , mean_reward:  -109.20606213939209 , time_score:  454 , memory:  733687\n","Episode:  1665  , Epsilon:  0.01 , Reward -41.54339708204747 , mean_reward:  -100.55713463612474 , time_score:  500 , memory:  736187\n","Episode:  1670  , Epsilon:  0.01 , Reward 35.53635033887345 , mean_reward:  -91.97822948814942 , time_score:  500 , memory:  738687\n","Episode:  1675  , Epsilon:  0.01 , Reward -69.40991544414791 , mean_reward:  -87.69173629027149 , time_score:  357 , memory:  741041\n","Episode:  1680  , Epsilon:  0.01 , Reward -14.466187910676984 , mean_reward:  -80.79832613647663 , time_score:  500 , memory:  743541\n","Episode:  1685  , Epsilon:  0.01 , Reward -45.51456832885536 , mean_reward:  -76.530783535964 , time_score:  500 , memory:  745991\n","Episode:  1690  , Epsilon:  0.01 , Reward 51.85374046749595 , mean_reward:  -71.42839414394473 , time_score:  500 , memory:  748381\n","Episode:  1695  , Epsilon:  0.01 , Reward -221.8077390471297 , mean_reward:  -68.84495407530841 , time_score:  471 , memory:  750756\n","Episode:  1700  , Epsilon:  0.01 , Reward -62.35743329578115 , mean_reward:  -59.473970747645616 , time_score:  373 , memory:  753129\n","Episode:  1705  , Epsilon:  0.01 , Reward 5.959619532130163 , mean_reward:  -52.83118648024459 , time_score:  500 , memory:  755607\n","Episode:  1710  , Epsilon:  0.01 , Reward 169.92641038598197 , mean_reward:  -42.259620599996055 , time_score:  447 , memory:  758054\n","Episode:  1715  , Epsilon:  0.01 , Reward -4.237828082535058 , mean_reward:  -36.60950940226234 , time_score:  500 , memory:  760554\n","Episode:  1720  , Epsilon:  0.01 , Reward 9.432575937515763 , mean_reward:  -31.740736185622662 , time_score:  500 , memory:  763054\n","Episode:  1725  , Epsilon:  0.01 , Reward -33.9048082973003 , mean_reward:  -27.741745874153832 , time_score:  500 , memory:  765554\n","Episode:  1730  , Epsilon:  0.01 , Reward -12.406125196709525 , mean_reward:  -24.935541717515072 , time_score:  500 , memory:  768054\n","Episode:  1735  , Epsilon:  0.01 , Reward -9.51761185515496 , mean_reward:  -23.20666393898227 , time_score:  500 , memory:  770554\n","Episode:  1740  , Epsilon:  0.01 , Reward -3.1502128883255436 , mean_reward:  -19.960111791748574 , time_score:  500 , memory:  773054\n","Episode:  1745  , Epsilon:  0.01 , Reward 20.070799425738883 , mean_reward:  -18.297481749253976 , time_score:  500 , memory:  775486\n","Episode:  1750  , Epsilon:  0.01 , Reward 79.94315383661383 , mean_reward:  -12.93707401844649 , time_score:  500 , memory:  777981\n","Episode:  1755  , Epsilon:  0.01 , Reward 61.56510384863779 , mean_reward:  -8.4642602244451 , time_score:  500 , memory:  780425\n","Episode:  1760  , Epsilon:  0.01 , Reward 47.005300701935184 , mean_reward:  -10.44615951082548 , time_score:  500 , memory:  782779\n","Episode:  1765  , Epsilon:  0.01 , Reward 202.51616203670335 , mean_reward:  -4.7702590858176395 , time_score:  467 , memory:  785236\n","Episode:  1770  , Epsilon:  0.01 , Reward -11.683096244394575 , mean_reward:  -4.950225596625408 , time_score:  500 , memory:  787736\n","Episode:  1775  , Epsilon:  0.01 , Reward -15.432220596888037 , mean_reward:  -1.8477686715420156 , time_score:  500 , memory:  790236\n","Episode:  1780  , Epsilon:  0.01 , Reward 52.09935564290511 , mean_reward:  0.6517650262916445 , time_score:  500 , memory:  792736\n","Episode:  1785  , Epsilon:  0.01 , Reward 57.09685851992994 , mean_reward:  5.334842859576782 , time_score:  500 , memory:  795236\n","Episode:  1790  , Epsilon:  0.01 , Reward 53.75531760743732 , mean_reward:  8.593342714951147 , time_score:  500 , memory:  797736\n","Episode:  1795  , Epsilon:  0.01 , Reward 13.697249061102625 , mean_reward:  15.226792170514637 , time_score:  500 , memory:  800236\n","Episode:  1800  , Epsilon:  0.01 , Reward 29.372014054224593 , mean_reward:  14.510501696746784 , time_score:  500 , memory:  802708\n","Episode:  1805  , Epsilon:  0.01 , Reward -11.996415565513894 , mean_reward:  14.92322559366001 , time_score:  500 , memory:  805208\n","Episode:  1810  , Epsilon:  0.01 , Reward 23.162861988624673 , mean_reward:  12.21337770547765 , time_score:  500 , memory:  807708\n","Episode:  1815  , Epsilon:  0.01 , Reward -4.836045994255544 , mean_reward:  12.765792276097386 , time_score:  500 , memory:  810208\n","Episode:  1820  , Epsilon:  0.01 , Reward 2.5800840967326337 , mean_reward:  11.44032523271179 , time_score:  500 , memory:  812708\n","Episode:  1825  , Epsilon:  0.01 , Reward -203.86526625800653 , mean_reward:  10.369605340939243 , time_score:  426 , memory:  815134\n","Episode:  1830  , Epsilon:  0.01 , Reward -1.9288513018746871 , mean_reward:  11.353172729330911 , time_score:  500 , memory:  817634\n","Episode:  1835  , Epsilon:  0.01 , Reward -1.8346376497239703 , mean_reward:  11.362533992551898 , time_score:  500 , memory:  820134\n","Episode:  1840  , Epsilon:  0.01 , Reward 15.12932948582109 , mean_reward:  10.644867876456885 , time_score:  500 , memory:  822634\n","Episode:  1845  , Epsilon:  0.01 , Reward 12.195771792524743 , mean_reward:  12.335278295473442 , time_score:  500 , memory:  825134\n","Episode:  1850  , Epsilon:  0.01 , Reward -11.109877015835199 , mean_reward:  10.696730615118698 , time_score:  500 , memory:  827634\n","Episode:  1855  , Epsilon:  0.01 , Reward 5.325560360260713 , mean_reward:  11.244273524568518 , time_score:  500 , memory:  830134\n","Episode:  1860  , Epsilon:  0.01 , Reward 18.24010974373541 , mean_reward:  11.548748311035029 , time_score:  500 , memory:  832634\n","Episode:  1865  , Epsilon:  0.01 , Reward -60.8110460358625 , mean_reward:  5.798602259427778 , time_score:  500 , memory:  835134\n","Episode:  1870  , Epsilon:  0.01 , Reward 1.3812456407133256 , mean_reward:  3.5180201248125678 , time_score:  500 , memory:  837601\n","Episode:  1875  , Epsilon:  0.01 , Reward 3.63983260367051 , mean_reward:  4.259357710540017 , time_score:  500 , memory:  840101\n","Episode:  1880  , Epsilon:  0.01 , Reward 7.384692102350968 , mean_reward:  3.3457137950378417 , time_score:  500 , memory:  842601\n","Episode:  1885  , Epsilon:  0.01 , Reward 22.136717162218723 , mean_reward:  2.161214394005895 , time_score:  500 , memory:  845101\n","Episode:  1890  , Epsilon:  0.01 , Reward 24.777754117213586 , mean_reward:  2.2915078175129278 , time_score:  500 , memory:  847601\n","Episode:  1895  , Epsilon:  0.01 , Reward 20.263470838724835 , mean_reward:  0.4772327227520233 , time_score:  500 , memory:  850101\n","Episode:  1900  , Epsilon:  0.01 , Reward 28.58050384017464 , mean_reward:  0.9230480553634262 , time_score:  500 , memory:  852601\n","Episode:  1905  , Epsilon:  0.01 , Reward 27.58725849092473 , mean_reward:  0.5513663555805012 , time_score:  500 , memory:  855101\n","Episode:  1910  , Epsilon:  0.01 , Reward 45.74337902737683 , mean_reward:  1.8808368160233817 , time_score:  500 , memory:  857601\n","Episode:  1915  , Epsilon:  0.01 , Reward 11.46765417106667 , mean_reward:  1.5729316190952283 , time_score:  500 , memory:  860101\n","Episode:  1920  , Epsilon:  0.01 , Reward -1.944068633609281 , mean_reward:  1.7438230530869665 , time_score:  500 , memory:  862601\n","Episode:  1925  , Epsilon:  0.01 , Reward 58.56794161554261 , mean_reward:  2.3864219457262488 , time_score:  500 , memory:  865101\n","Episode:  1930  , Epsilon:  0.01 , Reward -15.458794625891128 , mean_reward:  1.9878873905938663 , time_score:  500 , memory:  867601\n","Episode:  1935  , Epsilon:  0.01 , Reward 1.040948482280555 , mean_reward:  2.2737325591957376 , time_score:  500 , memory:  870101\n","Episode:  1940  , Epsilon:  0.01 , Reward 51.89753424973705 , mean_reward:  4.1029022553583285 , time_score:  500 , memory:  872601\n","Episode:  1945  , Epsilon:  0.01 , Reward -19.674064341829915 , mean_reward:  3.513901584314131 , time_score:  500 , memory:  875101\n","Episode:  1950  , Epsilon:  0.01 , Reward -1.722943675453233 , mean_reward:  4.110954119992031 , time_score:  500 , memory:  877601\n","Episode:  1955  , Epsilon:  0.01 , Reward 40.05274839251503 , mean_reward:  5.575624315690472 , time_score:  500 , memory:  880101\n","Episode:  1960  , Epsilon:  0.01 , Reward -27.254195741430156 , mean_reward:  6.2565894806538145 , time_score:  500 , memory:  882601\n","Episode:  1965  , Epsilon:  0.01 , Reward 7.862183099669483 , mean_reward:  8.266118350735425 , time_score:  500 , memory:  885101\n","Episode:  1970  , Epsilon:  0.01 , Reward 47.68315760986523 , mean_reward:  11.359216630090664 , time_score:  500 , memory:  887601\n","Episode:  1975  , Epsilon:  0.01 , Reward 11.091709496916492 , mean_reward:  12.484920657908265 , time_score:  500 , memory:  890101\n","Episode:  1980  , Epsilon:  0.01 , Reward 5.064315970254055 , mean_reward:  13.487694957902589 , time_score:  500 , memory:  892601\n","Episode:  1985  , Epsilon:  0.01 , Reward 29.28565711086624 , mean_reward:  14.533726351949987 , time_score:  500 , memory:  895101\n","Episode:  1990  , Epsilon:  0.01 , Reward 26.601086117513738 , mean_reward:  14.337401716586367 , time_score:  500 , memory:  897601\n","Episode:  1995  , Epsilon:  0.01 , Reward 0.5966901403652702 , mean_reward:  15.856107428083474 , time_score:  500 , memory:  900101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LctZX16UkZ2z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oUZZ81CkZ5P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LigtDnbikZ7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pic26PzvkZ-I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM06jVdTkaA0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eb-td7BDkaDf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjInw1qkaF_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8MT-kCZkaIY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHXj0aMkaLE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-NkHivkaNq"},"source":[""],"execution_count":null,"outputs":[]}]}